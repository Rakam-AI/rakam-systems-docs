"use strict";(self.webpackChunkrakam_systems_docs_portal=self.webpackChunkrakam_systems_docs_portal||[]).push([[3919],{8453(e,n,t){t.d(n,{R:()=>l,x:()=>o});var i=t(6540);const s={},a=i.createContext(s);function l(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),i.createElement(a.Provider,{value:n},e.children)}},8649(e,n,t){t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"tools/evaluation","title":"Rakam Eval SDK Documentation","description":"This document provides a comprehensive guide to using the rakam-system-tools, a Python client for interacting with the DeepEval API. This SDK allows you to run evaluations on your text and schema data, either synchronously or in the background.","source":"@site/docs/tools/evaluation.md","sourceDirName":"tools","slug":"/tools/evaluation","permalink":"/rakam-systems-docs/tools/evaluation","draft":false,"unlisted":false,"editUrl":"https://github.com/Rakam-AI/rakam_systems/edit/main/docs/docs/tools/evaluation.md","tags":[],"version":"current","lastUpdatedBy":"Mohamed Bashar Touil","lastUpdatedAt":1771249661000,"frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Vectorstore","permalink":"/rakam-systems-docs/ai-components/vectorstore/"},"next":{"title":"Rakam Eval SDK Documentation","permalink":"/rakam-systems-docs/tools/evaluation"}}');var s=t(4848),a=t(8453);const l={},o="Rakam Eval SDK Documentation",r={},c=[{value:"Installation",id:"installation",level:2},{value:"Configuration",id:"configuration",level:2},{value:"Initializing the Client",id:"initializing-the-client",level:3},{value:"Configuration Options",id:"configuration-options",level:3},{value:"Usage",id:"usage",level:2},{value:"Text Evaluation",id:"text-evaluation",level:3},{value:"Data and Metrics",id:"data-and-metrics",level:4},{value:"Methods",id:"methods",level:4},{value:"Example",id:"example",level:4},{value:"Schema Evaluation",id:"schema-evaluation",level:3},{value:"Data and Metrics",id:"data-and-metrics-1",level:4},{value:"Methods",id:"methods-1",level:4},{value:"Example",id:"example-1",level:4},{value:"Probabilistic Evaluation",id:"probabilistic-evaluation",level:3},{value:"Error Handling",id:"error-handling",level:3},{value:"Client-Side Metrics",id:"client-side-metrics",level:3},{value:"<code>ClientSideMetricConfig</code>",id:"clientsidemetricconfig",level:4},{value:"Example",id:"example-2",level:4}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"rakam-eval-sdk-documentation",children:"Rakam Eval SDK Documentation"})}),"\n",(0,s.jsxs)(n.p,{children:["This document provides a comprehensive guide to using the ",(0,s.jsx)(n.code,{children:"rakam-system-tools"}),", a Python client for interacting with the DeepEval API. This SDK allows you to run evaluations on your text and schema data, either synchronously or in the background."]}),"\n",(0,s.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,s.jsxs)(n.p,{children:["To get started, install the ",(0,s.jsx)(n.code,{children:"rakam-system-tools"})," package using pip:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install -i https://test.pypi.org/simple/ rakam-system-tools==0.1.12\n"})}),"\n",(0,s.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"DeepEvalClient"})," is the main entry point for using the SDK. To use it, you need to configure the client with your API endpoint and token."]}),"\n",(0,s.jsx)(n.h3,{id:"initializing-the-client",children:"Initializing the Client"}),"\n",(0,s.jsxs)(n.p,{children:["You can initialize the ",(0,s.jsx)(n.code,{children:"DeepEvalClient"})," by providing the ",(0,s.jsx)(n.code,{children:"base_url"})," and ",(0,s.jsx)(n.code,{children:"api_token"})," directly:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_tools.evaluation import DeepEvalClient\n\nclient = DeepEvalClient(\n    base_url="http://your-deepeval-api-url.com",\n    api_token="your-api-token"\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"configuration-options",children:"Configuration Options"}),"\n",(0,s.jsx)(n.p,{children:"The client can be configured in three ways, in the following order of precedence:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Directly in the constructor:"})," As shown in the example above."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Using a settings module:"})," You can pass a settings module to the client. The client will look for ",(0,s.jsx)(n.code,{children:"EVALFRAMEWORK_URL"})," and ",(0,s.jsx)(n.code,{children:"EVALFRAMWORK_API_KEY"})," attributes in the module."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# settings.py\nEVALFRAMEWORK_URL = "http://your-deepeval-api-url.com"\nEVALFRAMWORK_API_KEY = "your-api-token"\n\n# your_app.py\nfrom rakam_systems_tools.evaluation import DeepEvalClient\nimport settings # for django can be something like: from django.conf import settings\n\nclient = DeepEvalClient(settings_module=settings)\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Using environment variables:"})," The client will automatically pick up the ",(0,s.jsx)(n.code,{children:"EVALFRAMEWORK_URL"})," and ",(0,s.jsx)(n.code,{children:"EVALFRAMWORK_API_KEY"})," environment variables if they are set."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'export EVALFRAMEWORK_URL="http://your-deepeval-api-url.com"\nexport EVALFRAMWORK_API_KEY="your-api-token"\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from rakam_systems_tools.evaluation import DeepEvalClient\n\nclient = DeepEvalClient()\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["If no ",(0,s.jsx)(n.code,{children:"base_url"})," is provided, it defaults to ",(0,s.jsx)(n.code,{children:"http://localhost:8080"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,s.jsx)(n.p,{children:"The SDK provides methods for evaluating text and schema data. For each type of evaluation, there are synchronous and background (asynchronous) methods."}),"\n",(0,s.jsx)(n.h3,{id:"text-evaluation",children:"Text Evaluation"}),"\n",(0,s.jsx)(n.p,{children:"Text evaluation is used for tasks like checking correctness, relevancy, faithfulness, and toxicity of text data."}),"\n",(0,s.jsx)(n.h4,{id:"data-and-metrics",children:"Data and Metrics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"TextInputItem"})}),": Represents a single item for text evaluation. It includes ",(0,s.jsx)(n.code,{children:"id"}),", ",(0,s.jsx)(n.code,{children:"input"}),", ",(0,s.jsx)(n.code,{children:"output"}),", ",(0,s.jsx)(n.code,{children:"expected_output"})," (optional), and ",(0,s.jsx)(n.code,{children:"retrieval_context"})," (optional)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"MetricConfig"})}),": Defines the metrics to be used for evaluation. Available text evaluation metrics are:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"CorrectnessConfig"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"AnswerRelevancyConfig"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"FaithfulnessConfig"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"ToxicityConfig"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"methods",children:"Methods"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"text_eval()"}),": Runs a synchronous text evaluation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"text_eval_background()"}),": Runs a background text evaluation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"maybe_text_eval()"}),": Randomly runs ",(0,s.jsx)(n.code,{children:"text_eval"})," based on a given probability."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"maybe_text_eval_background()"}),": Randomly runs ",(0,s.jsx)(n.code,{children:"text_eval_background"})," based on a given probability."]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"example",children:"Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_tools.evaluation import DeepEvalClient, TextInputItem, CorrectnessConfig\n\nclient = DeepEvalClient()\n\ndata = [\n    TextInputItem(\n        input="What is the capital of France?",\n        output="Paris",\n        expected_output="The capital of France is Paris."\n    )\n]\n\nmetrics = [\n    CorrectnessConfig(model="gpt-4.1", steps=["Check if the output correctly answers the input."])\n]\n\nresult = client.text_eval(data=data, metrics=metrics, component="faq-bot")\nprint(result)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"schema-evaluation",children:"Schema Evaluation"}),"\n",(0,s.jsx)(n.p,{children:"Schema evaluation is used for tasks that involve structured data, such as JSON. It can be used to check for JSON correctness and the presence of specific fields."}),"\n",(0,s.jsx)(n.h4,{id:"data-and-metrics-1",children:"Data and Metrics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"SchemaInputItem"})}),": Represents a single item for schema evaluation. It includes ",(0,s.jsx)(n.code,{children:"input"})," and ",(0,s.jsx)(n.code,{children:"output"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"SchemaMetricConfig"})}),": Defines the metrics for schema evaluation. Available metrics are:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"JsonCorrectnessConfig"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"FieldsPresenceConfig"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"methods-1",children:"Methods"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"schema_eval()"}),": Runs a synchronous schema evaluation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"schema_eval_background()"}),": Runs a background schema evaluation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"maybe_schema_eval()"}),": Randomly runs ",(0,s.jsx)(n.code,{children:"schema_eval"})," based on a given probability."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"maybe_schema_eval_background()"}),": Randomly runs ",(0,s.jsx)(n.code,{children:"schema_eval_background"})," based on a given probability."]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"example-1",children:"Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_tools.evaluation import DeepEvalClient, SchemaInputItem, JsonCorrectnessConfig\n\nclient = DeepEvalClient()\n\ndata = [\n    SchemaInputItem(\n        input="Generate a JSON object with name and age.",\n        output=\'{"name": "John", "age": 30}\'\n    )\n]\n\nmetrics = [\n    JsonCorrectnessConfig(\n        excpected_schema={"type": "object", "properties": {"name": {"type": "string"}, "age": {"type": "number"}}}\n    )\n]\n\nresult = client.schema_eval(data=data, metrics=metrics, component="json-generator")\nprint(result)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"probabilistic-evaluation",children:"Probabilistic Evaluation"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"maybe_*"})," methods provide a way to run evaluations probabilistically. This can be useful for reducing the load on the evaluation service or for sampling evaluations. The ",(0,s.jsx)(n.code,{children:"chance"})," parameter is a float between 0 and 1 that determines the probability of the evaluation running."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# This evaluation will run approximately 10% of the time.\nclient.maybe_text_eval(data=data, metrics=metrics, chance=0.1)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"error-handling",children:"Error Handling"}),"\n",(0,s.jsxs)(n.p,{children:['By default, the client methods will not raise exceptions on network errors or non-2xx HTTP responses. Instead, they will return a dictionary with an "error" key. To change this behavior and have exceptions raised, you can set ',(0,s.jsx)(n.code,{children:"raise_exception=True"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'try:\n    result = client.text_eval(data=data, metrics=metrics, raise_exception=True)\nexcept requests.RequestException as e:\n    print(f"An error occurred: {e}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"client-side-metrics",children:"Client-Side Metrics"}),"\n",(0,s.jsx)(n.p,{children:"In addition to server-side evaluations, the SDK supports logging metrics that are calculated on the client side. This is useful when you have your own evaluation methods or want to log scores from other systems. These client-side metrics are sent along with the input data and are logged without any server-side evaluation."}),"\n",(0,s.jsx)(n.h4,{id:"clientsidemetricconfig",children:(0,s.jsx)(n.code,{children:"ClientSideMetricConfig"})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"ClientSideMetricConfig"})," class allows you to define your own metrics. The fields are:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"name"})," (str): The name of the metric."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"score"})," (float): The score of the metric."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"success"})," (Optional[int]): Whether the evaluation was successful (1 for success, 0 for failure). Defaults to 1."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"evaluation_cost"})," (Optional[float]): The cost of the evaluation. Defaults to 0."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"reason"})," (Optional[str]): An optional reason or explanation for the score."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"threshold"})," (Optional[float]): An optional threshold for the metric. Defaults to 0."]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"example-2",children:"Example"}),"\n",(0,s.jsxs)(n.p,{children:["You can include a list of ",(0,s.jsx)(n.code,{children:"ClientSideMetricConfig"})," objects in the ",(0,s.jsx)(n.code,{children:"metrics"})," field of each ",(0,s.jsx)(n.code,{children:"TextInputItem"})," or ",(0,s.jsx)(n.code,{children:"SchemaInputItem"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_tools.evaluation import DeepEvalClient, TextInputItem, ClientSideMetricConfig\n\nclient = DeepEvalClient()\n\n# Assume you have a custom function to evaluate sentiment\ndef calculate_sentiment_score(text):\n    # Your custom logic here\n    if "happy" in text:\n        return 1.0\n    return 0.2\n\noutput_text = "I am happy with this product."\nsentiment_score = calculate_sentiment_score(output_text)\n\ndata = [\n    TextInputItem(\n        input="User review",\n        output=output_text,\n        metrics=[\n            ClientSideMetricConfig(\n                name="sentiment",\n                score=sentiment_score,\n                reason="The user expressed a positive sentiment."\n            )\n        ]\n    )\n]\n\n# You can send client-side metrics without any server-side metrics\nresult = client.text_eval(data=data, metrics=[], component="review-analyzer")\nprint(result)\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.code,{children:"Note: When you send client-side metrics, you can pass an empty list to the `metrics` parameter of the `text_eval` or `schema_eval` methods if you don't want to run any server-side evaluations. "})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);