"use strict";(self.webpackChunkrakam_systems_docs_portal=self.webpackChunkrakam_systems_docs_portal||[]).push([[6464],{282(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"getting-started/development_guide","title":"Rakam\'s Development Guide","description":"Rakam Systems is a modular AI framework designed to build production-ready AI applications. It provides a comprehensive set of components for building AI agents, vector stores, and LLM-powered applications.","source":"@site/docs/getting-started/development_guide.md","sourceDirName":"getting-started","slug":"/getting-started/development_guide","permalink":"/rakam-systems-docs/getting-started/development_guide","draft":false,"unlisted":false,"editUrl":"https://github.com/Rakam-AI/rakam_systems/edit/main/docs/docs/getting-started/development_guide.md","tags":[],"version":"current","lastUpdatedBy":null,"lastUpdatedAt":null,"frontMatter":{}}');var s=t(4848),a=t(8453);const o={},i="Rakam's Development Guide",c={},d=[{value:"\ud83d\udcd1 Table of Contents",id:"-table-of-contents",level:2},{value:"\ud83c\udfd7\ufe0f Architecture Overview",id:"\ufe0f-architecture-overview",level:2},{value:"Design Principles",id:"design-principles",level:3},{value:"Core Package (<code>rakam-systems-core</code>)",id:"core-package-rakam-systems-core",level:2},{value:"BaseComponent",id:"basecomponent",level:3},{value:"Interfaces",id:"interfaces",level:3},{value:"AgentComponent",id:"agentcomponent",level:4},{value:"ToolComponent",id:"toolcomponent",level:4},{value:"ToolRegistry",id:"toolregistry",level:4},{value:"LLMGateway",id:"llmgateway",level:4},{value:"VectorStore",id:"vectorstore",level:4},{value:"Loader",id:"loader",level:4},{value:"Tracking System",id:"tracking-system",level:3},{value:"Configuration Loader",id:"configuration-loader",level:3},{value:"\ud83e\udd16 Agent Package (<code>rakam-systems-agent</code>)",id:"-agent-package-rakam-systems-agent",level:2},{value:"BaseAgent",id:"baseagent",level:3},{value:"Dynamic System Prompts",id:"dynamic-system-prompts",level:4},{value:"LLM Gateways",id:"llm-gateways",level:3},{value:"OpenAI Gateway",id:"openai-gateway",level:4},{value:"Mistral Gateway",id:"mistral-gateway",level:4},{value:"Gateway Factory",id:"gateway-factory",level:4},{value:"Chat History",id:"chat-history",level:3},{value:"JSON Chat History",id:"json-chat-history",level:4},{value:"SQL Chat History (SQLite)",id:"sql-chat-history-sqlite",level:4},{value:"PostgreSQL Chat History",id:"postgresql-chat-history",level:4},{value:"\ud83d\udd0d Vectorstore Package (<code>rakam-systems-vectorstore</code>)",id:"-vectorstore-package-rakam-systems-vectorstore",level:2},{value:"Core Data Structures",id:"core-data-structures",level:3},{value:"ConfigurablePgVectorStore",id:"configurablepgvectorstore",level:3},{value:"Keyword Search",id:"keyword-search",level:4},{value:"Multi-Model Support",id:"multi-model-support",level:4},{value:"ConfigurableEmbeddings",id:"configurableembeddings",level:3},{value:"Factory Function",id:"factory-function",level:4},{value:"AdaptiveLoader",id:"adaptiveloader",level:3},{value:"Factory Function",id:"factory-function-1",level:4},{value:"Specialized Loaders",id:"specialized-loaders",level:3},{value:"PdfLoaderLight",id:"pdfloaderlight",level:4},{value:"Image Extraction Support",id:"image-extraction-support",level:4},{value:"TextChunker",id:"textchunker",level:3},{value:"AdvancedChunker",id:"advancedchunker",level:3},{value:"Logging Utilities",id:"logging-utilities",level:3},{value:"\u2699\ufe0f Configuration System",id:"\ufe0f-configuration-system",level:2},{value:"Benefits of Configuration-First Design",id:"benefits-of-configuration-first-design",level:3},{value:"Real-World Scenarios",id:"real-world-scenarios",level:3},{value:"\u2699\ufe0f Configuration System Details",id:"\ufe0f-configuration-system-details",level:2},{value:"VectorStoreConfig",id:"vectorstoreconfig",level:3},{value:"YAML Configuration Example",id:"yaml-configuration-example",level:3},{value:"Agent Configuration Example",id:"agent-configuration-example",level:3},{value:"\ud83d\ude80 Quick Start Examples",id:"-quick-start-examples",level:2},{value:"Basic Agent",id:"basic-agent",level:3},{value:"Agent with Tools",id:"agent-with-tools",level:3},{value:"Document Search Pipeline",id:"document-search-pipeline",level:3},{value:"Full RAG Pipeline",id:"full-rag-pipeline",level:3},{value:"Environment Variables",id:"environment-variables",level:2},{value:"\u2705 Best Practices",id:"-best-practices",level:2},{value:"\ud83d\udcda Further Reading",id:"-further-reading",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"rakams-development-guide",children:"Rakam's Development Guide"})}),"\n",(0,s.jsx)(n.p,{children:"Rakam Systems is a modular AI framework designed to build production-ready AI applications. It provides a comprehensive set of components for building AI agents, vector stores, and LLM-powered applications."}),"\n",(0,s.jsx)(n.h2,{id:"-table-of-contents",children:"\ud83d\udcd1 Table of Contents"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#%EF%B8%8F-architecture-overview",children:"\ud83c\udfd7\ufe0f Architecture Overview"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsxs)(n.a,{href:"#core-package-rakam-systems-core",children:["\ud83e\uddf1 Core Package (",(0,s.jsx)(n.code,{children:"rakam-system-core"}),")"]})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsxs)(n.a,{href:"#-agent-package-rakam-systems-agent",children:["\ud83e\udd16 Agent Package (",(0,s.jsx)(n.code,{children:"rakam-systems-agent"}),")"]})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsxs)(n.a,{href:"#-vectorstore-package-rakam-systems-vectorstore",children:["\ud83d\udd0d Vectorstore Package (",(0,s.jsx)(n.code,{children:"rakam-systems-vectorstore"}),")"]})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#%EF%B8%8F-configuration-system",children:"\u2699\ufe0f Configuration System"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#-quick-start-examples",children:"\ud83d\ude80 Quick Start Examples"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#environment-variables",children:"\ud83c\udf0d Environment Variables"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#-best-practices",children:"\u2705 Best Practices"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#-further-reading",children:"\ud83d\udcda Further Reading"})}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"\ufe0f-architecture-overview",children:"\ud83c\udfd7\ufe0f Architecture Overview"}),"\n",(0,s.jsx)(n.p,{children:"Rakam Systems is organized into three independent packages:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"rakam-systems/\n\u251c\u2500\u2500 core/           # Core abstractions, interfaces, and base classes\n\u2502   \u2514\u2500\u2500 src/rakam_system_core/\n\u2502       \u251c\u2500\u2500 ai_core/             # Core interfaces and base component\n\u2502       \u2502   \u251c\u2500\u2500 base.py          # BaseComponent\n\u2502       \u2502   \u251c\u2500\u2500 interfaces/      # Abstract interfaces\n\u2502       \u2502   \u251c\u2500\u2500 config_loader.py # Configuration system\n\u2502       \u2502   \u2514\u2500\u2500 tracking.py      # Input/output tracking\n\u2502       \u2514\u2500\u2500 ai_utils/            # Logging utilities\n\u251c\u2500\u2500 ai-components/agent/          # Agent implementations (depends on core)\n\u2502   \u2514\u2500\u2500 src/rakam_systems_agent/\n\u2502       \u2514\u2500\u2500 components/\n\u2502           \u251c\u2500\u2500 base_agent.py    # BaseAgent implementation\n\u2502           \u251c\u2500\u2500 llm_gateway/     # LLM provider gateways\n\u2502           \u251c\u2500\u2500 chat_history/    # Chat history backends\n\u2502           \u2514\u2500\u2500 tools/           # Built-in tools\n\u2514\u2500\u2500 ai-components/vectorstore/    # Vector storage (depends on core)\n    \u2514\u2500\u2500 src/rakam_systems_vectorstore/\n        \u251c\u2500\u2500 core.py              # Node, VSFile data structures\n        \u251c\u2500\u2500 config.py            # VectorStoreConfig\n        \u2514\u2500\u2500 components/\n            \u251c\u2500\u2500 vectorstore/     # Store implementations\n            \u251c\u2500\u2500 embedding_model/ # Embedding models\n            \u251c\u2500\u2500 loader/          # Document loaders\n            \u2514\u2500\u2500 chunker/         # Text chunkers\n"})}),"\n",(0,s.jsx)(n.h3,{id:"design-principles",children:"Design Principles"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Modular Architecture"}),": Three independent packages that can be installed separately"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Clear Dependencies"}),": Agent and vectorstore packages depend on core"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Component-Based"}),": All components extend ",(0,s.jsx)(n.code,{children:"BaseComponent"})," with lifecycle management (",(0,s.jsx)(n.code,{children:"setup()"}),", ",(0,s.jsx)(n.code,{children:"shutdown()"}),")"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Interface-Driven"}),": Abstract interfaces define contracts for extensibility"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Configuration-First"}),": YAML/JSON configuration support for all components"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Provider-Agnostic"}),": Support for multiple LLM providers, embedding models, and vector stores"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.h2,{id:"core-package-rakam-systems-core",children:["Core Package (",(0,s.jsx)(n.code,{children:"rakam-systems-core"}),")"]}),"\n",(0,s.jsx)(n.p,{children:"The core package provides foundational abstractions used throughout the system. This package must be installed before using agent or vectorstore packages."}),"\n",(0,s.jsx)(n.h3,{id:"basecomponent",children:"BaseComponent"}),"\n",(0,s.jsx)(n.p,{children:"The base class for all components, providing lifecycle management and evaluation capabilities."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_core.base import BaseComponent\n\nclass BaseComponent(ABC):\n    """\n    Base class with:\n    - name and config attributes\n    - setup()/shutdown() lifecycle hooks\n    - __call__ for auto-setup execution\n    - Context manager support\n    - Built-in evaluation harness\n    """\n\n    def __init__(self, name: str, config: Optional[Dict] = None):\n        self.name = name\n        self.config = config or {}\n        self.initialized = False\n\n    def setup(self) -> None:\n        """Initialize heavy resources - override in subclasses."""\n        self.initialized = True\n\n    def shutdown(self) -> None:\n        """Release resources - override in subclasses."""\n        self.initialized = False\n\n    @abstractmethod\n    def run(self, *args, **kwargs) -> Any:\n        """Execute the primary operation."""\n        raise NotImplementedError\n'})}),"\n",(0,s.jsx)(n.h3,{id:"interfaces",children:"Interfaces"}),"\n",(0,s.jsxs)(n.p,{children:["Located in ",(0,s.jsx)(n.code,{children:"ai_core/interfaces/"}),", these define the contracts for various component types:"]}),"\n",(0,s.jsx)(n.h4,{id:"agentcomponent",children:"AgentComponent"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_core.interfaces.agent import AgentComponent, AgentInput, AgentOutput\n\nclass AgentInput:\n    """Input DTO for agents."""\n    input_text: str\n    context: Dict[str, Any]\n\nclass AgentOutput:\n    """Output DTO for agents."""\n    output_text: str\n    metadata: Dict[str, Any]\n    output: Optional[Any]  # Structured output when output_type is used\n\nclass AgentComponent(BaseComponent, ABC):\n    """Abstract agent interface with streaming and async support."""\n\n    def run(input_data, deps=None, model_settings=None) -> AgentOutput\n    async def arun(input_data, deps=None, model_settings=None) -> AgentOutput\n    def stream(input_data, deps=None) -> Iterator[str]\n    async def astream(input_data, deps=None) -> AsyncIterator[str]\n'})}),"\n",(0,s.jsx)(n.h4,{id:"toolcomponent",children:"ToolComponent"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_core.interfaces.tool import ToolComponent\n\nclass ToolComponent(BaseComponent, ABC):\n    """\n    Base class for callable tools, compatible with Pydantic AI.\n\n    Attributes:\n        name: Unique tool name\n        description: Human-readable description\n        function: The callable function\n        json_schema: JSON schema for parameters\n        takes_ctx: Whether tool takes context as first argument\n    """\n\n    @classmethod\n    def from_function(cls, function, name, description, json_schema, takes_ctx=False):\n        """Create a ToolComponent from a standalone function."""\n'})}),"\n",(0,s.jsx)(n.h4,{id:"toolregistry",children:"ToolRegistry"}),"\n",(0,s.jsx)(n.p,{children:"Central registry for managing tools across the system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_core.interfaces.tool_registry import ToolRegistry, ToolMode\n\nregistry = ToolRegistry()\n\n# Register a direct tool\nregistry.register_direct_tool(\n    name="calculate",\n    function=lambda x, y: x + y,\n    description="Add two numbers",\n    json_schema={...},\n    category="math",\n    tags=["arithmetic"]\n)\n\n# Register an MCP tool\nregistry.register_mcp_tool(\n    name="search",\n    mcp_server="search_server",\n    mcp_tool_name="web_search",\n    description="Search the web"\n)\n\n# Query tools\ntools = registry.get_tools_by_category("math")\ntools = registry.get_tools_by_tag("arithmetic")\ntools = registry.get_tools_by_mode(ToolMode.DIRECT)\n'})}),"\n",(0,s.jsx)(n.h4,{id:"llmgateway",children:"LLMGateway"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_core.interfaces.llm_gateway import LLMGateway, LLMRequest, LLMResponse\n\nclass LLMRequest(BaseModel):\n    system_prompt: Optional[str]\n    user_prompt: str\n    temperature: Optional[float]\n    max_tokens: Optional[int]\n    extra_params: Dict[str, Any]\n\nclass LLMResponse(BaseModel):\n    content: str\n    parsed_content: Optional[Any]\n    usage: Optional[Dict[str, Any]]\n    model: Optional[str]\n    finish_reason: Optional[str]\n\nclass LLMGateway(BaseComponent, ABC):\n    """Abstract LLM gateway for provider-agnostic LLM interactions."""\n\n    def generate(request: LLMRequest) -> LLMResponse\n    def generate_structured(request: LLMRequest, schema: Type[T]) -> T\n    def stream(request: LLMRequest) -> Iterator[str]\n    def count_tokens(text: str, model: str = None) -> int\n'})}),"\n",(0,s.jsx)(n.h4,{id:"vectorstore",children:"VectorStore"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_core.interfaces.vectorstore import VectorStore\n\nclass VectorStore(BaseComponent, ABC):\n    """Abstract vector store interface."""\n\n    def add(vectors: List[List[float]], metadatas: List[Dict]) -> Any\n    def query(vector: List[float], top_k: int = 5) -> List[Dict]\n    def count() -> Optional[int]\n'})}),"\n",(0,s.jsx)(n.h4,{id:"loader",children:"Loader"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_core.interfaces.loader import Loader\n\nclass Loader(BaseComponent, ABC):\n    """Abstract document loader interface."""\n\n    def load_as_text(source: Union[str, Path]) -> str\n    def load_as_chunks(source: Union[str, Path]) -> List[str]\n    def load_as_nodes(source, source_id=None, custom_metadata=None) -> List[Node]\n    def load_as_vsfile(file_path, custom_metadata=None) -> VSFile\n'})}),"\n",(0,s.jsx)(n.h3,{id:"tracking-system",children:"Tracking System"}),"\n",(0,s.jsx)(n.p,{children:"Built-in input/output tracking for debugging and evaluation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from rakam_systems_core.tracking import TrackingManager, track_method, TrackingMixin\n\nclass MyAgent(TrackingMixin, BaseAgent):\n    @track_method()\n    async def arun(self, input_data, deps=None):\n        return await super().arun(input_data, deps)\n\n# Enable tracking\nagent.enable_tracking(output_dir=\"./tracking\")\n\n# Export tracking data\nagent.export_tracking_data(format='csv')\nagent.export_tracking_data(format='json')\n\n# Get statistics\nstats = agent.get_tracking_statistics()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"configuration-loader",children:"Configuration Loader"}),"\n",(0,s.jsx)(n.p,{children:"Load agent configurations from YAML files:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_core.config_loader import ConfigurationLoader\n\nloader = ConfigurationLoader()\nconfig = loader.load_from_yaml("agent_config.yaml")\n\n# Create agents from config\nagent = loader.create_agent("my_agent", config)\nall_agents = loader.create_all_agents(config)\n\n# Get tool registry\nregistry = loader.get_tool_registry(config)\n\n# Validate configuration\nis_valid, errors = loader.validate_config("config.yaml")\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.h2,{id:"-agent-package-rakam-systems-agent",children:["\ud83e\udd16 Agent Package (",(0,s.jsx)(n.code,{children:"rakam-systems-agent"}),")"]}),"\n",(0,s.jsxs)(n.p,{children:["The agent package provides AI agent implementations powered by Pydantic AI. Install with ",(0,s.jsx)(n.code,{children:"pip install rakam-systems-agent"})," (requires core)."]}),"\n",(0,s.jsx)(n.h3,{id:"baseagent",children:"BaseAgent"}),"\n",(0,s.jsx)(n.p,{children:"The main agent implementation using Pydantic AI:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_agent import BaseAgent\nfrom rakam_systems_core.interfaces.agent import AgentInput, AgentOutput, ModelSettings\n\nagent = BaseAgent(\n    name="my_agent",\n    model="openai:gpt-4o",\n    system_prompt="You are a helpful assistant.",\n    tools=[my_tool],  # Optional tools\n    output_type=MyOutputModel,  # Optional structured output\n    enable_tracking=True  # Optional tracking\n)\n\n# Async inference (required for Pydantic AI)\nresult = await agent.arun("What is AI?")\nprint(result.output_text)\n\n# With dependencies\nresult = await agent.arun("Hello", deps={"user_id": "123"})\n\n# With model settings\nsettings = ModelSettings(temperature=0.5, max_tokens=1000)\nresult = await agent.arun("Explain quantum computing", model_settings=settings)\n\n# Streaming\nasync for chunk in agent.astream("Tell me a story"):\n    print(chunk, end="")\n'})}),"\n",(0,s.jsx)(n.h4,{id:"dynamic-system-prompts",children:"Dynamic System Prompts"}),"\n",(0,s.jsx)(n.p,{children:"Dynamic system prompts allow you to inject context at runtime based on current state, user information, or external data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from datetime import date, datetime\nfrom pydantic_ai import RunContext\n\nagent = BaseAgent(\n    name="dynamic_agent",\n    model="openai:gpt-4o",\n    system_prompt="You are a helpful assistant."\n)\n\n# Method 1: Decorator syntax\n@agent.dynamic_system_prompt\ndef add_date() -> str:\n    """Add current date to system prompt."""\n    return f"Today\'s date is {date.today().strftime(\'%B %d, %Y\')}."\n\n@agent.dynamic_system_prompt\ndef add_user_context(ctx: RunContext[dict]) -> str:\n    """Add user-specific context from dependencies."""\n    if ctx.deps and "user_name" in ctx.deps:\n        return f"You are assisting {ctx.deps[\'user_name\']}."\n    return ""\n\n# Method 2: Direct registration\ndef add_time_context() -> str:\n    """Add current time to system prompt."""\n    return f"Current time: {datetime.now().strftime(\'%H:%M:%S\')}"\n\nagent.add_dynamic_system_prompt(add_time_context)\n\n# Method 3: Async dynamic prompts\n@agent.dynamic_system_prompt\nasync def fetch_external_context(ctx: RunContext[dict]) -> str:\n    """Fetch and add external context asynchronously."""\n    # Example: fetch from API or database\n    import asyncio\n    await asyncio.sleep(0.1)\n    return "Additional context from external source."\n\n# Usage with dependencies\nresult = await agent.arun(\n    "What day is it?",\n    deps={"user_name": "Alice", "user_id": "123"}\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"llm-gateways",children:"LLM Gateways"}),"\n",(0,s.jsx)(n.h4,{id:"openai-gateway",children:"OpenAI Gateway"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_agent import OpenAIGateway, LLMRequest\n\ngateway = OpenAIGateway(\n    model="gpt-4o",\n    api_key="...",  # Or use OPENAI_API_KEY env var\n    default_temperature=0.7\n)\n\n# Text generation\nrequest = LLMRequest(\n    system_prompt="You are a helpful assistant",\n    user_prompt="What is AI?",\n    temperature=0.7\n)\nresponse = gateway.generate(request)\nprint(response.content)\n\n# Structured output\nfrom pydantic import BaseModel\n\nclass Answer(BaseModel):\n    answer: str\n    confidence: float\n\nresult = gateway.generate_structured(request, Answer)\nprint(result.answer, result.confidence)\n\n# Streaming\nfor chunk in gateway.stream(request):\n    print(chunk, end="")\n\n# Token counting\ntoken_count = gateway.count_tokens("Hello, world!")\n'})}),"\n",(0,s.jsx)(n.h4,{id:"mistral-gateway",children:"Mistral Gateway"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_agent import MistralGateway\n\ngateway = MistralGateway(\n    model="mistral-large-latest",\n    api_key="..."  # Or use MISTRAL_API_KEY env var\n)\n'})}),"\n",(0,s.jsx)(n.h4,{id:"gateway-factory",children:"Gateway Factory"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_agent import LLMGatewayFactory, get_llm_gateway\n\n# Using factory\ngateway = LLMGatewayFactory.create(\n    provider="openai",\n    model="gpt-4o",\n    api_key="..."\n)\n\n# Using convenience function\ngateway = get_llm_gateway(provider="openai", model="gpt-4o")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"chat-history",children:"Chat History"}),"\n",(0,s.jsx)(n.h4,{id:"json-chat-history",children:"JSON Chat History"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_agent.components.chat_history import JSONChatHistory\n\nhistory = JSONChatHistory(config={"storage_path": "./chat_history.json"})\n\n# Add messages\nhistory.add_message("chat123", {"role": "user", "content": "Hello!"})\nhistory.add_message("chat123", {"role": "assistant", "content": "Hi there!"})\n\n# Get history\nmessages = history.get_chat_history("chat123")\nreadable = history.get_readable_chat_history("chat123")\n\n# Pydantic AI integration\nmessage_history = history.get_message_history("chat123")\nresult = await agent.run("Hello", message_history=message_history)\nhistory.save_messages("chat123", result.all_messages())\n\n# Manage chats\nall_chats = history.get_all_chat_ids()\nhistory.delete_chat_history("chat123")\nhistory.clear_all()\n'})}),"\n",(0,s.jsx)(n.h4,{id:"sql-chat-history-sqlite",children:"SQL Chat History (SQLite)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_agent.components.chat_history import SQLChatHistory\n\nhistory = SQLChatHistory(config={"db_path": "./chat_history.db"})\n\n# Same API as JSON Chat History\nhistory.add_message("chat123", {"role": "user", "content": "Hello!"})\nhistory.add_message("chat123", {"role": "assistant", "content": "Hi there!"})\n\n# Get history\nmessages = history.get_chat_history("chat123")\n\n# Pydantic AI integration\nmessage_history = history.get_message_history("chat123")\nresult = await agent.run("Hello", message_history=message_history)\nhistory.save_messages("chat123", result.all_messages())\n'})}),"\n",(0,s.jsx)(n.h4,{id:"postgresql-chat-history",children:"PostgreSQL Chat History"}),"\n",(0,s.jsx)(n.p,{children:"For production deployments with PostgreSQL-backed storage:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_agent.components.chat_history import PostgresChatHistory\n\n# Configuration\nhistory = PostgresChatHistory(config={\n    "host": "localhost",\n    "port": 5432,\n    "database": "chat_db",\n    "user": "postgres",\n    "password": "postgres"\n})\n\n# Or use environment variables (POSTGRES_HOST, POSTGRES_PORT, etc.)\nhistory = PostgresChatHistory()\n\n# Same API as other chat history backends\nhistory.add_message("chat123", {"role": "user", "content": "Hello!"})\nhistory.add_message("chat123", {"role": "assistant", "content": "Hi there!"})\n\n# Get history\nmessages = history.get_chat_history("chat123")\nreadable = history.get_readable_chat_history("chat123")\n\n# Pydantic AI integration\nmessage_history = history.get_message_history("chat123")\nresult = await agent.run("Hello", message_history=message_history)\nhistory.save_messages("chat123", result.all_messages())\n\n# Manage chats\nall_chats = history.get_all_chat_ids()\nhistory.delete_chat_history("chat123")\nhistory.clear_all()\n\n# Cleanup\nhistory.shutdown()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.h2,{id:"-vectorstore-package-rakam-systems-vectorstore",children:["\ud83d\udd0d Vectorstore Package (",(0,s.jsx)(n.code,{children:"rakam-systems-vectorstore"}),")"]}),"\n",(0,s.jsxs)(n.p,{children:["The vectorstore package provides vector database solutions and document processing. Install with ",(0,s.jsx)(n.code,{children:"pip install rakam-systems-vectorstore"})," (requires core)."]}),"\n",(0,s.jsx)(n.h3,{id:"core-data-structures",children:"Core Data Structures"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore.core import Node, NodeMetadata, VSFile\n\n# VSFile - Represents a document source\nvsfile = VSFile(file_path="/path/to/document.pdf")\nprint(vsfile.uuid, vsfile.file_name, vsfile.mime_type)\n\n# NodeMetadata - Metadata for document chunks\nmetadata = NodeMetadata(\n    source_file_uuid=str(vsfile.uuid),\n    position=0,  # Page number or chunk position\n    custom={"author": "John", "date": "2024-01-01"}\n)\n\n# Node - A chunk with content and metadata\nnode = Node(content="Document content here...", metadata=metadata)\nnode.embedding = [0.1, 0.2, 0.3, ...]  # Set after embedding\n'})}),"\n",(0,s.jsx)(n.h3,{id:"configurablepgvectorstore",children:"ConfigurablePgVectorStore"}),"\n",(0,s.jsx)(n.p,{children:"Enhanced PostgreSQL vector store with full configuration support:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig\n\n# From configuration object\nconfig = VectorStoreConfig()\nstore = ConfigurablePgVectorStore(config=config)\n\n# From YAML file\nstore = ConfigurablePgVectorStore(config="vectorstore_config.yaml")\n\n# From dictionary\nstore = ConfigurablePgVectorStore(config={\n    "name": "my_store",\n    "embedding": {\n        "model_type": "sentence_transformer",\n        "model_name": "Snowflake/snowflake-arctic-embed-m"\n    },\n    "search": {\n        "similarity_metric": "cosine",\n        "enable_hybrid_search": True,\n        "hybrid_alpha": 0.7\n    }\n})\n\n# Setup (initializes embedding model, database tables)\nstore.setup()\n\n# Add documents\nstore.add_nodes(nodes)\nstore.add_vsfile(vsfile)\n\n# Vector search (semantic similarity)\nresults = store.search("What is machine learning?", top_k=5)\n\n# Hybrid search (combines vector + keyword search)\nresults = store.hybrid_search("machine learning", top_k=10, alpha=0.7)\n\n# Keyword search (full-text search with BM25 or ts_rank)\nresults = store.keyword_search(\n    query="machine learning algorithms",\n    top_k=10,\n    ranking_algorithm="bm25",  # or "ts_rank"\n    k1=1.2,  # BM25 parameter\n    b=0.75   # BM25 parameter\n)\n\n# Update vectors\nstore.update_vector(node_id, new_embedding)\n\n# Cleanup\nstore.shutdown()\n'})}),"\n",(0,s.jsx)(n.h4,{id:"keyword-search",children:"Keyword Search"}),"\n",(0,s.jsx)(n.p,{children:"Full-text search using PostgreSQL's built-in capabilities with BM25 or ts_rank ranking:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig\n\nconfig = VectorStoreConfig(\n    search={\n        "keyword_ranking_algorithm": "bm25",  # or "ts_rank"\n        "keyword_k1": 1.2,  # BM25 k1 parameter\n        "keyword_b": 0.75   # BM25 b parameter\n    }\n)\n\nstore = ConfigurablePgVectorStore(config=config)\nstore.setup()\n\n# Keyword search with BM25 ranking\nresults = store.keyword_search(\n    query="machine learning neural networks",\n    top_k=10,\n    ranking_algorithm="bm25"\n)\n\n# Keyword search with ts_rank\nresults = store.keyword_search(\n    query="deep learning",\n    top_k=10,\n    ranking_algorithm="ts_rank"\n)\n\n# Results include content and relevance scores\nfor result in results:\n    print(f"Score: {result[\'score\']:.4f}")\n    print(f"Content: {result[\'content\'][:200]}...")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Ranking Algorithms:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"BM25"}),": Best Match 25, probabilistic ranking function","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"k1"}),": Term frequency saturation parameter (default: 1.2)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"b"}),": Length normalization parameter (default: 0.75)"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ts_rank"}),": PostgreSQL's text search ranking function","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Weights different parts of documents differently"}),"\n",(0,s.jsx)(n.li,{children:"Good for structured documents"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"multi-model-support",children:"Multi-Model Support"}),"\n",(0,s.jsx)(n.p,{children:"Each embedding model automatically gets dedicated tables:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Using different models - each gets its own tables\nstore_minilm = ConfigurablePgVectorStore(config=config_minilm)\nstore_mpnet = ConfigurablePgVectorStore(config=config_mpnet)\n\n# Table names are based on model names:\n# - application_nodeentry_all_minilm_l6_v2\n# - application_nodeentry_snowflake_arctic_embed_m\n\n# Disable model-specific tables if needed (not recommended)\nstore = ConfigurablePgVectorStore(\n    config=config,\n    use_dimension_specific_tables=False\n)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"configurableembeddings",children:"ConfigurableEmbeddings"}),"\n",(0,s.jsx)(n.p,{children:"Multi-backend embedding model with unified interface:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore import ConfigurableEmbeddings, create_embedding_model\n\n# Using Sentence Transformers (local)\nembeddings = ConfigurableEmbeddings(config={\n    "model_type": "sentence_transformer",\n    "model_name": "Snowflake/snowflake-arctic-embed-m",\n    "batch_size": 128,\n    "normalize": True\n})\n\n# Using OpenAI (with batch processing)\nembeddings = ConfigurableEmbeddings(config={\n    "model_type": "openai",\n    "model_name": "text-embedding-3-small",\n    "api_key": "...",  # Or use OPENAI_API_KEY\n    "batch_size": 100   # OpenAI supports larger batches\n})\n\n# Using Cohere\nembeddings = ConfigurableEmbeddings(config={\n    "model_type": "cohere",\n    "model_name": "embed-english-v3.0",\n    "api_key": "..."  # Or use COHERE_API_KEY\n})\n\n# Using HuggingFace models with authentication\nembeddings = ConfigurableEmbeddings(config={\n    "model_type": "sentence_transformer",\n    "model_name": "private/model-name",\n    # Uses HUGGINGFACE_TOKEN environment variable\n})\n\nembeddings.setup()\n\n# Encode texts with automatic batch processing\nvectors = embeddings.run(["Hello world", "How are you?"])\n\n# Encode large datasets with progress tracking\nlarge_texts = ["text" + str(i) for i in range(10000)]\nvectors = embeddings.run(large_texts)  # Shows progress bar\n\n# Encode queries (optimized for single texts)\nquery_vector = embeddings.encode_query("What is AI?")\n\n# Encode documents (optimized for batches)\ndoc_vectors = embeddings.encode_documents(documents)\n\n# Get dimension\ndim = embeddings.embedding_dimension\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Performance Features:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Automatic batch processing with progress tracking"}),"\n",(0,s.jsx)(n.li,{children:"Memory optimization with garbage collection"}),"\n",(0,s.jsx)(n.li,{children:"Token truncation for oversized texts"}),"\n",(0,s.jsx)(n.li,{children:"Mini-batch processing for large datasets"}),"\n",(0,s.jsx)(n.li,{children:"CUDA memory management for GPU acceleration"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"factory-function",children:"Factory Function"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'embeddings = create_embedding_model(\n    model_type="sentence_transformer",\n    model_name="all-MiniLM-L6-v2",\n    batch_size=64\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"adaptiveloader",children:"AdaptiveLoader"}),"\n",(0,s.jsx)(n.p,{children:"Automatically detects and processes various file types:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore import AdaptiveLoader, create_adaptive_loader\n\nloader = AdaptiveLoader(config={\n    "encoding": "utf-8",\n    "chunk_size": 512,\n    "chunk_overlap": 50\n})\n\n# Supported file types:\n# - Text: .txt, .text\n# - Markdown: .md, .markdown\n# - Documents: .pdf, .docx, .doc, .odt\n# - Email: .eml, .msg\n# - Data: .json, .csv, .tsv, .xlsx, .xls\n# - HTML: .html, .htm, .xhtml\n# - Code: .py, .js, .ts, .java, .cpp, .go, .rs, .rb, etc.\n\n# Load as single text\ntext = loader.load_as_text("document.pdf")\n\n# Load as chunks\nchunks = loader.load_as_chunks("document.pdf")\n\n# Load as nodes (with metadata)\nnodes = loader.load_as_nodes("document.pdf", custom_metadata={"category": "science"})\n\n# Load as VSFile\nvsfile = loader.load_as_vsfile("document.pdf")\n\n# Also handles raw text\nchunks = loader.load_as_chunks("This is raw text content...")\n'})}),"\n",(0,s.jsx)(n.h4,{id:"factory-function-1",children:"Factory Function"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"loader = create_adaptive_loader(\n    chunk_size=1024,\n    chunk_overlap=100,\n    encoding='utf-8'\n)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"specialized-loaders",children:"Specialized Loaders"}),"\n",(0,s.jsxs)(n.p,{children:["Located in ",(0,s.jsx)(n.code,{children:"ai_vectorstore/components/loader/"}),":"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Loader"}),(0,s.jsx)(n.th,{children:"File Types"}),(0,s.jsx)(n.th,{children:"Features"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"PdfLoader"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:".pdf"})}),(0,s.jsx)(n.td,{children:"Advanced PDF processing with Docling, image extraction, table detection"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"PdfLoaderLight"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:".pdf"})}),(0,s.jsx)(n.td,{children:"Lightweight PDF processing with pymupdf4llm, markdown conversion, image extraction"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"DocLoader"})}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:".docx"}),", ",(0,s.jsx)(n.code,{children:".doc"})]}),(0,s.jsx)(n.td,{children:"Microsoft Word documents, image extraction"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"OdtLoader"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:".odt"})}),(0,s.jsx)(n.td,{children:"OpenDocument Text, image extraction"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"MdLoader"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:".md"})}),(0,s.jsx)(n.td,{children:"Markdown with structure preservation, YAML frontmatter"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"HtmlLoader"})}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:".html"}),", ",(0,s.jsx)(n.code,{children:".htm"})]}),(0,s.jsx)(n.td,{children:"HTML parsing and text extraction"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"EmlLoader"})}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:".eml"}),", ",(0,s.jsx)(n.code,{children:".msg"})]}),(0,s.jsx)(n.td,{children:"Email files (loaded as single nodes)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"TabularLoader"})}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:".csv"}),", ",(0,s.jsx)(n.code,{children:".tsv"}),", ",(0,s.jsx)(n.code,{children:".xlsx"})]}),(0,s.jsx)(n.td,{children:"Tabular data processing, preserves column structure"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"CodeLoader"})}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:".py"}),", ",(0,s.jsx)(n.code,{children:".js"}),", etc."]}),(0,s.jsx)(n.td,{children:"Code-aware chunking with syntax preservation"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"pdfloaderlight",children:"PdfLoaderLight"}),"\n",(0,s.jsx)(n.p,{children:"A lightweight alternative to PdfLoader using pymupdf4llm for efficient PDF processing:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore.components.loader import PdfLoaderLight\n\nloader = PdfLoaderLight(\n    name="pdf_loader_light",\n    config={\n        "chunk_size": 512,\n        "chunk_overlap": 50,\n        "extract_images": True,\n        "image_path": "./extracted_images",\n        "page_chunks": True,  # Create one chunk per page\n        "write_images": True  # Save images to disk\n    }\n)\n\n# Load as markdown\nmarkdown_text = loader.load_as_text("document.pdf")\n\n# Load as chunks (one per page or custom chunking)\nchunks = loader.load_as_chunks("document.pdf")\n\n# Load as nodes with metadata\nnodes = loader.load_as_nodes("document.pdf")\n\n# Access extracted images\nimage_paths = loader.get_image_paths()\nfor img_id, img_path in image_paths.items():\n    print(f"Image {img_id}: {img_path}")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Key Features:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Fast PDF to markdown conversion"}),"\n",(0,s.jsx)(n.li,{children:"Optional image extraction and saving"}),"\n",(0,s.jsx)(n.li,{children:"Page-aware chunking"}),"\n",(0,s.jsx)(n.li,{children:"Thread-safe operations"}),"\n",(0,s.jsx)(n.li,{children:"Lower memory footprint than PdfLoader"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"image-extraction-support",children:"Image Extraction Support"}),"\n",(0,s.jsx)(n.p,{children:"Multiple loaders now support image extraction:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore.components.loader import DocLoader, OdtLoader, PdfLoaderLight\n\n# DocLoader with image extraction\ndoc_loader = DocLoader(config={\n    "extract_images": True,\n    "image_path": "./doc_images"\n})\nnodes = doc_loader.load_as_nodes("document.docx")\n\n# Access extracted images\nfor img_id, img_path in doc_loader.get_image_paths().items():\n    print(f"Image {img_id}: {img_path}")\n\n# OdtLoader with image extraction\nodt_loader = OdtLoader(config={\n    "extract_images": True,\n    "image_path": "./odt_images"\n})\nnodes = odt_loader.load_as_nodes("document.odt")\n\n# PdfLoaderLight with image extraction\npdf_loader = PdfLoaderLight(config={\n    "extract_images": True,\n    "image_path": "./pdf_images",\n    "write_images": True\n})\nnodes = pdf_loader.load_as_nodes("document.pdf")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"textchunker",children:"TextChunker"}),"\n",(0,s.jsx)(n.p,{children:"Sentence-based text chunking using Chonkie:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore.components.chunker import TextChunker, create_text_chunker\n\nchunker = TextChunker(\n    chunk_size=512,        # Tokens per chunk\n    chunk_overlap=50,      # Overlap in tokens\n    min_sentences_per_chunk=1,\n    tokenizer="character"  # Or "gpt2", HuggingFace tokenizer\n)\n\nchunks = chunker.chunk_text("Long document text...")\n# Returns: [{"text": "...", "token_count": 100, "start_index": 0, "end_index": 500}, ...]\n\n# Process multiple documents\nall_chunks = chunker.run(["doc1 text", "doc2 text"])\n'})}),"\n",(0,s.jsx)(n.h3,{id:"advancedchunker",children:"AdvancedChunker"}),"\n",(0,s.jsx)(n.p,{children:"Advanced document chunking using Docling for context-aware chunking with heading preservation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore.components.chunker import AdvancedChunker\n\nchunker = AdvancedChunker(\n    name="advanced_chunker",\n    config={\n        "max_tokens": 512,           # Maximum tokens per chunk\n        "merge_peers": True,          # Merge peer sections\n        "min_chunk_tokens": 64,       # Minimum tokens per chunk\n        "filter_toc": True,           # Filter table of contents\n        "include_heading_markers": True  # Include markdown headings\n    }\n)\n\n# Chunk text with context preservation\nchunks = chunker.chunk_text("Document text with headings...")\n\n# Each chunk includes:\n# - text: The chunk content\n# - token_count: Number of tokens\n# - start_index: Starting position\n# - end_index: Ending position\n# - heading_context: Hierarchical heading information\n\n# Process with heading markers\nchunker_with_markers = AdvancedChunker(config={\n    "include_heading_markers": True\n})\nchunks = chunker_with_markers.chunk_text("""\n# Main Title\n## Section 1\nContent here...\n## Section 2\nMore content...\n""")\n# Output includes markdown-style headings in chunks\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Key Features:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Context-aware chunking with heading hierarchy"}),"\n",(0,s.jsx)(n.li,{children:"Automatic merging of small chunks"}),"\n",(0,s.jsx)(n.li,{children:"Table of contents filtering"}),"\n",(0,s.jsx)(n.li,{children:"Image and table fragment handling"}),"\n",(0,s.jsx)(n.li,{children:"Markdown heading markers support"}),"\n",(0,s.jsx)(n.li,{children:"Configurable token limits and merging behavior"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"logging-utilities",children:"Logging Utilities"}),"\n",(0,s.jsx)(n.p,{children:"The core package includes logging utilities:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_tools.utils import logging\n\nlogger = logging.getLogger(__name__)\nlogger.info("Processing document...")\nlogger.debug("Detailed debug info")\nlogger.error("An error occurred")\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"\ufe0f-configuration-system",children:"\u2699\ufe0f Configuration System"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"The Core Advantage: Configuration Without Code Changes"})}),"\n",(0,s.jsx)(n.p,{children:"Rakam Systems embraces a configuration-first approach, allowing you to modify agent behavior, vector store settings, and system parameters without touching your application code. This provides:"}),"\n",(0,s.jsx)(n.h3,{id:"benefits-of-configuration-first-design",children:"Benefits of Configuration-First Design"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Rapid Iteration"}),": Test different models, prompts, or parameters instantly"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Environment Management"}),": Use different configs for dev/staging/production"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"A/B Testing"}),": Compare performance of different settings by swapping configs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Team Collaboration"}),": Non-developers can tune prompts and parameters"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cost Optimization"}),": Switch to cheaper models for development, expensive for production"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Risk Reduction"}),": Change behavior without code deployment risks"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"real-world-scenarios",children:"Real-World Scenarios"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Scenario 1: Model Optimization"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# Week 1: Start with GPT-4o\nmodel: openai:gpt-4o\ntemperature: 0.7\n\n# Week 2: Test GPT-4o-mini for cost savings (no code changes!)\nmodel: openai:gpt-4o-mini\ntemperature: 0.7\n\n# Week 3: Back to GPT-4o for production (just revert config)\nmodel: openai:gpt-4o\ntemperature: 0.5  # Also tuned temperature\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Scenario 2: Search Algorithm Testing"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# Current: Using BM25 for keyword search\nsearch:\n  keyword_ranking_algorithm: bm25\n\n# Test: Try ts_rank (just update config, no code changes!)\nsearch:\n  keyword_ranking_algorithm: ts_rank\n\n# Decide: Compare results and keep the best one\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Scenario 3: Multi-Environment Deployment"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Application code (never changes)\nimport os\nconfig_file = os.getenv("AGENT_CONFIG", "config/agent_config.yaml")\nconfig = loader.load_from_yaml(config_file)\nagent = loader.create_agent("my_agent", config)\n\n# Dev: AGENT_CONFIG=config/agent_config_dev.yaml\n# Staging: AGENT_CONFIG=config/agent_config_staging.yaml\n# Prod: AGENT_CONFIG=config/agent_config_prod.yaml\n'})}),"\n",(0,s.jsx)(n.h2,{id:"\ufe0f-configuration-system-details",children:"\u2699\ufe0f Configuration System Details"}),"\n",(0,s.jsx)(n.h3,{id:"vectorstoreconfig",children:"VectorStoreConfig"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore.config import (\n    VectorStoreConfig,\n    EmbeddingConfig,\n    DatabaseConfig,\n    SearchConfig,\n    IndexConfig,\n    load_config\n)\n\n# Programmatic configuration\nconfig = VectorStoreConfig(\n    name="my_vectorstore",\n    embedding=EmbeddingConfig(\n        model_type="sentence_transformer",\n        model_name="Snowflake/snowflake-arctic-embed-m",\n        batch_size=128,\n        normalize=True\n    ),\n    database=DatabaseConfig(\n        host="localhost",\n        port=5432,\n        database="vectorstore_db",\n        user="postgres",\n        password="postgres"\n    ),\n    search=SearchConfig(\n        similarity_metric="cosine",\n        default_top_k=5,\n        enable_hybrid_search=True,\n        hybrid_alpha=0.7\n    ),\n    index=IndexConfig(\n        chunk_size=512,\n        chunk_overlap=50,\n        batch_insert_size=10000\n    )\n)\n\n# From YAML file\nconfig = VectorStoreConfig.from_yaml("config.yaml")\n\n# From JSON file\nconfig = VectorStoreConfig.from_json("config.json")\n\n# From dictionary\nconfig = VectorStoreConfig.from_dict(config_dict)\n\n# Validation\nconfig.validate()\n\n# Save configuration\nconfig.save_yaml("output_config.yaml")\nconfig.save_json("output_config.json")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"yaml-configuration-example",children:"YAML Configuration Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# vectorstore_config.yaml\nname: production_vectorstore\n\nembedding:\n  model_type: sentence_transformer\n  model_name: Snowflake/snowflake-arctic-embed-m\n  batch_size: 128\n  normalize: true\n\ndatabase:\n  host: localhost\n  port: 5432\n  database: vectorstore_db\n  user: postgres\n  password: postgres\n\nsearch:\n  similarity_metric: cosine\n  default_top_k: 5\n  enable_hybrid_search: true\n  hybrid_alpha: 0.7\n\nindex:\n  chunk_size: 512\n  chunk_overlap: 50\n  batch_insert_size: 10000\n\nenable_caching: true\ncache_size: 1000\nenable_logging: true\nlog_level: INFO\n"})}),"\n",(0,s.jsx)(n.h3,{id:"agent-configuration-example",children:"Agent Configuration Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# agent_config.yaml\nagents:\n  my_agent:\n    name: my_agent\n    llm_config:\n      model: openai:gpt-4o\n      temperature: 0.7\n      max_tokens: 2000\n      parallel_tool_calls: true\n    prompt_config: default_prompt\n    tools:\n      - search_tool\n      - calculator\n    deps_type: myapp.models.AgentDeps\n    output_type:\n      name: AgentOutput\n      fields:\n        answer:\n          type: str\n          description: The answer\n        confidence:\n          type: float\n          description: Confidence score\n    enable_tracking: true\n\nprompts:\n  default_prompt:\n    system_prompt: |\n      You are a helpful AI assistant.\n      Always provide accurate and helpful responses.\n\ntools:\n  search_tool:\n    name: search_tool\n    type: direct\n    module: myapp.tools\n    function: search\n    description: Search for information\n    json_schema:\n      type: object\n      properties:\n        query:\n          type: string\n      required: [query]\n\n  calculator:\n    name: calculator\n    type: direct\n    module: myapp.tools\n    function: calculate\n    description: Perform calculations\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-quick-start-examples",children:"\ud83d\ude80 Quick Start Examples"}),"\n",(0,s.jsx)(n.h3,{id:"basic-agent",children:"Basic Agent"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_systems_agent import BaseAgent\n\nasync def main():\n    agent = BaseAgent(\n        name="assistant",\n        model="openai:gpt-4o",\n        system_prompt="You are a helpful assistant."\n    )\n\n    result = await agent.arun("What is the capital of France?")\n    print(result.output_text)\n\nasyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.h3,{id:"agent-with-tools",children:"Agent with Tools"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_systems_agent import BaseAgent\nfrom rakam_systems_core.interfaces.tool import ToolComponent\n\ndef get_weather(city: str) -> str:\n    return f"The weather in {city} is sunny, 25\xb0C"\n\nweather_tool = ToolComponent.from_function(\n    function=get_weather,\n    name="get_weather",\n    description="Get the current weather for a city",\n    json_schema={\n        "type": "object",\n        "properties": {"city": {"type": "string"}},\n        "required": ["city"]\n    }\n)\n\nasync def main():\n    agent = BaseAgent(\n        name="weather_assistant",\n        model="openai:gpt-4o",\n        system_prompt="You help users with weather information.",\n        tools=[weather_tool]\n    )\n\n    result = await agent.arun("What\'s the weather in Paris?")\n    print(result.output_text)\n\nasyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.h3,{id:"document-search-pipeline",children:"Document Search Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore import (\n    ConfigurablePgVectorStore,\n    VectorStoreConfig,\n    AdaptiveLoader\n)\n\n# Configure vector store\nconfig = VectorStoreConfig()\nstore = ConfigurablePgVectorStore(config=config)\nstore.setup()\n\n# Load documents\nloader = AdaptiveLoader(config={"chunk_size": 512})\nnodes = loader.load_as_nodes("documents/research_paper.pdf")\n\n# Add to vector store\nstore.add_nodes(nodes)\n\n# Search\nresults = store.search("What are the main findings?", top_k=5)\nfor result in results:\n    print(f"Score: {result[\'score\']:.4f}")\n    print(f"Content: {result[\'content\'][:200]}...")\n    print("---")\n\nstore.shutdown()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"full-rag-pipeline",children:"Full RAG Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_systems_agent import BaseAgent\nfrom rakam_systems_vectorstore import ConfigurablePgVectorStore, AdaptiveLoader, VectorStoreConfig\nfrom rakam_systems_core.interfaces.tool import ToolComponent\n\n# Setup vector store\nconfig = VectorStoreConfig()\nstore = ConfigurablePgVectorStore(config=config)\nstore.setup()\n\n# Index documents\nloader = AdaptiveLoader()\nfor doc_path in ["doc1.pdf", "doc2.pdf", "doc3.pdf"]:\n    nodes = loader.load_as_nodes(doc_path)\n    store.add_nodes(nodes)\n\n# Create search tool\ndef search_documents(query: str, top_k: int = 5) -> str:\n    results = store.search(query, top_k=top_k)\n    return "\\n\\n".join([r[\'content\'] for r in results])\n\nsearch_tool = ToolComponent.from_function(\n    function=search_documents,\n    name="search_documents",\n    description="Search the document database",\n    json_schema={\n        "type": "object",\n        "properties": {\n            "query": {"type": "string", "description": "Search query"},\n            "top_k": {"type": "integer", "description": "Number of results"}\n        },\n        "required": ["query"]\n    }\n)\n\n# Create RAG agent\nasync def main():\n    agent = BaseAgent(\n        name="rag_agent",\n        model="openai:gpt-4o",\n        system_prompt="""You are a helpful assistant with access to a document database.\n        Use the search_documents tool to find relevant information before answering questions.""",\n        tools=[search_tool]\n    )\n\n    result = await agent.arun("What are the key points from the documents?")\n    print(result.output_text)\n\nasyncio.run(main())\nstore.shutdown()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"environment-variables",children:"Environment Variables"}),"\n",(0,s.jsx)(n.p,{children:"The system supports the following environment variables:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Variable"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Used By"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"OPENAI_API_KEY"})}),(0,s.jsx)(n.td,{children:"OpenAI API key"}),(0,s.jsx)(n.td,{children:"OpenAIGateway, ConfigurableEmbeddings"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"MISTRAL_API_KEY"})}),(0,s.jsx)(n.td,{children:"Mistral API key"}),(0,s.jsx)(n.td,{children:"MistralGateway"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"COHERE_API_KEY"})}),(0,s.jsx)(n.td,{children:"Cohere API key"}),(0,s.jsx)(n.td,{children:"ConfigurableEmbeddings"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"POSTGRES_HOST"})}),(0,s.jsx)(n.td,{children:"PostgreSQL host"}),(0,s.jsx)(n.td,{children:"DatabaseConfig"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"POSTGRES_PORT"})}),(0,s.jsx)(n.td,{children:"PostgreSQL port"}),(0,s.jsx)(n.td,{children:"DatabaseConfig"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"POSTGRES_DB"})}),(0,s.jsx)(n.td,{children:"PostgreSQL database"}),(0,s.jsx)(n.td,{children:"DatabaseConfig"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"POSTGRES_USER"})}),(0,s.jsx)(n.td,{children:"PostgreSQL user"}),(0,s.jsx)(n.td,{children:"DatabaseConfig"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"POSTGRES_PASSWORD"})}),(0,s.jsx)(n.td,{children:"PostgreSQL password"}),(0,s.jsx)(n.td,{children:"DatabaseConfig"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-best-practices",children:"\u2705 Best Practices"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Always use context managers"})," or explicit ",(0,s.jsx)(n.code,{children:"setup()"}),"/",(0,s.jsx)(n.code,{children:"shutdown()"})," for proper resource management"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use configuration files"})," for production deployments instead of hardcoded values"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Enable tracking"})," during development for debugging and evaluation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use model-specific tables"})," (default) to prevent mixing incompatible vector spaces"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Batch operations"})," when processing large document collections"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use async methods"})," (",(0,s.jsx)(n.code,{children:"arun"}),", ",(0,s.jsx)(n.code,{children:"astream"}),") for agents as they are powered by Pydantic AI"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Validate configurations"})," before deployment using ",(0,s.jsx)(n.code,{children:"config.validate()"})," or ",(0,s.jsx)(n.code,{children:"loader.validate_config()"})]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-further-reading",children:"\ud83d\udcda Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Example configurations: ",(0,s.jsx)(n.code,{children:"examples/configs/"})]}),"\n",(0,s.jsxs)(n.li,{children:["Agent examples: ",(0,s.jsx)(n.code,{children:"examples/ai_agents_examples/"})]}),"\n",(0,s.jsxs)(n.li,{children:["Vector store examples: ",(0,s.jsx)(n.code,{children:"examples/ai_vectorstore_examples/"})]}),"\n",(0,s.jsxs)(n.li,{children:["Loader documentation: ",(0,s.jsx)(n.code,{children:"rakam-systems-vectorstore/src/rakam_systems_vectorstore/components/loader/docs/"})]}),"\n",(0,s.jsxs)(n.li,{children:["Architecture documentation: ",(0,s.jsx)(n.code,{children:"rakam-systems-vectorstore/src/rakam_systems_vectorstore/docs/ARCHITECTURE.md"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>i});var r=t(6540);const s={},a=r.createContext(s);function o(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);