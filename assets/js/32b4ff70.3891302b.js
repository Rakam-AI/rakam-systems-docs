"use strict";(self.webpackChunkrakam_systems_docs_portal=self.webpackChunkrakam_systems_docs_portal||[]).push([[4263],{5127(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"getting-started/components","title":"Rakam Systems Components Guide","description":"Welcome! This guide introduces the modular components of Rakam Systems, a framework for building robust, production-ready AI applications. Here, you\'ll find clear explanations and practical examples for using agents, vector stores, and configuration systems.","source":"@site/docs/getting-started/components.md","sourceDirName":"getting-started","slug":"/getting-started/components","permalink":"/rakam-systems-docs/getting-started/components","draft":false,"unlisted":false,"editUrl":"https://github.com/Rakam-AI/rakam_systems/edit/main/docs/docs/getting-started/components.md","tags":[],"version":"current","lastUpdatedBy":"Mohamed Bashar Touil","lastUpdatedAt":1771282847000,"frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Quick Start Guide","permalink":"/rakam-systems-docs/getting-started/quick_start"},"next":{"title":"Rakam\'s Development Guide","permalink":"/rakam-systems-docs/getting-started/development_guide"}}');var r=t(4848),a=t(8453);const i={},o="Rakam Systems Components Guide",c={},d=[{value:"\ud83d\udcd1 Table of Contents",id:"-table-of-contents",level:2},{value:"\ud83c\udfd7\ufe0f Architecture Overview",id:"\ufe0f-architecture-overview",level:2},{value:"Core Package (<code>rakam-systems-core</code>)",id:"core-package-rakam-systems-core",level:2},{value:"\ud83e\udd16 Agent Package (<code>rakam-systems-agent</code>)",id:"-agent-package-rakam-systems-agent",level:2},{value:"LLM Gateways",id:"llm-gateways",level:3},{value:"OpenAI Gateway",id:"openai-gateway",level:4},{value:"Mistral Gateway",id:"mistral-gateway",level:4},{value:"Gateway Factory",id:"gateway-factory",level:4},{value:"Chat History",id:"chat-history",level:3},{value:"JSON Chat History",id:"json-chat-history",level:4},{value:"SQL Chat History (SQLite)",id:"sql-chat-history-sqlite",level:4},{value:"PostgreSQL Chat History",id:"postgresql-chat-history",level:4},{value:"\ud83d\udd0d Vectorstore Package (<code>rakam-systems-vectorstore</code>)",id:"-vectorstore-package-rakam-systems-vectorstore",level:2},{value:"Keyword Search",id:"keyword-search",level:4},{value:"Multi-Model Support",id:"multi-model-support",level:4},{value:"ConfigurableEmbeddings",id:"configurableembeddings",level:3},{value:"Factory Function",id:"factory-function",level:4},{value:"AdaptiveLoader",id:"adaptiveloader",level:3},{value:"Factory Function",id:"factory-function-1",level:4},{value:"Specialized Loaders",id:"specialized-loaders",level:3},{value:"PdfLoaderLight",id:"pdfloaderlight",level:4},{value:"Image Extraction Support",id:"image-extraction-support",level:4},{value:"TextChunker",id:"textchunker",level:3},{value:"AdvancedChunker",id:"advancedchunker",level:3},{value:"Logging Utilities",id:"logging-utilities",level:3},{value:"\u2699\ufe0f Configuration System",id:"\ufe0f-configuration-system",level:2},{value:"\ud83d\ude80 Quick Start Examples",id:"-quick-start-examples",level:2},{value:"Basic Agent",id:"basic-agent",level:3},{value:"Agent with Tools",id:"agent-with-tools",level:3},{value:"Document Search Pipeline",id:"document-search-pipeline",level:3},{value:"Full RAG Pipeline",id:"full-rag-pipeline",level:3},{value:"Environment Variables",id:"environment-variables",level:2},{value:"\u2705 Best Practices",id:"-best-practices",level:2},{value:"\ud83d\udcda Further Reading",id:"-further-reading",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"rakam-systems-components-guide",children:"Rakam Systems Components Guide"})}),"\n",(0,r.jsx)(n.p,{children:"Welcome! This guide introduces the modular components of Rakam Systems, a framework for building robust, production-ready AI applications. Here, you'll find clear explanations and practical examples for using agents, vector stores, and configuration systems."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-table-of-contents",children:"\ud83d\udcd1 Table of Contents"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#%EF%B8%8F-architecture-overview",children:"\ud83c\udfd7\ufe0f Architecture Overview"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsxs)(n.a,{href:"#core-package-rakam-systems-core",children:["\ud83e\uddf1 Core Package (",(0,r.jsx)(n.code,{children:"rakam-systems-core"}),")"]})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsxs)(n.a,{href:"#-agent-package-rakam-systems-agent",children:["\ud83e\udd16 Agent Package (",(0,r.jsx)(n.code,{children:"rakam-systems-agent"}),")"]})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsxs)(n.a,{href:"#-vectorstore-package-rakam-systems-vectorstore",children:["\ud83d\udd0d Vectorstore Package (",(0,r.jsx)(n.code,{children:"rakam-systems-vectorstore"}),")"]})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#%EF%B8%8F-configuration-system",children:"\u2699\ufe0f Configuration System"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#-quick-start-examples",children:"\ud83d\ude80 Quick Start Examples"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#environment-variables",children:"\ud83c\udf0d Environment Variables"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#-best-practices",children:"\u2705 Best Practices"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#-further-reading",children:"\ud83d\udcda Further Reading"})}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"\ufe0f-architecture-overview",children:"\ud83c\udfd7\ufe0f Architecture Overview"}),"\n",(0,r.jsx)(n.p,{children:"Rakam Systems is split into three main packages:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Core"}),": Abstractions, interfaces, and base classes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Agent"}),": AI agent implementations (depends on core)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vectorstore"}),": Vector storage and document processing (depends on core)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Design Highlights:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Modular: Install only what you need"}),"\n",(0,r.jsx)(n.li,{children:"Clear dependencies: Agent/vectorstore depend on core"}),"\n",(0,r.jsxs)(n.li,{children:["Component-based: All components extend ",(0,r.jsx)(n.code,{children:"BaseComponent"})," with lifecycle methods (",(0,r.jsx)(n.code,{children:"setup()"}),", ",(0,r.jsx)(n.code,{children:"shutdown()"}),")"]}),"\n",(0,r.jsx)(n.li,{children:"Interface-driven: Abstract interfaces for easy extension"}),"\n",(0,r.jsx)(n.li,{children:"Configuration-first: YAML/JSON config for all components"}),"\n",(0,r.jsx)(n.li,{children:"Provider-agnostic: Works with many LLMs, embedding models, and vector stores"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.h2,{id:"core-package-rakam-systems-core",children:["Core Package (",(0,r.jsx)(n.code,{children:"rakam-systems-core"}),")"]}),"\n",(0,r.jsx)(n.p,{children:"The core package provides the foundational building blocks for Rakam Systems. Install this first before using agents or vector stores."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Base classes for all components"}),"\n",(0,r.jsx)(n.li,{children:"Abstract interfaces for agents, tools, LLM gateways, vector stores, and loaders"}),"\n",(0,r.jsx)(n.li,{children:"Built-in tracking for debugging and evaluation"}),"\n",(0,r.jsx)(n.li,{children:"Configuration loader for YAML/JSON configs"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example: Using the BaseComponent"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from rakam_system_core.base import BaseComponent\n\nclass MyComponent(BaseComponent):\n    def setup(self):\n        # Initialization logic\n        pass\n    def shutdown(self):\n        # Cleanup logic\n        pass\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Tracking Example:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from rakam_systems_core.tracking import TrackingMixin, track_method\n\nclass MyAgent(TrackingMixin, BaseAgent):\n    @track_method()\n    def run(self, input):\n        ...\n\n# Enable tracking\nagent.enable_tracking(output_dir=\"./tracking\")\nagent.export_tracking_data(format='csv')\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Configuration Loader Example:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_core.config_loader import ConfigurationLoader\n\nloader = ConfigurationLoader()\nconfig = loader.load_from_yaml("agent_config.yaml")\nagent = loader.create_agent("my_agent", config)\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.h2,{id:"-agent-package-rakam-systems-agent",children:["\ud83e\udd16 Agent Package (",(0,r.jsx)(n.code,{children:"rakam-systems-agent"}),")"]}),"\n",(0,r.jsx)(n.p,{children:"The agent package provides ready-to-use AI agent implementations powered by Pydantic AI. Install with:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install rakam-systems-agent\n"})}),"\n",(0,r.jsx)(n.p,{children:"(Requires the core package.)"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"BaseAgent class for building and running agents"}),"\n",(0,r.jsx)(n.li,{children:"Dynamic system prompts for context-aware responses"}),"\n",(0,r.jsx)(n.li,{children:"Multiple chat history backends (JSON, SQLite, PostgreSQL)"}),"\n",(0,r.jsx)(n.li,{children:"LLM gateway support (OpenAI, Mistral, and more)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example: Running an Agent"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_agent import BaseAgent\n\nagent = BaseAgent(name="my_agent", enable_tracking=True)\nresult = await agent.arun("What is AI?")\nprint(result.output_text)\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Dynamic System Prompts:"})}),"\n",(0,r.jsx)(n.p,{children:"Inject context at runtime using decorators or direct registration:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"@agent.dynamic_system_prompt\n\n    return f\"Today's date is {date.today().strftime('%B %d, %Y')}.\"\n\nagent.add_dynamic_system_prompt(add_time_context)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"llm-gateways",children:"LLM Gateways"}),"\n",(0,r.jsx)(n.h4,{id:"openai-gateway",children:"OpenAI Gateway"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_agent import OpenAIGateway, LLMRequest\n\ngateway = OpenAIGateway(\n    model="gpt-4o",\n    api_key="...",  # Or use OPENAI_API_KEY env var\n    default_temperature=0.7\n)\n\n# Text generation\nrequest = LLMRequest(\n    system_prompt="You are a helpful assistant",\n    user_prompt="What is AI?",\n    temperature=0.7\n)\nresponse = gateway.generate(request)\nprint(response.content)\n\n# Structured output\nfrom pydantic import BaseModel\n\nclass Answer(BaseModel):\n    answer: str\n    confidence: float\n\nresult = gateway.generate_structured(request, Answer)\nprint(result.answer, result.confidence)\n\n# Streaming\nfor chunk in gateway.stream(request):\n    print(chunk, end="")\n\n# Token counting\ntoken_count = gateway.count_tokens("Hello, world!")\n'})}),"\n",(0,r.jsx)(n.h4,{id:"mistral-gateway",children:"Mistral Gateway"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_agent import MistralGateway\n\ngateway = MistralGateway(\n    model="mistral-large-latest",\n    api_key="..."  # Or use MISTRAL_API_KEY env var\n)\n'})}),"\n",(0,r.jsx)(n.h4,{id:"gateway-factory",children:"Gateway Factory"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_agent import LLMGatewayFactory, get_llm_gateway\n\n# Using factory\ngateway = LLMGatewayFactory.create(\n    provider="openai",\n    model="gpt-4o",\n    api_key="..."\n)\n\n# Using convenience function\ngateway = get_llm_gateway(provider="openai", model="gpt-4o")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"chat-history",children:"Chat History"}),"\n",(0,r.jsx)(n.h4,{id:"json-chat-history",children:"JSON Chat History"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_agent.components.chat_history import JSONChatHistory\n\nhistory = JSONChatHistory(config={"storage_path": "./chat_history.json"})\n\n# Add messages\nhistory.add_message("chat123", {"role": "user", "content": "Hello!"})\nhistory.add_message("chat123", {"role": "assistant", "content": "Hi there!"})\n\n# Get history\nmessages = history.get_chat_history("chat123")\nreadable = history.get_readable_chat_history("chat123")\n\n# Pydantic AI integration\nmessage_history = history.get_message_history("chat123")\nresult = await agent.run("Hello", message_history=message_history)\nhistory.save_messages("chat123", result.all_messages())\n\n# Manage chats\nall_chats = history.get_all_chat_ids()\nhistory.delete_chat_history("chat123")\nhistory.clear_all()\n'})}),"\n",(0,r.jsx)(n.h4,{id:"sql-chat-history-sqlite",children:"SQL Chat History (SQLite)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_agent.components.chat_history import SQLChatHistory\n\nhistory = SQLChatHistory(config={"db_path": "./chat_history.db"})\n\n# Same API as JSON Chat History\nhistory.add_message("chat123", {"role": "user", "content": "Hello!"})\nhistory.add_message("chat123", {"role": "assistant", "content": "Hi there!"})\n\n# Get history\nmessages = history.get_chat_history("chat123")\n\n# Pydantic AI integration\nmessage_history = history.get_message_history("chat123")\nresult = await agent.run("Hello", message_history=message_history)\nhistory.save_messages("chat123", result.all_messages())\n'})}),"\n",(0,r.jsx)(n.h4,{id:"postgresql-chat-history",children:"PostgreSQL Chat History"}),"\n",(0,r.jsx)(n.p,{children:"For production deployments with PostgreSQL-backed storage:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_agent.components.chat_history import PostgresChatHistory\n\n# Configuration\nhistory = PostgresChatHistory(config={\n    "host": "localhost",\n    "port": 5432,\n    "database": "chat_db",\n    "user": "postgres",\n    "password": "postgres"\n})\n\n# Or use environment variables (POSTGRES_HOST, POSTGRES_PORT, etc.)\nhistory = PostgresChatHistory()\n\n# Same API as other chat history backends\nhistory.add_message("chat123", {"role": "user", "content": "Hello!"})\nhistory.add_message("chat123", {"role": "assistant", "content": "Hi there!"})\n\n# Get history\nmessages = history.get_chat_history("chat123")\nreadable = history.get_readable_chat_history("chat123")\n\n# Pydantic AI integration\nmessage_history = history.get_message_history("chat123")\nresult = await agent.run("Hello", message_history=message_history)\nhistory.save_messages("chat123", result.all_messages())\n\n# Manage chats\nall_chats = history.get_all_chat_ids()\nhistory.delete_chat_history("chat123")\nhistory.clear_all()\n\n# Cleanup\nhistory.shutdown()\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.h2,{id:"-vectorstore-package-rakam-systems-vectorstore",children:["\ud83d\udd0d Vectorstore Package (",(0,r.jsx)(n.code,{children:"rakam-systems-vectorstore"}),")"]}),"\n",(0,r.jsx)(n.p,{children:"The vectorstore package provides vector database and document processing tools. Install with:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install -e ./rakam-systems-vectorstore\n"})}),"\n",(0,r.jsx)(n.p,{children:"(Requires the core package.)"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Store and search document embeddings"}),"\n",(0,r.jsx)(n.li,{children:"Hybrid, vector, and keyword search"}),"\n",(0,r.jsx)(n.li,{children:"Multi-backend embedding support (OpenAI, Cohere, HuggingFace, etc.)"}),"\n",(0,r.jsx)(n.li,{children:"Adaptive file loaders for many formats"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example: Using the Vector Store"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig\n\nconfig = VectorStoreConfig()\nstore = ConfigurablePgVectorStore(config=config)\nstore.setup()\nresults = store.search("What is machine learning?", top_k=5)\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h4,{id:"keyword-search",children:"Keyword Search"}),"\n",(0,r.jsx)(n.p,{children:"Full-text search using PostgreSQL's built-in capabilities with BM25 or ts_rank ranking:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig\n\nconfig = VectorStoreConfig(\n    search={\n        "keyword_ranking_algorithm": "bm25",  # or "ts_rank"\n        "keyword_k1": 1.2,  # BM25 k1 parameter\n        "keyword_b": 0.75   # BM25 b parameter\n    }\n)\n\nstore = ConfigurablePgVectorStore(config=config)\nstore.setup()\n\n# Keyword search with BM25 ranking\nresults = store.keyword_search(\n    query="machine learning neural networks",\n    top_k=10,\n    ranking_algorithm="bm25"\n)\n\n# Keyword search with ts_rank\nresults = store.keyword_search(\n    query="deep learning",\n    top_k=10,\n    ranking_algorithm="ts_rank"\n)\n\n# Results include content and relevance scores\nfor result in results:\n    print(f"Score: {result[\'score\']:.4f}")\n    print(f"Content: {result[\'content\'][:200]}...")\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Ranking Algorithms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"BM25"}),": Best Match 25, probabilistic ranking function","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"k1"}),": Term frequency saturation parameter (default: 1.2)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"b"}),": Length normalization parameter (default: 0.75)"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ts_rank"}),": PostgreSQL's text search ranking function","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Weights different parts of documents differently"}),"\n",(0,r.jsx)(n.li,{children:"Good for structured documents"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"multi-model-support",children:"Multi-Model Support"}),"\n",(0,r.jsx)(n.p,{children:"Each embedding model automatically gets dedicated tables:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Using different models - each gets its own tables\nstore_minilm = ConfigurablePgVectorStore(config=config_minilm)\nstore_mpnet = ConfigurablePgVectorStore(config=config_mpnet)\n\n# Table names are based on model names:\n# - application_nodeentry_all_minilm_l6_v2\n# - application_nodeentry_snowflake_arctic_embed_m\n\n# Disable model-specific tables if needed (not recommended)\nstore = ConfigurablePgVectorStore(\n    config=config,\n    use_dimension_specific_tables=False\n)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"configurableembeddings",children:"ConfigurableEmbeddings"}),"\n",(0,r.jsx)(n.p,{children:"Multi-backend embedding model with unified interface:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore import ConfigurableEmbeddings, create_embedding_model\n\n# Using Sentence Transformers (local)\nembeddings = ConfigurableEmbeddings(config={\n    "model_type": "sentence_transformer",\n    "model_name": "Snowflake/snowflake-arctic-embed-m",\n    "batch_size": 128,\n    "normalize": True\n})\n\n# Using OpenAI (with batch processing)\nembeddings = ConfigurableEmbeddings(config={\n    "model_type": "openai",\n    "model_name": "text-embedding-3-small",\n    "api_key": "...",  # Or use OPENAI_API_KEY\n    "batch_size": 100   # OpenAI supports larger batches\n})\n\n# Using Cohere\nembeddings = ConfigurableEmbeddings(config={\n    "model_type": "cohere",\n    "model_name": "embed-english-v3.0",\n    "api_key": "..."  # Or use COHERE_API_KEY\n})\n\n# Using HuggingFace models with authentication\nembeddings = ConfigurableEmbeddings(config={\n    "model_type": "sentence_transformer",\n    "model_name": "private/model-name",\n    # Uses HUGGINGFACE_TOKEN environment variable\n})\n\nembeddings.setup()\n\n# Encode texts with automatic batch processing\nvectors = embeddings.run(["Hello world", "How are you?"])\n\n# Encode large datasets with progress tracking\nlarge_texts = ["text" + str(i) for i in range(10000)]\nvectors = embeddings.run(large_texts)  # Shows progress bar\n\n# Encode queries (optimized for single texts)\nquery_vector = embeddings.encode_query("What is AI?")\n\n# Encode documents (optimized for batches)\ndoc_vectors = embeddings.encode_documents(documents)\n\n# Get dimension\ndim = embeddings.embedding_dimension\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Performance Features:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Automatic batch processing with progress tracking"}),"\n",(0,r.jsx)(n.li,{children:"Memory optimization with garbage collection"}),"\n",(0,r.jsx)(n.li,{children:"Token truncation for oversized texts"}),"\n",(0,r.jsx)(n.li,{children:"Mini-batch processing for large datasets"}),"\n",(0,r.jsx)(n.li,{children:"CUDA memory management for GPU acceleration"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"factory-function",children:"Factory Function"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'embeddings = create_embedding_model(\n    model_type="sentence_transformer",\n    model_name="all-MiniLM-L6-v2",\n    batch_size=64\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"adaptiveloader",children:"AdaptiveLoader"}),"\n",(0,r.jsx)(n.p,{children:"Automatically detects and processes various file types:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore import AdaptiveLoader, create_adaptive_loader\n\nloader = AdaptiveLoader(config={\n    "encoding": "utf-8",\n    "chunk_size": 512,\n    "chunk_overlap": 50\n})\n\n# Supported file types:\n# - Text: .txt, .text\n# - Markdown: .md, .markdown\n# - Documents: .pdf, .docx, .doc, .odt\n# - Email: .eml, .msg\n# - Data: .json, .csv, .tsv, .xlsx, .xls\n# - HTML: .html, .htm, .xhtml\n# - Code: .py, .js, .ts, .java, .cpp, .go, .rs, .rb, etc.\n\n# Load as single text\ntext = loader.load_as_text("document.pdf")\n\n# Load as chunks\nchunks = loader.load_as_chunks("document.pdf")\n\n# Load as nodes (with metadata)\nnodes = loader.load_as_nodes("document.pdf", custom_metadata={"category": "science"})\n\n# Load as VSFile\nvsfile = loader.load_as_vsfile("document.pdf")\n\n# Also handles raw text\nchunks = loader.load_as_chunks("This is raw text content...")\n'})}),"\n",(0,r.jsx)(n.h4,{id:"factory-function-1",children:"Factory Function"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"loader = create_adaptive_loader(\n    chunk_size=1024,\n    chunk_overlap=100,\n    encoding='utf-8'\n)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"specialized-loaders",children:"Specialized Loaders"}),"\n",(0,r.jsxs)(n.p,{children:["Located in ",(0,r.jsx)(n.code,{children:"ai_vectorstore/components/loader/"}),":"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Loader"}),(0,r.jsx)(n.th,{children:"File Types"}),(0,r.jsx)(n.th,{children:"Features"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"PdfLoader"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:".pdf"})}),(0,r.jsx)(n.td,{children:"Advanced PDF processing with Docling, image extraction, table detection"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"PdfLoaderLight"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:".pdf"})}),(0,r.jsx)(n.td,{children:"Lightweight PDF processing with pymupdf4llm, markdown conversion, image extraction"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"DocLoader"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:".docx"}),", ",(0,r.jsx)(n.code,{children:".doc"})]}),(0,r.jsx)(n.td,{children:"Microsoft Word documents, image extraction"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"OdtLoader"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:".odt"})}),(0,r.jsx)(n.td,{children:"OpenDocument Text, image extraction"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"MdLoader"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:".md"})}),(0,r.jsx)(n.td,{children:"Markdown with structure preservation, YAML frontmatter"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"HtmlLoader"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:".html"}),", ",(0,r.jsx)(n.code,{children:".htm"})]}),(0,r.jsx)(n.td,{children:"HTML parsing and text extraction"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"EmlLoader"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:".eml"}),", ",(0,r.jsx)(n.code,{children:".msg"})]}),(0,r.jsx)(n.td,{children:"Email files (loaded as single nodes)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"TabularLoader"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:".csv"}),", ",(0,r.jsx)(n.code,{children:".tsv"}),", ",(0,r.jsx)(n.code,{children:".xlsx"})]}),(0,r.jsx)(n.td,{children:"Tabular data processing, preserves column structure"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"CodeLoader"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:".py"}),", ",(0,r.jsx)(n.code,{children:".js"}),", etc."]}),(0,r.jsx)(n.td,{children:"Code-aware chunking with syntax preservation"})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"pdfloaderlight",children:"PdfLoaderLight"}),"\n",(0,r.jsx)(n.p,{children:"A lightweight alternative to PdfLoader using pymupdf4llm for efficient PDF processing:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore.components.loader import PdfLoaderLight\n\nloader = PdfLoaderLight(\n    name="pdf_loader_light",\n    config={\n        "chunk_size": 512,\n        "chunk_overlap": 50,\n        "extract_images": True,\n        "image_path": "./extracted_images",\n        "page_chunks": True,  # Create one chunk per page\n        "write_images": True  # Save images to disk\n    }\n)\n\n# Load as markdown\nmarkdown_text = loader.load_as_text("document.pdf")\n\n# Load as chunks (one per page or custom chunking)\nchunks = loader.load_as_chunks("document.pdf")\n\n# Load as nodes with metadata\nnodes = loader.load_as_nodes("document.pdf")\n\n# Access extracted images\nimage_paths = loader.get_image_paths()\nfor img_id, img_path in image_paths.items():\n    print(f"Image {img_id}: {img_path}")\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Fast PDF to markdown conversion"}),"\n",(0,r.jsx)(n.li,{children:"Optional image extraction and saving"}),"\n",(0,r.jsx)(n.li,{children:"Page-aware chunking"}),"\n",(0,r.jsx)(n.li,{children:"Thread-safe operations"}),"\n",(0,r.jsx)(n.li,{children:"Lower memory footprint than PdfLoader"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"image-extraction-support",children:"Image Extraction Support"}),"\n",(0,r.jsx)(n.p,{children:"Multiple loaders now support image extraction:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore.components.loader import DocLoader, OdtLoader, PdfLoaderLight\n\n# DocLoader with image extraction\ndoc_loader = DocLoader(config={\n    "extract_images": True,\n    "image_path": "./doc_images"\n})\nnodes = doc_loader.load_as_nodes("document.docx")\n\n# Access extracted images\nfor img_id, img_path in doc_loader.get_image_paths().items():\n    print(f"Image {img_id}: {img_path}")\n\n# OdtLoader with image extraction\nodt_loader = OdtLoader(config={\n    "extract_images": True,\n    "image_path": "./odt_images"\n})\nnodes = odt_loader.load_as_nodes("document.odt")\n\n# PdfLoaderLight with image extraction\npdf_loader = PdfLoaderLight(config={\n    "extract_images": True,\n    "image_path": "./pdf_images",\n    "write_images": True\n})\nnodes = pdf_loader.load_as_nodes("document.pdf")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"textchunker",children:"TextChunker"}),"\n",(0,r.jsx)(n.p,{children:"Sentence-based text chunking using Chonkie:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore.components.chunker import TextChunker, create_text_chunker\n\nchunker = TextChunker(\n    chunk_size=512,        # Tokens per chunk\n    chunk_overlap=50,      # Overlap in tokens\n    min_sentences_per_chunk=1,\n    tokenizer="character"  # Or "gpt2", HuggingFace tokenizer\n)\n\nchunks = chunker.chunk_text("Long document text...")\n# Returns: [{"text": "...", "token_count": 100, "start_index": 0, "end_index": 500}, ...]\n\n# Process multiple documents\nall_chunks = chunker.run(["doc1 text", "doc2 text"])\n'})}),"\n",(0,r.jsx)(n.h3,{id:"advancedchunker",children:"AdvancedChunker"}),"\n",(0,r.jsx)(n.p,{children:"Advanced document chunking using Docling for context-aware chunking with heading preservation:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore.components.chunker import AdvancedChunker\n\nchunker = AdvancedChunker(\n    name="advanced_chunker",\n    config={\n        "max_tokens": 512,           # Maximum tokens per chunk\n        "merge_peers": True,          # Merge peer sections\n        "min_chunk_tokens": 64,       # Minimum tokens per chunk\n        "filter_toc": True,           # Filter table of contents\n        "include_heading_markers": True  # Include markdown headings\n    }\n)\n\n# Chunk text with context preservation\nchunks = chunker.chunk_text("Document text with headings...")\n\n# Each chunk includes:\n# - text: The chunk content\n# - token_count: Number of tokens\n# - start_index: Starting position\n# - end_index: Ending position\n# - heading_context: Hierarchical heading information\n\n# Process with heading markers\nchunker_with_markers = AdvancedChunker(config={\n    "include_heading_markers": True\n})\nchunks = chunker_with_markers.chunk_text("""\n# Main Title\n## Section 1\nContent here...\n## Section 2\nMore content...\n""")\n# Output includes markdown-style headings in chunks\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Context-aware chunking with heading hierarchy"}),"\n",(0,r.jsx)(n.li,{children:"Automatic merging of small chunks"}),"\n",(0,r.jsx)(n.li,{children:"Table of contents filtering"}),"\n",(0,r.jsx)(n.li,{children:"Image and table fragment handling"}),"\n",(0,r.jsx)(n.li,{children:"Markdown heading markers support"}),"\n",(0,r.jsx)(n.li,{children:"Configurable token limits and merging behavior"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"logging-utilities",children:"Logging Utilities"}),"\n",(0,r.jsx)(n.p,{children:"The core package includes logging utilities:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_tools.utils import logging\n\nlogger = logging.getLogger(__name__)\nlogger.info("Processing document...")\nlogger.debug("Detailed debug info")\nlogger.error("An error occurred")\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"\ufe0f-configuration-system",children:"\u2699\ufe0f Configuration System"}),"\n",(0,r.jsx)(n.p,{children:"Rakam Systems is configuration-first: you can change agent behavior, vector store settings, and more\u2014without touching your code."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why use configuration?"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Rapidly test different models, prompts, or parameters"}),"\n",(0,r.jsx)(n.li,{children:"Manage dev/staging/production environments easily"}),"\n",(0,r.jsx)(n.li,{children:"Enable A/B testing and team collaboration"}),"\n",(0,r.jsx)(n.li,{children:"Optimize costs and reduce deployment risk"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example: Switching Models with YAML"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"# Week 1: Use GPT-4o\nmodel: openai:gpt-4o\ntemperature: 0.7\n\n# Week 2: Try GPT-4o-mini (no code changes!)\nmodel: openai:gpt-4o-mini\ntemperature: 0.7\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example: Programmatic Configuration"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore.config import VectorStoreConfig\n\nconfig = VectorStoreConfig(name="my_vectorstore")\nconfig.save_yaml("output_config.yaml")\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-quick-start-examples",children:"\ud83d\ude80 Quick Start Examples"}),"\n",(0,r.jsx)(n.h3,{id:"basic-agent",children:"Basic Agent"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_systems_agent import BaseAgent\n\nasync def main():\n    agent = BaseAgent(\n        name="assistant",\n        model="openai:gpt-4o",\n        system_prompt="You are a helpful assistant."\n    )\n\n    result = await agent.arun("What is the capital of France?")\n    print(result.output_text)\n\nasyncio.run(main())\n'})}),"\n",(0,r.jsx)(n.h3,{id:"agent-with-tools",children:"Agent with Tools"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_systems_agent import BaseAgent\nfrom rakam_systems_core.interfaces.tool import ToolComponent\n\ndef get_weather(city: str) -> str:\n    return f"The weather in {city} is sunny, 25\xb0C"\n\nweather_tool = ToolComponent.from_function(\n    function=get_weather,\n    name="get_weather",\n    description="Get the current weather for a city",\n    json_schema={\n        "type": "object",\n        "properties": {"city": {"type": "string"}},\n        "required": ["city"]\n    }\n)\n\nasync def main():\n    agent = BaseAgent(\n        name="weather_assistant",\n        model="openai:gpt-4o",\n        system_prompt="You help users with weather information.",\n        tools=[weather_tool]\n    )\n\n    result = await agent.arun("What\'s the weather in Paris?")\n    print(result.output_text)\n\nasyncio.run(main())\n'})}),"\n",(0,r.jsx)(n.h3,{id:"document-search-pipeline",children:"Document Search Pipeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rakam_systems_vectorstore import (\n    ConfigurablePgVectorStore,\n    VectorStoreConfig,\n    AdaptiveLoader\n)\n\n# Configure vector store\nconfig = VectorStoreConfig()\nstore = ConfigurablePgVectorStore(config=config)\nstore.setup()\n\n# Load documents\nloader = AdaptiveLoader(config={"chunk_size": 512})\nnodes = loader.load_as_nodes("documents/research_paper.pdf")\n\n# Add to vector store\nstore.add_nodes(nodes)\n\n# Search\nresults = store.search("What are the main findings?", top_k=5)\nfor result in results:\n    print(f"Score: {result[\'score\']:.4f}")\n    print(f"Content: {result[\'content\'][:200]}...")\n    print("---")\n\nstore.shutdown()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"full-rag-pipeline",children:"Full RAG Pipeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_systems_agent import BaseAgent\nfrom rakam_systems_vectorstore import ConfigurablePgVectorStore, AdaptiveLoader, VectorStoreConfig\nfrom rakam_systems_core.interfaces.tool import ToolComponent\n\n# Setup vector store\nconfig = VectorStoreConfig()\nstore = ConfigurablePgVectorStore(config=config)\nstore.setup()\n\n# Index documents\nloader = AdaptiveLoader()\nfor doc_path in ["doc1.pdf", "doc2.pdf", "doc3.pdf"]:\n    nodes = loader.load_as_nodes(doc_path)\n    store.add_nodes(nodes)\n\n# Create search tool\ndef search_documents(query: str, top_k: int = 5) -> str:\n    results = store.search(query, top_k=top_k)\n    return "\\n\\n".join([r[\'content\'] for r in results])\n\nsearch_tool = ToolComponent.from_function(\n    function=search_documents,\n    name="search_documents",\n    description="Search the document database",\n    json_schema={\n        "type": "object",\n        "properties": {\n            "query": {"type": "string", "description": "Search query"},\n            "top_k": {"type": "integer", "description": "Number of results"}\n        },\n        "required": ["query"]\n    }\n)\n\n# Create RAG agent\nasync def main():\n    agent = BaseAgent(\n        name="rag_agent",\n        model="openai:gpt-4o",\n        system_prompt="""You are a helpful assistant with access to a document database.\n        Use the search_documents tool to find relevant information before answering questions.""",\n        tools=[search_tool]\n    )\n\n    result = await agent.arun("What are the key points from the documents?")\n    print(result.output_text)\n\nasyncio.run(main())\nstore.shutdown()\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"environment-variables",children:"Environment Variables"}),"\n",(0,r.jsx)(n.p,{children:"The system supports the following environment variables:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Variable"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Used By"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"OPENAI_API_KEY"})}),(0,r.jsx)(n.td,{children:"OpenAI API key"}),(0,r.jsx)(n.td,{children:"OpenAIGateway, ConfigurableEmbeddings"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"MISTRAL_API_KEY"})}),(0,r.jsx)(n.td,{children:"Mistral API key"}),(0,r.jsx)(n.td,{children:"MistralGateway"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"COHERE_API_KEY"})}),(0,r.jsx)(n.td,{children:"Cohere API key"}),(0,r.jsx)(n.td,{children:"ConfigurableEmbeddings"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"POSTGRES_HOST"})}),(0,r.jsx)(n.td,{children:"PostgreSQL host"}),(0,r.jsx)(n.td,{children:"DatabaseConfig"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"POSTGRES_PORT"})}),(0,r.jsx)(n.td,{children:"PostgreSQL port"}),(0,r.jsx)(n.td,{children:"DatabaseConfig"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"POSTGRES_DB"})}),(0,r.jsx)(n.td,{children:"PostgreSQL database"}),(0,r.jsx)(n.td,{children:"DatabaseConfig"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"POSTGRES_USER"})}),(0,r.jsx)(n.td,{children:"PostgreSQL user"}),(0,r.jsx)(n.td,{children:"DatabaseConfig"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"POSTGRES_PASSWORD"})}),(0,r.jsx)(n.td,{children:"PostgreSQL password"}),(0,r.jsx)(n.td,{children:"DatabaseConfig"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-best-practices",children:"\u2705 Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Always use context managers"})," or explicit ",(0,r.jsx)(n.code,{children:"setup()"}),"/",(0,r.jsx)(n.code,{children:"shutdown()"})," for proper resource management"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use configuration files"})," for production deployments instead of hardcoded values"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Enable tracking"})," during development for debugging and evaluation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use model-specific tables"})," (default) to prevent mixing incompatible vector spaces"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Batch operations"})," when processing large document collections"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use async methods"})," (",(0,r.jsx)(n.code,{children:"arun"}),", ",(0,r.jsx)(n.code,{children:"astream"}),") for agents as they are powered by Pydantic AI"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Validate configurations"})," before deployment using ",(0,r.jsx)(n.code,{children:"config.validate()"})," or ",(0,r.jsx)(n.code,{children:"loader.validate_config()"})]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-further-reading",children:"\ud83d\udcda Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Example configurations: ",(0,r.jsx)(n.code,{children:"examples/configs/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Agent examples: ",(0,r.jsx)(n.code,{children:"examples/ai_agents_examples/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Vector store examples: ",(0,r.jsx)(n.code,{children:"examples/ai_vectorstore_examples/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Loader documentation: ",(0,r.jsx)(n.code,{children:"rakam-systems-vectorstore/src/rakam_systems_vectorstore/components/loader/docs/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Architecture documentation: ",(0,r.jsx)(n.code,{children:"rakam-systems-vectorstore/src/rakam_systems_vectorstore/docs/ARCHITECTURE.md"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453(e,n,t){t.d(n,{R:()=>i,x:()=>o});var s=t(6540);const r={},a=s.createContext(r);function i(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);