"use strict";(self.webpackChunkrakam_systems_docs_portal=self.webpackChunkrakam_systems_docs_portal||[]).push([[8369],{3669(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"getting-started/quick_start","title":"Quick Start Guide","description":"Get up and running with Rakam Systems in 5 minutes!","source":"@site/docs/getting-started/quick_start.md","sourceDirName":"getting-started","slug":"/getting-started/quick_start","permalink":"/rakam-systems-docs/getting-started/quick_start","draft":false,"unlisted":false,"editUrl":"https://github.com/Rakam-AI/rakam_systems/edit/main/docs/docs/getting-started/quick_start.md","tags":[],"version":"current","lastUpdatedBy":"Mohamed Bashar Touil","lastUpdatedAt":1770933700000,"frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Installation Guide","permalink":"/rakam-systems-docs/getting-started/installation"},"next":{"title":"Components Documentation","permalink":"/rakam-systems-docs/getting-started/components"}}');var s=t(4848),i=t(8453);const o={},a="Quick Start Guide",c={},d=[{value:"\ud83d\udcd1 Table of Contents",id:"-table-of-contents",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Installation",id:"installation",level:2},{value:"Quick Start: AI Agent",id:"quick-start-ai-agent",level:2},{value:"With Streaming",id:"with-streaming",level:3},{value:"With Model Settings",id:"with-model-settings",level:3},{value:"Quick Start: Agent with Tools",id:"quick-start-agent-with-tools",level:2},{value:"Quick Start: Structured Output",id:"quick-start-structured-output",level:2},{value:"Quick Start: Chat History",id:"quick-start-chat-history",level:2},{value:"JSON Chat History (File-based)",id:"json-chat-history-file-based",level:3},{value:"PostgreSQL Chat History (Production)",id:"postgresql-chat-history-production",level:3},{value:"SQLite Chat History (Local Database)",id:"sqlite-chat-history-local-database",level:3},{value:"Quick Start: Configurable Agent",id:"quick-start-configurable-agent",level:2},{value:"Agent Configuration File",id:"agent-configuration-file",level:3},{value:"Loading Agents from Configuration",id:"loading-agents-from-configuration",level:3},{value:"Using Tool Registry",id:"using-tool-registry",level:3},{value:"Configuration Options Reference",id:"configuration-options-reference",level:3},{value:"Model Configuration (<code>llm_config</code>)",id:"model-configuration-llm_config",level:4},{value:"Tool Configuration",id:"tool-configuration",level:4},{value:"Inline Output Type",id:"inline-output-type",level:4},{value:"Quick Start: Vector Store",id:"quick-start-vector-store",level:2},{value:"Using FAISS (In-Memory)",id:"using-faiss-in-memory",level:3},{value:"Using PostgreSQL with pgvector",id:"using-postgresql-with-pgvector",level:3},{value:"Quick Start: Configurable Vector Store",id:"quick-start-configurable-vector-store",level:2},{value:"VectorStoreConfig Overview",id:"vectorstoreconfig-overview",level:3},{value:"Configuration from YAML",id:"configuration-from-yaml",level:3},{value:"Using Different Embedding Models",id:"using-different-embedding-models",level:3},{value:"Multi-Model Support",id:"multi-model-support",level:3},{value:"Hybrid Search",id:"hybrid-search",level:3},{value:"Keyword Search",id:"keyword-search",level:3},{value:"Full Example with All Features",id:"full-example-with-all-features",level:3},{value:"Configuration Reference",id:"configuration-reference",level:3},{value:"EmbeddingConfig",id:"embeddingconfig",level:4},{value:"DatabaseConfig",id:"databaseconfig",level:4},{value:"SearchConfig",id:"searchconfig",level:4},{value:"IndexConfig",id:"indexconfig",level:4},{value:"Quick Start: RAG Pipeline",id:"quick-start-rag-pipeline",level:2},{value:"Quick Start: LLM Gateway",id:"quick-start-llm-gateway",level:2},{value:"Structured Output with Gateway",id:"structured-output-with-gateway",level:3},{value:"Streaming with Gateway",id:"streaming-with-gateway",level:3},{value:"Quick Start: Document Loading",id:"quick-start-document-loading",level:2},{value:"Supported File Types",id:"supported-file-types",level:3},{value:"Quick Start: Configuration from YAML",id:"quick-start-configuration-from-yaml",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Common Patterns",id:"common-patterns",level:2},{value:"Error Handling",id:"error-handling",level:3},{value:"Context Manager Pattern",id:"context-manager-pattern",level:3},{value:"Async Batch Processing",id:"async-batch-processing",level:3}];function l(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"quick-start-guide",children:"Quick Start Guide"})}),"\n",(0,s.jsx)(n.p,{children:"Get up and running with Rakam Systems in 5 minutes!"}),"\n",(0,s.jsx)(n.h2,{id:"-table-of-contents",children:"\ud83d\udcd1 Table of Contents"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#prerequisites",children:"Prerequisites"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#installation",children:"Installation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#quick-start-ai-agent",children:"Quick Start: AI Agent"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#quick-start-agent-with-tools",children:"Quick Start: Agent with Tools"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#quick-start-structured-output",children:"Quick Start: Structured Output"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#quick-start-configurable-agent",children:"Quick Start: Configurable Agent"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#quick-start-vector-store",children:"Quick Start: Vector Store"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#quick-start-configurable-vector-store",children:"Quick Start: Configurable Vector Store"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#quick-start-rag-pipeline",children:"Quick Start: RAG Pipeline"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#quick-start-llm-gateway",children:"Quick Start: LLM Gateway"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#quick-start-document-loading",children:"Quick Start: Document Loading"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#quick-start-configuration-from-yaml",children:"Quick Start: Configuration from YAML"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#common-patterns",children:"Common Patterns"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#next-steps",children:"Next Steps"})}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Python 3.10 or higher"}),"\n",(0,s.jsx)(n.li,{children:"OpenAI API key (for most examples)"}),"\n",(0,s.jsx)(n.li,{children:"pip package manager"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Install core package (required)\npip install -e ./rakam-system-core\n\n# Install agent package for AI agents\npip install -e ./rakam-system-agent\n\n# Install vectorstore package for vector search\npip install -e ./rakam-system-vectorstore\n\n# Or install everything at once\npip install -e ./rakam-system-core ./rakam-system-agent ./rakam-system-vectorstore\n"})}),"\n",(0,s.jsx)(n.p,{children:"Set your API key:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'export OPENAI_API_KEY="sk-your-api-key"\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"quick-start-ai-agent",children:"Quick Start: AI Agent"}),"\n",(0,s.jsx)(n.p,{children:"Create a simple AI agent in just a few lines:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nfrom rakam_system_agent import BaseAgent\n\nasync def main():\n    # Create an agent\n    agent = BaseAgent(\n        name="my_assistant",\n        model="openai:gpt-4o",\n        system_prompt="You are a helpful assistant that provides concise answers."\n    )\n\n    # Ask a question\n    result = await agent.arun("What is Python?")\n    print(result.output_text)\n\nasyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.h3,{id:"with-streaming",children:"With Streaming"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_system_agent import BaseAgent\n\nasync def main():\n    agent = BaseAgent(\n        name="streaming_agent",\n        model="openai:gpt-4o",\n        system_prompt="You are a helpful assistant."\n    )\n\n    # Stream the response\n    print("Response: ", end="", flush=True)\n    async for chunk in agent.astream("Tell me a short story about a robot."):\n        print(chunk, end="", flush=True)\n    print()\n\nasyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.h3,{id:"with-model-settings",children:"With Model Settings"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_system_agent import BaseAgent\nfrom rakam_system_core.ai_core.interfaces import ModelSettings\n\nasync def main():\n    agent = BaseAgent(\n        name="creative_agent",\n        model="openai:gpt-4o",\n        system_prompt="You are a creative writer."\n    )\n\n    # Use custom temperature and max tokens\n    result = await agent.arun(\n        "Write a haiku about programming.",\n        model_settings=ModelSettings(temperature=0.9, max_tokens=100)\n    )\n    print(result.output_text)\n\nasyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"quick-start-agent-with-tools",children:"Quick Start: Agent with Tools"}),"\n",(0,s.jsx)(n.p,{children:"Create an agent that can use tools:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_system_agent import BaseAgent\nfrom rakam_system_core.ai_core.interfaces.tool import ToolComponent\n\n# Define a tool function\ndef get_weather(city: str, units: str = "celsius") -> str:\n    """Get weather for a city (mock implementation)."""\n    return f"Weather in {city}: 22\xb0{\'C\' if units == \'celsius\' else \'F\'}, Sunny"\n\ndef calculate(expression: str) -> str:\n    """Calculate a mathematical expression."""\n    try:\n        result = eval(expression)\n        return f"Result: {result}"\n    except Exception as e:\n        return f"Error: {e}"\n\n# Create tool components\nweather_tool = ToolComponent.from_function(\n    function=get_weather,\n    name="get_weather",\n    description="Get the current weather for a city",\n    json_schema={\n        "type": "object",\n        "properties": {\n            "city": {"type": "string", "description": "City name"},\n            "units": {\n                "type": "string",\n                "enum": ["celsius", "fahrenheit"],\n                "default": "celsius"\n            }\n        },\n        "required": ["city"],\n        "additionalProperties": False\n    }\n)\n\ncalculator_tool = ToolComponent.from_function(\n    function=calculate,\n    name="calculator",\n    description="Calculate a mathematical expression",\n    json_schema={\n        "type": "object",\n        "properties": {\n            "expression": {"type": "string", "description": "Math expression to evaluate"}\n        },\n        "required": ["expression"],\n        "additionalProperties": False\n    }\n)\n\nasync def main():\n    # Create agent with tools\n    agent = BaseAgent(\n        name="tool_agent",\n        model="openai:gpt-4o",\n        system_prompt="You are a helpful assistant with access to tools.",\n        tools=[weather_tool, calculator_tool]\n    )\n\n    # Ask questions that use tools\n    result = await agent.arun("What\'s the weather in Paris?")\n    print(f"Weather: {result.output_text}\\n")\n\n    result = await agent.arun("What is 25 * 4 + 100?")\n    print(f"Calculation: {result.output_text}")\n\nasyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"quick-start-structured-output",children:"Quick Start: Structured Output"}),"\n",(0,s.jsx)(n.p,{children:"Get structured, typed responses from your agent:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom pydantic import BaseModel, Field\nfrom rakam_system_agent import BaseAgent\n\n# Define output structure\nclass MovieReview(BaseModel):\n    title: str = Field(description="Movie title")\n    rating: float = Field(ge=0, le=10, description="Rating out of 10")\n    summary: str = Field(description="Brief summary")\n    pros: list[str] = Field(description="List of positive aspects")\n    cons: list[str] = Field(description="List of negative aspects")\n    recommended: bool = Field(description="Whether to recommend")\n\nasync def main():\n    # Create agent with structured output\n    agent = BaseAgent(\n        name="movie_critic",\n        model="openai:gpt-4o",\n        system_prompt="You are a movie critic. Analyze movies thoroughly.",\n        output_type=MovieReview  # Enforces structured output\n    )\n\n    result = await agent.arun("Review the movie \'Inception\' by Christopher Nolan")\n\n    # Access typed output\n    review: MovieReview = result.output\n    print(f"Title: {review.title}")\n    print(f"Rating: {review.rating}/10")\n    print(f"Summary: {review.summary}")\n    print(f"Pros: {\', \'.join(review.pros)}")\n    print(f"Cons: {\', \'.join(review.cons)}")\n    print(f"Recommended: {\'Yes\' if review.recommended else \'No\'}")\n\nasyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"quick-start-chat-history",children:"Quick Start: Chat History"}),"\n",(0,s.jsx)(n.p,{children:"Maintain conversation context across multiple interactions:"}),"\n",(0,s.jsx)(n.h3,{id:"json-chat-history-file-based",children:"JSON Chat History (File-based)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_system_agent import BaseAgent\nfrom rakam_system_agent.components.chat_history import JSONChatHistory\n\nasync def main():\n    # Initialize chat history\n    history = JSONChatHistory(config={\n        "storage_path": "./chat_history.json"\n    })\n\n    # Create agent\n    agent = BaseAgent(\n        name="chat_agent",\n        model="openai:gpt-4o",\n        system_prompt="You are a helpful assistant with memory."\n    )\n\n    chat_id = "user_123"\n\n    # First conversation\n    message_history = history.get_message_history(chat_id)\n    result = await agent.arun(\n        "My name is Alice and I love Python programming.",\n        message_history=message_history\n    )\n    history.save_messages(chat_id, result.all_messages())\n    print(f"Agent: {result.output_text}\\n")\n\n    # Second conversation (agent remembers)\n    message_history = history.get_message_history(chat_id)\n    result = await agent.arun(\n        "What\'s my name and what do I love?",\n        message_history=message_history\n    )\n    history.save_messages(chat_id, result.all_messages())\n    print(f"Agent: {result.output_text}")\n\nasyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.h3,{id:"postgresql-chat-history-production",children:"PostgreSQL Chat History (Production)"}),"\n",(0,s.jsx)(n.p,{children:"For production deployments with PostgreSQL:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_system_agent import BaseAgent\nfrom rakam_system_agent.components.chat_history import PostgresChatHistory\n\nasync def main():\n    # Initialize PostgreSQL chat history\n    history = PostgresChatHistory(config={\n        "host": "localhost",\n        "port": 5432,\n        "database": "chat_db",\n        "user": "postgres",\n        "password": "postgres"\n    })\n\n    # Or use environment variables (POSTGRES_HOST, POSTGRES_PORT, etc.)\n    # history = PostgresChatHistory()\n\n    # Create agent\n    agent = BaseAgent(\n        name="chat_agent",\n        model="openai:gpt-4o",\n        system_prompt="You are a helpful assistant."\n    )\n\n    chat_id = "user_456"\n\n    # Get existing history\n    message_history = history.get_message_history(chat_id)\n\n    # Continue conversation\n    result = await agent.arun(\n        "Tell me about machine learning.",\n        message_history=message_history\n    )\n\n    # Save new messages\n    history.save_messages(chat_id, result.all_messages())\n    print(f"Agent: {result.output_text}")\n\n    # Get readable history\n    readable = history.get_readable_chat_history(chat_id)\n    print(f"\\nConversation History:\\n{readable}")\n\n    # Cleanup\n    history.shutdown()\n\nasyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.h3,{id:"sqlite-chat-history-local-database",children:"SQLite Chat History (Local Database)"}),"\n",(0,s.jsx)(n.p,{children:"For local development with SQLite:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_system_agent import BaseAgent\nfrom rakam_system_agent.components.chat_history import SQLChatHistory\n\nasync def main():\n    # Initialize SQLite chat history\n    history = SQLChatHistory(config={\n        "db_path": "./chat_history.db"\n    })\n\n    agent = BaseAgent(\n        name="chat_agent",\n        model="openai:gpt-4o",\n        system_prompt="You are a helpful assistant."\n    )\n\n    chat_id = "user_789"\n\n    # Use with agent\n    message_history = history.get_message_history(chat_id)\n    result = await agent.arun(\n        "Hello! How are you?",\n        message_history=message_history\n    )\n    history.save_messages(chat_id, result.all_messages())\n\n    print(f"Agent: {result.output_text}")\n\nasyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Choosing a Backend:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"JSONChatHistory"}),": Simple file-based storage, good for prototyping"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"SQLChatHistory"}),": SQLite database, good for local development"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"PostgresChatHistory"}),": Production-ready, scalable, concurrent access"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"quick-start-configurable-agent",children:"Quick Start: Configurable Agent"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Power of Configuration-First Design"}),": Create agents from YAML configuration files and change behavior without touching your code! This is perfect for:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Production deployments"}),": Tune parameters without redeploying"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"A/B testing"}),": Test different models/settings by swapping config files"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Environment management"}),": Different configs for dev/staging/production"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Rapid iteration"}),": Experiment with prompts, tools, and settings instantly"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"agent-configuration-file",children:"Agent Configuration File"}),"\n",(0,s.jsxs)(n.p,{children:["Create ",(0,s.jsx)(n.code,{children:"config/agent_config.yaml"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'version: "1.0"\n\n# Define reusable prompts\nprompts:\n  customer_support:\n    name: "customer_support"\n    description: "Friendly customer support specialist"\n    system_prompt: |\n      You are a friendly and empathetic customer support specialist.\n      Your goal is to help customers solve their problems efficiently.\n\n      Guidelines:\n      - Listen actively to customer concerns\n      - Provide clear, step-by-step solutions\n      - Always maintain a professional yet warm tone\n    skills:\n      - "Problem solving"\n      - "Customer communication"\n    tags:\n      - "support"\n\n  code_assistant:\n    name: "code_assistant"\n    description: "Expert programming assistant"\n    system_prompt: |\n      You are an expert programming assistant with deep knowledge of:\n      - Multiple programming languages (Python, JavaScript, TypeScript, etc.)\n      - Software design patterns and best practices\n      - Code optimization and debugging\n\n      When generating code:\n      1. Follow best practices and conventions\n      2. Include comments and documentation\n      3. Consider edge cases and error handling\n\n# Define reusable tools\ntools:\n  get_weather:\n    name: "get_weather"\n    type: "direct"\n    module: "rakam_system_agent.components.tools.example_tools"\n    function: "get_current_weather"\n    description: "Get the current weather for a location"\n    category: "utility"\n    tags: ["weather", "external"]\n    json_schema:\n      type: "object"\n      properties:\n        location:\n          type: "string"\n          description: "City name or location"\n        units:\n          type: "string"\n          enum: ["celsius", "fahrenheit"]\n          default: "celsius"\n      required: ["location"]\n      additionalProperties: false\n\n  analyze_sentiment:\n    name: "analyze_sentiment"\n    type: "direct"\n    module: "rakam_system_agent.components.tools.example_tools"\n    function: "analyze_sentiment"\n    description: "Analyze the sentiment of text"\n    category: "nlp"\n    json_schema:\n      type: "object"\n      properties:\n        text:\n          type: "string"\n          description: "Text to analyze"\n      required: ["text"]\n      additionalProperties: false\n\n# Define agents\nagents:\n  support_agent:\n    name: "support_agent"\n    description: "Customer support specialist"\n\n    # Model configuration\n    llm_config:\n      model: "openai:gpt-4o"\n      temperature: 0.7\n      max_tokens: 2000\n      parallel_tool_calls: true\n\n    # Reference to prompt library\n    prompt_config: "customer_support"\n\n    # Reference to tools library\n    tools:\n      - "get_weather"\n      - "analyze_sentiment"\n\n    # Enable input/output tracking\n    enable_tracking: true\n    tracking_output_dir: "./agent_tracking/support"\n\n    # Additional metadata\n    metadata:\n      version: "1.0"\n      department: "customer_support"\n\n  # Agent with structured output defined inline\n  sql_agent:\n    name: "sql_agent"\n    description: "SQL query assistant with structured output"\n\n    llm_config:\n      model: "openai:gpt-4o"\n      temperature: 0.2 # Low temperature for consistent output\n      max_tokens: 3000\n\n    prompt_config: "code_assistant"\n    tools: []\n\n    # Define structured output directly in YAML (no Python class needed!)\n    output_type:\n      name: "SQLAgentOutput"\n      description: "Structured output for SQL queries"\n      fields:\n        answer:\n          type: str\n          description: "The answer to the user\'s question"\n        sql_query:\n          type: str\n          description: "The generated SQL query"\n          default: ""\n        explanation:\n          type: str\n          description: "Explanation of the query"\n        tables_used:\n          type: list\n          description: "List of tables referenced"\n          default_factory: list\n\n    enable_tracking: true\n'})}),"\n",(0,s.jsx)(n.h3,{id:"loading-agents-from-configuration",children:"Loading Agents from Configuration"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Benefit"}),": Your application code stays the same - just swap config files!"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_system_core.ai_core.config_loader import ConfigurationLoader\n\nasync def main():\n    # Initialize configuration loader\n    loader = ConfigurationLoader()\n\n    # Load configuration from YAML\n    # Change behavior by using different config files - no code changes!\n    config_file = "config/agent_config.yaml"  # or "config/agent_config_prod.yaml"\n    config = loader.load_from_yaml(config_file)\n\n    # Validate configuration (optional but recommended)\n    is_valid, errors = loader.validate_config()\n    if not is_valid:\n        for error in errors:\n            print(f"Config error: {error}")\n        return\n\n    # Create a single agent\n    support_agent = loader.create_agent("support_agent", config)\n\n    # Use the agent - behavior determined by config, not code!\n    result = await support_agent.arun("What\'s the weather in New York?")\n    print(f"Support Agent: {result.output_text}")\n\n    # Create agent with structured output\n    sql_agent = loader.create_agent("sql_agent", config)\n    result = await sql_agent.arun("Write a query to find all users who signed up last month")\n\n    # Access structured output\n    print(f"Answer: {result.output.answer}")\n    print(f"SQL: {result.output.sql_query}")\n    print(f"Tables: {result.output.tables_used}")\n\n    # Create all agents at once\n    all_agents = loader.create_all_agents(config)\n    print(f"Created {len(all_agents)} agents: {list(all_agents.keys())}")\n\nasyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example: Switching Models Without Code Changes"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Development: Use faster, cheaper model\n# config/agent_config_dev.yaml\nagents:\n  my_agent:\n    llm_config:\n      model: openai:gpt-4o-mini\n      temperature: 0.7\n\n# Production: Use more capable model\n# config/agent_config_prod.yaml\nagents:\n  my_agent:\n    llm_config:\n      model: openai:gpt-4o\n      temperature: 0.5\n\n# Application code stays the same - just change config file path!\n# config = loader.load_from_yaml("config/agent_config_prod.yaml")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"using-tool-registry",children:"Using Tool Registry"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_system_core.ai_core.config_loader import ConfigurationLoader\nfrom rakam_system_core.ai_core.interfaces.tool_registry import ToolRegistry\n\nloader = ConfigurationLoader()\nconfig = loader.load_from_yaml("config/agent_config.yaml")\n\n# Get tool registry with all configured tools\nregistry = loader.get_tool_registry(config)\n\n# Query tools by category\nutility_tools = registry.get_tools_by_category("utility")\nprint(f"Utility tools: {[t.name for t in utility_tools]}")\n\n# Query tools by tag\nexternal_tools = registry.get_tools_by_tag("external")\nprint(f"External tools: {[t.name for t in external_tools]}")\n\n# Get specific tool\nweather_tool = registry.get_tool("get_weather")\nprint(f"Weather tool: {weather_tool.description}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"configuration-options-reference",children:"Configuration Options Reference"}),"\n",(0,s.jsxs)(n.h4,{id:"model-configuration-llm_config",children:["Model Configuration (",(0,s.jsx)(n.code,{children:"llm_config"}),")"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"model"})}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"required"}),(0,s.jsxs)(n.td,{children:["Model identifier (e.g., ",(0,s.jsx)(n.code,{children:"openai:gpt-4o"}),")"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"temperature"})}),(0,s.jsx)(n.td,{children:"float"}),(0,s.jsx)(n.td,{children:"0.7"}),(0,s.jsx)(n.td,{children:"Creativity (0.0-1.0)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"max_tokens"})}),(0,s.jsx)(n.td,{children:"int"}),(0,s.jsx)(n.td,{children:"2000"}),(0,s.jsx)(n.td,{children:"Maximum response tokens"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"parallel_tool_calls"})}),(0,s.jsx)(n.td,{children:"bool"}),(0,s.jsx)(n.td,{children:"true"}),(0,s.jsx)(n.td,{children:"Execute tools in parallel"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"extra_settings"})}),(0,s.jsx)(n.td,{children:"dict"}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"Additional provider settings"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"tool-configuration",children:"Tool Configuration"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"name"})}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"\u2713"}),(0,s.jsx)(n.td,{children:"Unique tool identifier"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"type"})}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"\u2713"}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:"direct"})," or ",(0,s.jsx)(n.code,{children:"mcp"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"module"})}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"\u2713*"}),(0,s.jsx)(n.td,{children:"Python module path (for direct)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"function"})}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"\u2713*"}),(0,s.jsx)(n.td,{children:"Function name (for direct)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"description"})}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"\u2713"}),(0,s.jsx)(n.td,{children:"Tool description for LLM"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"json_schema"})}),(0,s.jsx)(n.td,{children:"object"}),(0,s.jsx)(n.td,{children:"\u2713"}),(0,s.jsx)(n.td,{children:"Parameter schema"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"category"})}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Tool category for filtering"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"tags"})}),(0,s.jsx)(n.td,{children:"list"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Tags for filtering"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"inline-output-type",children:"Inline Output Type"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"name"})}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"\u2713"}),(0,s.jsx)(n.td,{children:"Model class name"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"description"})}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Model description"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"fields"})}),(0,s.jsx)(n.td,{children:"dict"}),(0,s.jsx)(n.td,{children:"\u2713"}),(0,s.jsx)(n.td,{children:"Field definitions"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:["Field options: ",(0,s.jsx)(n.code,{children:"type"}),", ",(0,s.jsx)(n.code,{children:"description"}),", ",(0,s.jsx)(n.code,{children:"default"}),", ",(0,s.jsx)(n.code,{children:"default_factory"}),", ",(0,s.jsx)(n.code,{children:"required"})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"quick-start-vector-store",children:"Quick Start: Vector Store"}),"\n",(0,s.jsx)(n.p,{children:"Create a vector store for semantic search:"}),"\n",(0,s.jsx)(n.h3,{id:"using-faiss-in-memory",children:"Using FAISS (In-Memory)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_system_vectorstore.components.vectorstore.faiss_vector_store import FaissStore\nfrom rakam_system_vectorstore.core import Node, NodeMetadata\n\n# Create sample documents\ndocuments = [\n    "Python is a high-level programming language.",\n    "Machine learning is a subset of artificial intelligence.",\n    "Deep learning uses neural networks with multiple layers.",\n    "Natural language processing helps computers understand text.",\n    "Vector databases store data based on semantic similarity.",\n]\n\n# Create nodes\nnodes = []\nfor i, doc in enumerate(documents):\n    metadata = NodeMetadata(\n        source_file_uuid="docs_001",\n        position=i,\n        custom={"category": "tech"}\n    )\n    nodes.append(Node(content=doc, metadata=metadata))\n\n# Initialize FAISS store\nstore = FaissStore(\n    name="my_store",\n    base_index_path="./my_indexes",\n    embedding_model="Snowflake/snowflake-arctic-embed-m",\n    initialising=True\n)\n\n# Create collection and add nodes\nstore.create_collection_from_nodes("tech_docs", nodes)\n\n# Search\nquery = "What is machine learning?"\nresults, result_nodes = store.search(\n    collection_name="tech_docs",\n    query=query,\n    distance_type="cosine",\n    number=3\n)\n\nprint(f"Query: {query}\\n")\nfor node_id, (metadata, content, distance) in results.items():\n    print(f"  [{distance:.4f}] {content}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"using-postgresql-with-pgvector",children:"Using PostgreSQL with pgvector"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import os\nimport django\nfrom django.conf import settings\n\n# Configure Django (required for PostgreSQL backend)\nif not settings.configured:\n    settings.configure(\n        INSTALLED_APPS=[\n            'django.contrib.contenttypes',\n            'rakam_system_vectorstore.components.vectorstore',\n        ],\n        DATABASES={\n            'default': {\n                'ENGINE': 'django.db.backends.postgresql',\n                'NAME': os.getenv('POSTGRES_DB', 'vectorstore_db'),\n                'USER': os.getenv('POSTGRES_USER', 'postgres'),\n                'PASSWORD': os.getenv('POSTGRES_PASSWORD', 'postgres'),\n                'HOST': os.getenv('POSTGRES_HOST', 'localhost'),\n                'PORT': os.getenv('POSTGRES_PORT', '5432'),\n            }\n        },\n        DEFAULT_AUTO_FIELD='django.db.models.BigAutoField',\n    )\n    django.setup()\n\nfrom rakam_system_vectorstore import (\n    ConfigurablePgVectorStore,\n    VectorStoreConfig,\n    Node,\n    NodeMetadata\n)\n\n# Create configuration\nconfig = VectorStoreConfig(\n    name=\"my_pg_store\",\n    embedding={\n        \"model_type\": \"sentence_transformer\",\n        \"model_name\": \"Snowflake/snowflake-arctic-embed-m\"\n    },\n    search={\n        \"similarity_metric\": \"cosine\",\n        \"default_top_k\": 5\n    }\n)\n\n# Initialize store\nstore = ConfigurablePgVectorStore(config=config)\nstore.setup()\n\n# Add documents\nnodes = [\n    Node(\n        content=\"Python is great for data science.\",\n        metadata=NodeMetadata(source_file_uuid=\"doc1\", position=0)\n    ),\n    Node(\n        content=\"JavaScript runs in web browsers.\",\n        metadata=NodeMetadata(source_file_uuid=\"doc1\", position=1)\n    ),\n]\nstore.add_nodes(nodes)\n\n# Search\nresults = store.search(\"What language is good for data?\", top_k=3)\nfor r in results:\n    print(f\"[{r['score']:.4f}] {r['content']}\")\n\n# Cleanup\nstore.shutdown()\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"quick-start-configurable-vector-store",children:"Quick Start: Configurable Vector Store"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Configuration-Driven Vector Storage"}),": The ",(0,s.jsx)(n.code,{children:"ConfigurablePgVectorStore"})," lets you change your entire vector store setup without modifying code:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Switch embedding models"}),": From local to OpenAI embeddings by editing config"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Change search algorithms"}),": Toggle between BM25 and ts_rank for keyword search"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Adjust search behavior"}),": Change similarity metrics, hybrid search weights, etc."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tune performance"}),": Modify batch sizes, chunk sizes, all via YAML"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Perfect for"}),": Testing different embedding models, optimizing search relevance, environment-specific settings"]}),"\n",(0,s.jsx)(n.h3,{id:"vectorstoreconfig-overview",children:"VectorStoreConfig Overview"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_system_vectorstore.config import (\n    VectorStoreConfig,\n    EmbeddingConfig,\n    DatabaseConfig,\n    SearchConfig,\n    IndexConfig,\n    load_config\n)\n\n# Create configuration programmatically\nconfig = VectorStoreConfig(\n    name="production_store",\n\n    # Embedding configuration\n    embedding=EmbeddingConfig(\n        model_type="sentence_transformer",  # or "openai", "cohere"\n        model_name="Snowflake/snowflake-arctic-embed-m",\n        batch_size=128,\n        normalize=True,\n        # api_key loaded from env: OPENAI_API_KEY or COHERE_API_KEY\n    ),\n\n    # Database configuration (uses env vars by default)\n    database=DatabaseConfig(\n        host="localhost",  # or POSTGRES_HOST env var\n        port=5432,         # or POSTGRES_PORT env var\n        database="vectorstore_db",\n        user="postgres",\n        password="postgres",\n    ),\n\n    # Search configuration\n    search=SearchConfig(\n        similarity_metric="cosine",  # "cosine", "l2", "dot_product"\n        default_top_k=5,\n        enable_hybrid_search=True,   # Combine vector + keyword search\n        hybrid_alpha=0.7,            # Vector weight (0.3 for keyword)\n        rerank=True,\n    ),\n\n    # Indexing configuration\n    index=IndexConfig(\n        chunk_size=512,\n        chunk_overlap=50,\n        batch_insert_size=10000,\n    ),\n\n    # Additional options\n    enable_caching=True,\n    cache_size=1000,\n    enable_logging=True,\n    log_level="INFO",\n)\n\n# Validate configuration\nconfig.validate()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"configuration-from-yaml",children:"Configuration from YAML"}),"\n",(0,s.jsxs)(n.p,{children:["Create ",(0,s.jsx)(n.code,{children:"config/vectorstore.yaml"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"name: production_vectorstore\n\nembedding:\n  model_type: sentence_transformer\n  model_name: Snowflake/snowflake-arctic-embed-m\n  batch_size: 128\n  normalize: true\n\ndatabase:\n  host: localhost\n  port: 5432\n  database: vectorstore_db\n  user: postgres\n  password: postgres\n\nsearch:\n  similarity_metric: cosine\n  default_top_k: 5\n  enable_hybrid_search: true\n  hybrid_alpha: 0.7\n  rerank: true\n\nindex:\n  chunk_size: 512\n  chunk_overlap: 50\n  batch_insert_size: 10000\n\nenable_caching: true\ncache_size: 1000\nenable_logging: true\nlog_level: INFO\n"})}),"\n",(0,s.jsx)(n.p,{children:"Load and use:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_system_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig\n\n# Load from YAML - all behavior controlled by config!\nconfig = VectorStoreConfig.from_yaml("config/vectorstore.yaml")\n\n# Or load from JSON\nconfig = VectorStoreConfig.from_json("config/vectorstore.json")\n\n# Or use the helper function (auto-detects format)\nfrom rakam_system_vectorstore.config import load_config\nconfig = load_config("config/vectorstore.yaml")\n\n# Create store - behavior defined entirely by config\nstore = ConfigurablePgVectorStore(config=config)\nstore.setup()\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example: Switching Embedding Models Without Code Changes"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# config/vectorstore_local.yaml - Use local embeddings (free, private)\nembedding:\n  model_type: sentence_transformer\n  model_name: Snowflake/snowflake-arctic-embed-m\n\n# config/vectorstore_openai.yaml - Use OpenAI embeddings (better quality)\nembedding:\n  model_type: openai\n  model_name: text-embedding-3-large\n\n# Application code stays the same - just change config file!\n# config = VectorStoreConfig.from_yaml("config/vectorstore_openai.yaml")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example: Enabling Hybrid Search Without Code Changes"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# Before: Vector search only\nsearch:\n  similarity_metric: cosine\n  default_top_k: 5\n  enable_hybrid_search: false\n\n# After: Hybrid search enabled - just update config!\nsearch:\n  similarity_metric: cosine\n  default_top_k: 5\n  enable_hybrid_search: true\n  hybrid_alpha: 0.7\n"})}),"\n",(0,s.jsx)(n.h3,{id:"using-different-embedding-models",children:"Using Different Embedding Models"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_system_vectorstore.config import VectorStoreConfig, EmbeddingConfig\n\n# Local embeddings with Sentence Transformers\nconfig_local = VectorStoreConfig(\n    embedding=EmbeddingConfig(\n        model_type="sentence_transformer",\n        model_name="Snowflake/snowflake-arctic-embed-m",  # 768 dimensions\n        # model_name="all-MiniLM-L6-v2",                  # 384 dimensions\n        batch_size=128,\n    )\n)\n\n# OpenAI embeddings\nconfig_openai = VectorStoreConfig(\n    embedding=EmbeddingConfig(\n        model_type="openai",\n        model_name="text-embedding-3-small",\n        # api_key loaded from OPENAI_API_KEY env var\n    )\n)\n\n# Cohere embeddings\nconfig_cohere = VectorStoreConfig(\n    embedding=EmbeddingConfig(\n        model_type="cohere",\n        model_name="embed-english-v3.0",\n        # api_key loaded from COHERE_API_KEY env var\n    )\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"multi-model-support",children:"Multi-Model Support"}),"\n",(0,s.jsx)(n.p,{children:"Each embedding model automatically gets dedicated tables, preventing mixing of incompatible vector spaces:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_system_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig, EmbeddingConfig\n\n# Store using MiniLM model\nconfig_minilm = VectorStoreConfig(\n    embedding=EmbeddingConfig(\n        model_type="sentence_transformer",\n        model_name="all-MiniLM-L6-v2"  # 384D\n    )\n)\nstore_minilm = ConfigurablePgVectorStore(config=config_minilm)\n# Tables: application_nodeentry_all_minilm_l6_v2\n\n# Store using Arctic model\nconfig_arctic = VectorStoreConfig(\n    embedding=EmbeddingConfig(\n        model_type="sentence_transformer",\n        model_name="Snowflake/snowflake-arctic-embed-m"  # 768D\n    )\n)\nstore_arctic = ConfigurablePgVectorStore(config=config_arctic)\n# Tables: application_nodeentry_snowflake_arctic_embed_m\n\n# Both can coexist without conflicts!\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Important"}),": Even if two models have the same dimensions, their vector spaces are different! Model-specific tables prevent meaningless search results from mixed embeddings."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"hybrid-search",children:"Hybrid Search"}),"\n",(0,s.jsx)(n.p,{children:"Combine vector similarity with keyword search:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_system_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig\n\nconfig = VectorStoreConfig()\nconfig.search.enable_hybrid_search = True\nconfig.search.hybrid_alpha = 0.7  # 70% vector, 30% keyword\n\nstore = ConfigurablePgVectorStore(config=config)\nstore.setup()\n\n# Regular search (uses config defaults)\nresults = store.search("machine learning algorithms", top_k=10)\n\n# Hybrid search with custom alpha\nresults = store.hybrid_search(\n    query="machine learning algorithms",\n    top_k=10,\n    alpha=0.5  # 50/50 split\n)\n\nfor r in results:\n    print(f"[{r[\'score\']:.4f}] {r[\'content\'][:100]}...")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"keyword-search",children:"Keyword Search"}),"\n",(0,s.jsx)(n.p,{children:"Full-text search using PostgreSQL's BM25 or ts_rank:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_system_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig\n\n# Configure keyword search\nconfig = VectorStoreConfig(\n    search={\n        "keyword_ranking_algorithm": "bm25",  # or "ts_rank"\n        "keyword_k1": 1.2,  # BM25 k1 parameter\n        "keyword_b": 0.75   # BM25 b parameter\n    }\n)\n\nstore = ConfigurablePgVectorStore(config=config)\nstore.setup()\n\n# Add some documents\nfrom rakam_system_vectorstore import Node, NodeMetadata\n\nnodes = [\n    Node(\n        content="Machine learning is a subset of artificial intelligence.",\n        metadata=NodeMetadata(source_file_uuid="doc1", position=0)\n    ),\n    Node(\n        content="Deep learning uses neural networks with multiple layers.",\n        metadata=NodeMetadata(source_file_uuid="doc1", position=1)\n    ),\n    Node(\n        content="Natural language processing helps computers understand text.",\n        metadata=NodeMetadata(source_file_uuid="doc1", position=2)\n    ),\n]\nstore.add_nodes(nodes)\n\n# Keyword search with BM25\nresults = store.keyword_search(\n    query="machine learning neural networks",\n    top_k=5,\n    ranking_algorithm="bm25"\n)\n\nprint("Keyword Search Results (BM25):")\nfor r in results:\n    print(f"  [{r[\'score\']:.4f}] {r[\'content\'][:80]}...")\n\n# Keyword search with ts_rank\nresults = store.keyword_search(\n    query="artificial intelligence",\n    top_k=5,\n    ranking_algorithm="ts_rank"\n)\n\nprint("\\nKeyword Search Results (ts_rank):")\nfor r in results:\n    print(f"  [{r[\'score\']:.4f}] {r[\'content\'][:80]}...")\n\nstore.shutdown()\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"When to use:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"BM25"}),": Best for general text search, handles term frequency well"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ts_rank"}),": Good for structured documents with varying importance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hybrid Search"}),": Combines semantic (vector) + keyword for best results"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"full-example-with-all-features",children:"Full Example with All Features"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import os\nimport django\nfrom django.conf import settings\n\n# Configure Django\nif not settings.configured:\n    settings.configure(\n        INSTALLED_APPS=[\n            'django.contrib.contenttypes',\n            'rakam_system_vectorstore.components.vectorstore',\n        ],\n        DATABASES={\n            'default': {\n                'ENGINE': 'django.db.backends.postgresql',\n                'NAME': os.getenv('POSTGRES_DB', 'vectorstore_db'),\n                'USER': os.getenv('POSTGRES_USER', 'postgres'),\n                'PASSWORD': os.getenv('POSTGRES_PASSWORD', 'postgres'),\n                'HOST': os.getenv('POSTGRES_HOST', 'localhost'),\n                'PORT': os.getenv('POSTGRES_PORT', '5432'),\n            }\n        },\n        DEFAULT_AUTO_FIELD='django.db.models.BigAutoField',\n    )\n    django.setup()\n\nfrom rakam_system_vectorstore import (\n    ConfigurablePgVectorStore,\n    VectorStoreConfig,\n    Node,\n    NodeMetadata,\n    VSFile\n)\nfrom rakam_system_vectorstore.components.loader import AdaptiveLoader\n\n# Create configuration\nconfig = VectorStoreConfig.from_yaml(\"config/vectorstore.yaml\")\n\n# Initialize store with context manager\nwith ConfigurablePgVectorStore(config=config) as store:\n\n    # Load documents\n    loader = AdaptiveLoader(config={\n        \"chunk_size\": config.index.chunk_size,\n        \"chunk_overlap\": config.index.chunk_overlap\n    })\n\n    # Load from file\n    vsfile = loader.load_as_vsfile(\"documents/manual.pdf\")\n    store.add_vsfile(vsfile)\n\n    # Or add nodes directly\n    nodes = [\n        Node(\n            content=\"Python is excellent for data science and machine learning.\",\n            metadata=NodeMetadata(\n                source_file_uuid=\"doc_001\",\n                position=0,\n                custom={\"category\": \"programming\", \"topic\": \"python\"}\n            )\n        ),\n        Node(\n            content=\"PostgreSQL with pgvector enables efficient vector similarity search.\",\n            metadata=NodeMetadata(\n                source_file_uuid=\"doc_001\",\n                position=1,\n                custom={\"category\": \"database\", \"topic\": \"vectors\"}\n            )\n        ),\n    ]\n    store.add_nodes(nodes)\n\n    # Search with filters\n    results = store.search(\n        query=\"vector database search\",\n        top_k=5,\n        # Metadata filters can be applied here\n    )\n\n    print(\"Search Results:\")\n    for r in results:\n        print(f\"  [{r['score']:.4f}] {r['content'][:80]}...\")\n        print(f\"           Source: {r.get('source_file_uuid', 'N/A')}\")\n\n    # Hybrid search\n    print(\"\\nHybrid Search Results:\")\n    hybrid_results = store.hybrid_search(\n        query=\"PostgreSQL vector\",\n        top_k=5,\n        alpha=0.6\n    )\n    for r in hybrid_results:\n        print(f\"  [{r['score']:.4f}] {r['content'][:80]}...\")\n\n    # Get collection stats\n    count = store.count()\n    print(f\"\\nTotal vectors in store: {count}\")\n\n# Store automatically cleaned up\n"})}),"\n",(0,s.jsx)(n.h3,{id:"configuration-reference",children:"Configuration Reference"}),"\n",(0,s.jsx)(n.h4,{id:"embeddingconfig",children:"EmbeddingConfig"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"model_type"})}),(0,s.jsx)(n.td,{children:"str"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"sentence_transformer"})}),(0,s.jsxs)(n.td,{children:["Backend: ",(0,s.jsx)(n.code,{children:"sentence_transformer"}),", ",(0,s.jsx)(n.code,{children:"openai"}),", ",(0,s.jsx)(n.code,{children:"cohere"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"model_name"})}),(0,s.jsx)(n.td,{children:"str"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"Snowflake/snowflake-arctic-embed-m"})}),(0,s.jsx)(n.td,{children:"Model identifier"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"api_key"})}),(0,s.jsx)(n.td,{children:"str"}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsx)(n.td,{children:"API key (auto-loaded from env)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"batch_size"})}),(0,s.jsx)(n.td,{children:"int"}),(0,s.jsx)(n.td,{children:"128"}),(0,s.jsx)(n.td,{children:"Batch size for encoding"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"normalize"})}),(0,s.jsx)(n.td,{children:"bool"}),(0,s.jsx)(n.td,{children:"True"}),(0,s.jsx)(n.td,{children:"Normalize embeddings"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"dimensions"})}),(0,s.jsx)(n.td,{children:"int"}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsx)(n.td,{children:"Vector dimensions (auto-detected)"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"databaseconfig",children:"DatabaseConfig"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"host"})}),(0,s.jsx)(n.td,{children:"str"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"localhost"})}),(0,s.jsxs)(n.td,{children:["PostgreSQL host (or ",(0,s.jsx)(n.code,{children:"POSTGRES_HOST"})," env)"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"port"})}),(0,s.jsx)(n.td,{children:"int"}),(0,s.jsx)(n.td,{children:"5432"}),(0,s.jsxs)(n.td,{children:["PostgreSQL port (or ",(0,s.jsx)(n.code,{children:"POSTGRES_PORT"})," env)"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"database"})}),(0,s.jsx)(n.td,{children:"str"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"vectorstore_db"})}),(0,s.jsxs)(n.td,{children:["Database name (or ",(0,s.jsx)(n.code,{children:"POSTGRES_DB"})," env)"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"user"})}),(0,s.jsx)(n.td,{children:"str"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"postgres"})}),(0,s.jsxs)(n.td,{children:["Username (or ",(0,s.jsx)(n.code,{children:"POSTGRES_USER"})," env)"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"password"})}),(0,s.jsx)(n.td,{children:"str"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"postgres"})}),(0,s.jsxs)(n.td,{children:["Password (or ",(0,s.jsx)(n.code,{children:"POSTGRES_PASSWORD"})," env)"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"pool_size"})}),(0,s.jsx)(n.td,{children:"int"}),(0,s.jsx)(n.td,{children:"10"}),(0,s.jsx)(n.td,{children:"Connection pool size"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"searchconfig",children:"SearchConfig"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"similarity_metric"})}),(0,s.jsx)(n.td,{children:"str"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"cosine"})}),(0,s.jsxs)(n.td,{children:["Metric: ",(0,s.jsx)(n.code,{children:"cosine"}),", ",(0,s.jsx)(n.code,{children:"l2"}),", ",(0,s.jsx)(n.code,{children:"dot_product"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"default_top_k"})}),(0,s.jsx)(n.td,{children:"int"}),(0,s.jsx)(n.td,{children:"5"}),(0,s.jsx)(n.td,{children:"Default results count"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"enable_hybrid_search"})}),(0,s.jsx)(n.td,{children:"bool"}),(0,s.jsx)(n.td,{children:"True"}),(0,s.jsx)(n.td,{children:"Enable keyword + vector search"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"hybrid_alpha"})}),(0,s.jsx)(n.td,{children:"float"}),(0,s.jsx)(n.td,{children:"0.7"}),(0,s.jsx)(n.td,{children:"Vector weight (1-alpha for keyword)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"rerank"})}),(0,s.jsx)(n.td,{children:"bool"}),(0,s.jsx)(n.td,{children:"True"}),(0,s.jsx)(n.td,{children:"Rerank results"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"indexconfig",children:"IndexConfig"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"chunk_size"})}),(0,s.jsx)(n.td,{children:"int"}),(0,s.jsx)(n.td,{children:"512"}),(0,s.jsx)(n.td,{children:"Chunk size in tokens"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"chunk_overlap"})}),(0,s.jsx)(n.td,{children:"int"}),(0,s.jsx)(n.td,{children:"50"}),(0,s.jsx)(n.td,{children:"Overlap between chunks"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"batch_insert_size"})}),(0,s.jsx)(n.td,{children:"int"}),(0,s.jsx)(n.td,{children:"10000"}),(0,s.jsx)(n.td,{children:"Batch size for inserts"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"quick-start-rag-pipeline",children:"Quick Start: RAG Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"Build a complete Retrieval-Augmented Generation pipeline:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_system_agent import BaseAgent\nfrom rakam_system_vectorstore.components.vectorstore.faiss_vector_store import FaissStore\nfrom rakam_system_vectorstore.components.loader import AdaptiveLoader\nfrom rakam_system_core.ai_core.interfaces.tool import ToolComponent\n\n# Step 1: Load and index documents\nloader = AdaptiveLoader(config={"chunk_size": 512, "chunk_overlap": 50})\n\n# Load documents (supports PDF, DOCX, TXT, MD, HTML, etc.)\n# nodes = loader.load_as_nodes("path/to/document.pdf")\n\n# For demo, use sample text\nfrom rakam_system_vectorstore.core import Node, NodeMetadata\n\nsample_docs = [\n    "Our company was founded in 2020 and specializes in AI solutions.",\n    "We offer three main products: AI Assistant, Data Analytics, and Automation.",\n    "The AI Assistant can handle customer queries 24/7 with 95% accuracy.",\n    "Our pricing starts at $99/month for small businesses.",\n    "Enterprise customers get dedicated support and custom integrations.",\n    "We have offices in New York, London, and Tokyo.",\n]\n\nnodes = [\n    Node(content=doc, metadata=NodeMetadata(source_file_uuid="company_info", position=i))\n    for i, doc in enumerate(sample_docs)\n]\n\n# Step 2: Create vector store\nstore = FaissStore(\n    name="rag_store",\n    base_index_path="./rag_indexes",\n    embedding_model="Snowflake/snowflake-arctic-embed-m",\n    initialising=True\n)\nstore.create_collection_from_nodes("knowledge_base", nodes)\n\n# Step 3: Create search tool\ndef search_knowledge_base(query: str, top_k: int = 3) -> str:\n    """Search the knowledge base for relevant information."""\n    results, _ = store.search(\n        collection_name="knowledge_base",\n        query=query,\n        distance_type="cosine",\n        number=top_k\n    )\n\n    context = "\\n".join([\n        f"- {content}" for _, (_, content, _) in results.items()\n    ])\n    return f"Relevant information:\\n{context}"\n\nsearch_tool = ToolComponent.from_function(\n    function=search_knowledge_base,\n    name="search_knowledge_base",\n    description="Search the company knowledge base for information",\n    json_schema={\n        "type": "object",\n        "properties": {\n            "query": {"type": "string", "description": "Search query"},\n            "top_k": {"type": "integer", "default": 3, "description": "Number of results"}\n        },\n        "required": ["query"],\n        "additionalProperties": False\n    }\n)\n\n# Step 4: Create RAG agent\nasync def main():\n    agent = BaseAgent(\n        name="rag_agent",\n        model="openai:gpt-4o",\n        system_prompt="""You are a helpful customer service agent for our company.\nUse the search_knowledge_base tool to find relevant information before answering.\nAlways base your answers on the retrieved information.""",\n        tools=[search_tool]\n    )\n\n    # Ask questions\n    questions = [\n        "When was the company founded?",\n        "What products do you offer?",\n        "How much does the service cost?",\n    ]\n\n    for question in questions:\n        print(f"\\nQ: {question}")\n        result = await agent.arun(question)\n        print(f"A: {result.output_text}")\n\nasyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"quick-start-llm-gateway",children:"Quick Start: LLM Gateway"}),"\n",(0,s.jsx)(n.p,{children:"Use the LLM Gateway for direct LLM interactions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_system_agent.components.llm_gateway import (\n    OpenAIGateway,\n    MistralGateway,\n    LLMGatewayFactory,\n    LLMRequest\n)\n\n# Create gateway using factory\ngateway = LLMGatewayFactory.create(\n    provider="openai",\n    model="gpt-4o"\n)\n\n# Simple text generation\nrequest = LLMRequest(\n    system_prompt="You are a helpful assistant.",\n    user_prompt="Explain quantum computing in simple terms.",\n    temperature=0.7,\n    max_tokens=200\n)\n\nresponse = gateway.generate(request)\nprint(f"Response: {response.content}")\nprint(f"Tokens used: {response.usage}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"structured-output-with-gateway",children:"Structured Output with Gateway"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from pydantic import BaseModel\nfrom rakam_system_agent.components.llm_gateway import OpenAIGateway, LLMRequest\n\nclass Recipe(BaseModel):\n    name: str\n    ingredients: list[str]\n    instructions: list[str]\n    prep_time_minutes: int\n    servings: int\n\ngateway = OpenAIGateway(model="gpt-4o")\n\nrequest = LLMRequest(\n    system_prompt="You are a chef. Create recipes based on user requests.",\n    user_prompt="Give me a simple pasta recipe."\n)\n\nrecipe = gateway.generate_structured(request, Recipe)\nprint(f"Recipe: {recipe.name}")\nprint(f"Prep time: {recipe.prep_time_minutes} minutes")\nprint(f"Ingredients: {\', \'.join(recipe.ingredients)}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"streaming-with-gateway",children:"Streaming with Gateway"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_system_agent.components.llm_gateway import OpenAIGateway, LLMRequest\n\ngateway = OpenAIGateway(model="gpt-4o")\n\nrequest = LLMRequest(\n    user_prompt="Write a poem about coding.",\n    temperature=0.8\n)\n\nprint("Poem:\\n")\nfor chunk in gateway.stream(request):\n    print(chunk, end="", flush=True)\nprint()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"quick-start-document-loading",children:"Quick Start: Document Loading"}),"\n",(0,s.jsx)(n.p,{children:"Load and process various document types:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_system_vectorstore.components.loader import AdaptiveLoader\n\n# Create loader\nloader = AdaptiveLoader(config={\n    "chunk_size": 512,\n    "chunk_overlap": 50,\n    "encoding": "utf-8"\n})\n\n# Load as text\ntext = loader.load_as_text("document.pdf")\nprint(f"Loaded {len(text)} characters")\n\n# Load as chunks\nchunks = loader.load_as_chunks("document.pdf")\nprint(f"Created {len(chunks)} chunks")\n\n# Load as nodes (with metadata)\nnodes = loader.load_as_nodes(\n    "document.pdf",\n    custom_metadata={"category": "technical", "author": "John"}\n)\nprint(f"Created {len(nodes)} nodes")\n\n# Load as VSFile (complete document representation)\nvsfile = loader.load_as_vsfile("document.pdf")\nprint(f"File: {vsfile.file_name}, UUID: {vsfile.uuid}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"supported-file-types",children:"Supported File Types"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Extensions"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Text"})}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:".txt"}),", ",(0,s.jsx)(n.code,{children:".text"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Markdown"})}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:".md"}),", ",(0,s.jsx)(n.code,{children:".markdown"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"PDF"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:".pdf"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Word"})}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:".docx"}),", ",(0,s.jsx)(n.code,{children:".doc"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"OpenDocument"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:".odt"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"HTML"})}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:".html"}),", ",(0,s.jsx)(n.code,{children:".htm"}),", ",(0,s.jsx)(n.code,{children:".xhtml"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Email"})}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:".eml"}),", ",(0,s.jsx)(n.code,{children:".msg"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Data"})}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:".json"}),", ",(0,s.jsx)(n.code,{children:".csv"}),", ",(0,s.jsx)(n.code,{children:".tsv"}),", ",(0,s.jsx)(n.code,{children:".xlsx"}),", ",(0,s.jsx)(n.code,{children:".xls"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Code"})}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:".py"}),", ",(0,s.jsx)(n.code,{children:".js"}),", ",(0,s.jsx)(n.code,{children:".ts"}),", ",(0,s.jsx)(n.code,{children:".java"}),", ",(0,s.jsx)(n.code,{children:".cpp"}),", ",(0,s.jsx)(n.code,{children:".go"}),", ",(0,s.jsx)(n.code,{children:".rs"}),", ",(0,s.jsx)(n.code,{children:".rb"})]})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"quick-start-configuration-from-yaml",children:"Quick Start: Configuration from YAML"}),"\n",(0,s.jsx)(n.p,{children:"Load agents and tools from configuration files:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# config/agents.yaml\nversion: "1.0"\n\nprompts:\n  assistant:\n    system_prompt: |\n      You are a helpful AI assistant.\n      Always be accurate and concise.\n\ntools:\n  get_time:\n    name: "get_time"\n    type: "direct"\n    module: "datetime"\n    function: "datetime.now"\n    description: "Get current date and time"\n    json_schema:\n      type: "object"\n      properties: {}\n\nagents:\n  main_agent:\n    name: "main_agent"\n    model_config:\n      model: "openai:gpt-4o"\n      temperature: 0.7\n    prompt_config: "assistant"\n    tools:\n      - "get_time"\n    enable_tracking: true\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_system_core.ai_core.config_loader import ConfigurationLoader\n\nloader = ConfigurationLoader()\n\n# Load configuration\nconfig = loader.load_from_yaml("config/agents.yaml")\n\n# Create agent from config\nagent = loader.create_agent("main_agent", config)\n\n# Use the agent\nresult = await agent.arun("What time is it?")\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"Now that you've completed the quick start:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Read the Full Documentation"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/rakam-systems-docs/getting-started/components",children:"Components Guide"})," - Detailed component documentation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/rakam-systems-docs/getting-started/development_guide",children:"Development Guide"})," - How to extend the framework"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/rakam-systems-docs/getting-started/installation",children:"Installation Guide"})," - Advanced installation options"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Explore More Examples"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"rakam_systems/examples/ai_agents_examples/"})," - Agent examples"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"rakam_systems/examples/ai_vectorstore_examples/"})," - Vector store examples"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"rakam_systems/examples/configs/"})," - Configuration examples"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Build Your Application"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Start with a simple agent"}),"\n",(0,s.jsx)(n.li,{children:"Add tools for your specific use case"}),"\n",(0,s.jsx)(n.li,{children:"Integrate vector storage for RAG"}),"\n",(0,s.jsx)(n.li,{children:"Scale with PostgreSQL for production"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Join the Community"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/Rakam-AI/rakam_systems",children:"GitHub Repository"})}),"\n",(0,s.jsx)(n.li,{children:"Report issues and request features"}),"\n",(0,s.jsx)(n.li,{children:"Contribute to the project"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"common-patterns",children:"Common Patterns"}),"\n",(0,s.jsx)(n.h3,{id:"error-handling",children:"Error Handling"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_system_agent import BaseAgent\n\nasync def main():\n    agent = BaseAgent(\n        name="safe_agent",\n        model="openai:gpt-4o",\n        system_prompt="You are helpful."\n    )\n\n    try:\n        result = await agent.arun("Hello!")\n        print(result.output_text)\n    except Exception as e:\n        print(f"Error: {e}")\n\nasyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.h3,{id:"context-manager-pattern",children:"Context Manager Pattern"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from rakam_system_vectorstore import ConfigurablePgVectorStore\n\n# Automatic setup and cleanup\nwith ConfigurablePgVectorStore(config=config) as store:\n    store.add_nodes(nodes)\n    results = store.search("query")\n# shutdown() called automatically\n'})}),"\n",(0,s.jsx)(n.h3,{id:"async-batch-processing",children:"Async Batch Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom rakam_system_agent import BaseAgent\n\nasync def process_queries(queries: list[str]):\n    agent = BaseAgent(\n        name="batch_agent",\n        model="openai:gpt-4o",\n        system_prompt="Answer concisely."\n    )\n\n    # Process queries concurrently\n    tasks = [agent.arun(q) for q in queries]\n    results = await asyncio.gather(*tasks)\n\n    return [r.output_text for r in results]\n\nqueries = ["What is Python?", "What is JavaScript?", "What is Rust?"]\nanswers = asyncio.run(process_queries(queries))\nfor q, a in zip(queries, answers):\n    print(f"Q: {q}\\nA: {a}\\n")\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Happy Building! \ud83d\ude80"})})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>a});var r=t(6540);const s={},i=r.createContext(s);function o(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);