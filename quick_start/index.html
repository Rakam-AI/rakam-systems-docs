<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Quick Start - rakam_system docs</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Quick Start";
        var mkdocs_page_input_path = "quick_start.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> rakam_system docs
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../contributing/">Contributing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../start/">Getting Started</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Quick Start</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#table-of-contents">ðŸ“‘ Table of Contents</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#prerequisites">Prerequisites</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#quick-start-ai-agent">Quick Start: AI Agent</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#with-streaming">With Streaming</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#with-model-settings">With Model Settings</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#quick-start-agent-with-tools">Quick Start: Agent with Tools</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#quick-start-structured-output">Quick Start: Structured Output</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#quick-start-configurable-agent">Quick Start: Configurable Agent</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#agent-configuration-file">Agent Configuration File</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#loading-agents-from-configuration">Loading Agents from Configuration</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using-tool-registry">Using Tool Registry</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#configuration-options-reference">Configuration Options Reference</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#model-configuration-llm_config">Model Configuration (llm_config)</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#tool-configuration">Tool Configuration</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#inline-output-type">Inline Output Type</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#quick-start-vector-store">Quick Start: Vector Store</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#using-faiss-in-memory">Using FAISS (In-Memory)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using-postgresql-with-pgvector">Using PostgreSQL with pgvector</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#quick-start-configurable-vector-store">Quick Start: Configurable Vector Store</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#vectorstoreconfig-overview">VectorStoreConfig Overview</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#configuration-from-yaml">Configuration from YAML</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using-different-embedding-models">Using Different Embedding Models</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#multi-model-support">Multi-Model Support</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#hybrid-search">Hybrid Search</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#full-example-with-all-features">Full Example with All Features</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#configuration-reference">Configuration Reference</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#embeddingconfig">EmbeddingConfig</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#databaseconfig">DatabaseConfig</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#searchconfig">SearchConfig</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#indexconfig">IndexConfig</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#quick-start-rag-pipeline">Quick Start: RAG Pipeline</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#quick-start-llm-gateway">Quick Start: LLM Gateway</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#structured-output-with-gateway">Structured Output with Gateway</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#streaming-with-gateway">Streaming with Gateway</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#quick-start-document-loading">Quick Start: Document Loading</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#supported-file-types">Supported File Types</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#quick-start-configuration-from-yaml">Quick Start: Configuration from YAML</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#next-steps">Next Steps</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#common-patterns">Common Patterns</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#error-handling">Error Handling</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#context-manager-pattern">Context Manager Pattern</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#async-batch-processing">Async Batch Processing</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../installation/">Installation</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../tutorials/">Tutorials</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../development_guide/">Development Guid</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../components/">Components</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../pipelines/">Pipelines</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../gradio/">Gradio</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../deployment/">Deployment & Operation</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../templates/">Templates</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">rakam_system docs</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Quick Start</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="rakam-systems-quick-start-guide">Rakam Systems Quick Start Guide</h1>
<p>Get up and running with <code>rakam_systems</code> in 5 minutes!</p>
<h2 id="table-of-contents">ðŸ“‘ Table of Contents</h2>
<ul>
<li><a href="#prerequisites">Prerequisites</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#quick-start-ai-agent">Quick Start: AI Agent</a></li>
<li><a href="#quick-start-agent-with-tools">Quick Start: Agent with Tools</a></li>
<li><a href="#quick-start-structured-output">Quick Start: Structured Output</a></li>
<li><a href="#quick-start-configurable-agent">Quick Start: Configurable Agent</a></li>
<li><a href="#quick-start-vector-store">Quick Start: Vector Store</a></li>
<li><a href="#quick-start-configurable-vector-store">Quick Start: Configurable Vector Store</a></li>
<li><a href="#quick-start-rag-pipeline">Quick Start: RAG Pipeline</a></li>
<li><a href="#quick-start-llm-gateway">Quick Start: LLM Gateway</a></li>
<li><a href="#quick-start-document-loading">Quick Start: Document Loading</a></li>
<li><a href="#quick-start-configuration-from-yaml">Quick Start: Configuration from YAML</a></li>
<li><a href="#common-patterns">Common Patterns</a></li>
<li><a href="#next-steps">Next Steps</a></li>
</ul>
<hr />
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>Python 3.10 or higher</li>
<li>OpenAI API key (for most examples)</li>
<li>pip package manager</li>
</ul>
<hr />
<h2 id="installation">Installation</h2>
<pre><code class="language-bash"># Navigate to the package
cd app/rakam_systems

# Install AI Agents module
pip install -e &quot;.[ai-agents]&quot;

# Or install everything
pip install -e &quot;.[all]&quot;
</code></pre>
<p>Set your API key:</p>
<pre><code class="language-bash">export OPENAI_API_KEY=&quot;sk-your-api-key&quot;
</code></pre>
<hr />
<h2 id="quick-start-ai-agent">Quick Start: AI Agent</h2>
<p>Create a simple AI agent in just a few lines:</p>
<pre><code class="language-python">import asyncio
from dotenv import load_dotenv

load_dotenv()

from rakam_systems.ai_agents import BaseAgent

async def main():
    # Create an agent
    agent = BaseAgent(
        name=&quot;my_assistant&quot;,
        model=&quot;openai:gpt-4o&quot;,
        system_prompt=&quot;You are a helpful assistant that provides concise answers.&quot;
    )

    # Ask a question
    result = await agent.arun(&quot;What is Python?&quot;)
    print(result.output_text)

asyncio.run(main())
</code></pre>
<h3 id="with-streaming">With Streaming</h3>
<pre><code class="language-python">import asyncio
from rakam_systems.ai_agents import BaseAgent

async def main():
    agent = BaseAgent(
        name=&quot;streaming_agent&quot;,
        model=&quot;openai:gpt-4o&quot;,
        system_prompt=&quot;You are a helpful assistant.&quot;
    )

    # Stream the response
    print(&quot;Response: &quot;, end=&quot;&quot;, flush=True)
    async for chunk in agent.astream(&quot;Tell me a short story about a robot.&quot;):
        print(chunk, end=&quot;&quot;, flush=True)
    print()

asyncio.run(main())
</code></pre>
<h3 id="with-model-settings">With Model Settings</h3>
<pre><code class="language-python">import asyncio
from rakam_systems.ai_agents import BaseAgent
from rakam_systems.ai_core.interfaces import ModelSettings

async def main():
    agent = BaseAgent(
        name=&quot;creative_agent&quot;,
        model=&quot;openai:gpt-4o&quot;,
        system_prompt=&quot;You are a creative writer.&quot;
    )

    # Use custom temperature and max tokens
    result = await agent.arun(
        &quot;Write a haiku about programming.&quot;,
        model_settings=ModelSettings(temperature=0.9, max_tokens=100)
    )
    print(result.output_text)

asyncio.run(main())
</code></pre>
<hr />
<h2 id="quick-start-agent-with-tools">Quick Start: Agent with Tools</h2>
<p>Create an agent that can use tools:</p>
<pre><code class="language-python">import asyncio
from rakam_systems.ai_agents import BaseAgent
from rakam_systems.ai_core.interfaces.tool import ToolComponent

# Define a tool function
def get_weather(city: str, units: str = &quot;celsius&quot;) -&gt; str:
    &quot;&quot;&quot;Get weather for a city (mock implementation).&quot;&quot;&quot;
    return f&quot;Weather in {city}: 22Â°{'C' if units == 'celsius' else 'F'}, Sunny&quot;

def calculate(expression: str) -&gt; str:
    &quot;&quot;&quot;Calculate a mathematical expression.&quot;&quot;&quot;
    try:
        result = eval(expression)
        return f&quot;Result: {result}&quot;
    except Exception as e:
        return f&quot;Error: {e}&quot;

# Create tool components
weather_tool = ToolComponent.from_function(
    function=get_weather,
    name=&quot;get_weather&quot;,
    description=&quot;Get the current weather for a city&quot;,
    json_schema={
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;city&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;City name&quot;},
            &quot;units&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;],
                &quot;default&quot;: &quot;celsius&quot;
            }
        },
        &quot;required&quot;: [&quot;city&quot;],
        &quot;additionalProperties&quot;: False
    }
)

calculator_tool = ToolComponent.from_function(
    function=calculate,
    name=&quot;calculator&quot;,
    description=&quot;Calculate a mathematical expression&quot;,
    json_schema={
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;expression&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Math expression to evaluate&quot;}
        },
        &quot;required&quot;: [&quot;expression&quot;],
        &quot;additionalProperties&quot;: False
    }
)

async def main():
    # Create agent with tools
    agent = BaseAgent(
        name=&quot;tool_agent&quot;,
        model=&quot;openai:gpt-4o&quot;,
        system_prompt=&quot;You are a helpful assistant with access to tools.&quot;,
        tools=[weather_tool, calculator_tool]
    )

    # Ask questions that use tools
    result = await agent.arun(&quot;What's the weather in Paris?&quot;)
    print(f&quot;Weather: {result.output_text}\n&quot;)

    result = await agent.arun(&quot;What is 25 * 4 + 100?&quot;)
    print(f&quot;Calculation: {result.output_text}&quot;)

asyncio.run(main())
</code></pre>
<hr />
<h2 id="quick-start-structured-output">Quick Start: Structured Output</h2>
<p>Get structured, typed responses from your agent:</p>
<pre><code class="language-python">import asyncio
from pydantic import BaseModel, Field
from rakam_systems.ai_agents import BaseAgent

# Define output structure
class MovieReview(BaseModel):
    title: str = Field(description=&quot;Movie title&quot;)
    rating: float = Field(ge=0, le=10, description=&quot;Rating out of 10&quot;)
    summary: str = Field(description=&quot;Brief summary&quot;)
    pros: list[str] = Field(description=&quot;List of positive aspects&quot;)
    cons: list[str] = Field(description=&quot;List of negative aspects&quot;)
    recommended: bool = Field(description=&quot;Whether to recommend&quot;)

async def main():
    # Create agent with structured output
    agent = BaseAgent(
        name=&quot;movie_critic&quot;,
        model=&quot;openai:gpt-4o&quot;,
        system_prompt=&quot;You are a movie critic. Analyze movies thoroughly.&quot;,
        output_type=MovieReview  # Enforces structured output
    )

    result = await agent.arun(&quot;Review the movie 'Inception' by Christopher Nolan&quot;)

    # Access typed output
    review: MovieReview = result.output
    print(f&quot;Title: {review.title}&quot;)
    print(f&quot;Rating: {review.rating}/10&quot;)
    print(f&quot;Summary: {review.summary}&quot;)
    print(f&quot;Pros: {', '.join(review.pros)}&quot;)
    print(f&quot;Cons: {', '.join(review.cons)}&quot;)
    print(f&quot;Recommended: {'Yes' if review.recommended else 'No'}&quot;)

asyncio.run(main())
</code></pre>
<hr />
<h2 id="quick-start-configurable-agent">Quick Start: Configurable Agent</h2>
<p>Create agents from YAML configuration files for production deployments:</p>
<h3 id="agent-configuration-file">Agent Configuration File</h3>
<p>Create <code>config/agent_config.yaml</code>:</p>
<pre><code class="language-yaml">version: &quot;1.0&quot;

# Define reusable prompts
prompts:
  customer_support:
    name: &quot;customer_support&quot;
    description: &quot;Friendly customer support specialist&quot;
    system_prompt: |
      You are a friendly and empathetic customer support specialist.
      Your goal is to help customers solve their problems efficiently.

      Guidelines:
      - Listen actively to customer concerns
      - Provide clear, step-by-step solutions
      - Always maintain a professional yet warm tone
    skills:
      - &quot;Problem solving&quot;
      - &quot;Customer communication&quot;
    tags:
      - &quot;support&quot;

  code_assistant:
    name: &quot;code_assistant&quot;
    description: &quot;Expert programming assistant&quot;
    system_prompt: |
      You are an expert programming assistant with deep knowledge of:
      - Multiple programming languages (Python, JavaScript, TypeScript, etc.)
      - Software design patterns and best practices
      - Code optimization and debugging

      When generating code:
      1. Follow best practices and conventions
      2. Include comments and documentation
      3. Consider edge cases and error handling

# Define reusable tools
tools:
  get_weather:
    name: &quot;get_weather&quot;
    type: &quot;direct&quot;
    module: &quot;rakam_systems.ai_agents.components.tools.example_tools&quot;
    function: &quot;get_current_weather&quot;
    description: &quot;Get the current weather for a location&quot;
    category: &quot;utility&quot;
    tags: [&quot;weather&quot;, &quot;external&quot;]
    json_schema:
      type: &quot;object&quot;
      properties:
        location:
          type: &quot;string&quot;
          description: &quot;City name or location&quot;
        units:
          type: &quot;string&quot;
          enum: [&quot;celsius&quot;, &quot;fahrenheit&quot;]
          default: &quot;celsius&quot;
      required: [&quot;location&quot;]
      additionalProperties: false

  analyze_sentiment:
    name: &quot;analyze_sentiment&quot;
    type: &quot;direct&quot;
    module: &quot;rakam_systems.ai_agents.components.tools.example_tools&quot;
    function: &quot;analyze_sentiment&quot;
    description: &quot;Analyze the sentiment of text&quot;
    category: &quot;nlp&quot;
    json_schema:
      type: &quot;object&quot;
      properties:
        text:
          type: &quot;string&quot;
          description: &quot;Text to analyze&quot;
      required: [&quot;text&quot;]
      additionalProperties: false

# Define agents
agents:
  support_agent:
    name: &quot;support_agent&quot;
    description: &quot;Customer support specialist&quot;

    # Model configuration
    llm_config:
      model: &quot;openai:gpt-4o&quot;
      temperature: 0.7
      max_tokens: 2000
      parallel_tool_calls: true

    # Reference to prompt library
    prompt_config: &quot;customer_support&quot;

    # Reference to tools library
    tools:
      - &quot;get_weather&quot;
      - &quot;analyze_sentiment&quot;

    # Enable input/output tracking
    enable_tracking: true
    tracking_output_dir: &quot;./agent_tracking/support&quot;

    # Additional metadata
    metadata:
      version: &quot;1.0&quot;
      department: &quot;customer_support&quot;

  # Agent with structured output defined inline
  sql_agent:
    name: &quot;sql_agent&quot;
    description: &quot;SQL query assistant with structured output&quot;

    llm_config:
      model: &quot;openai:gpt-4o&quot;
      temperature: 0.2  # Low temperature for consistent output
      max_tokens: 3000

    prompt_config: &quot;code_assistant&quot;
    tools: []

    # Define structured output directly in YAML (no Python class needed!)
    output_type:
      name: &quot;SQLAgentOutput&quot;
      description: &quot;Structured output for SQL queries&quot;
      fields:
        answer:
          type: str
          description: &quot;The answer to the user's question&quot;
        sql_query:
          type: str
          description: &quot;The generated SQL query&quot;
          default: &quot;&quot;
        explanation:
          type: str
          description: &quot;Explanation of the query&quot;
        tables_used:
          type: list
          description: &quot;List of tables referenced&quot;
          default_factory: list

    enable_tracking: true
</code></pre>
<h3 id="loading-agents-from-configuration">Loading Agents from Configuration</h3>
<pre><code class="language-python">import asyncio
from rakam_systems.ai_core.config_loader import ConfigurationLoader

async def main():
    # Initialize configuration loader
    loader = ConfigurationLoader()

    # Load configuration from YAML
    config = loader.load_from_yaml(&quot;config/agent_config.yaml&quot;)

    # Validate configuration (optional but recommended)
    is_valid, errors = loader.validate_config()
    if not is_valid:
        for error in errors:
            print(f&quot;Config error: {error}&quot;)
        return

    # Create a single agent
    support_agent = loader.create_agent(&quot;support_agent&quot;, config)

    # Use the agent
    result = await support_agent.arun(&quot;What's the weather in New York?&quot;)
    print(f&quot;Support Agent: {result.output_text}&quot;)

    # Create agent with structured output
    sql_agent = loader.create_agent(&quot;sql_agent&quot;, config)
    result = await sql_agent.arun(&quot;Write a query to find all users who signed up last month&quot;)

    # Access structured output
    print(f&quot;Answer: {result.output.answer}&quot;)
    print(f&quot;SQL: {result.output.sql_query}&quot;)
    print(f&quot;Tables: {result.output.tables_used}&quot;)

    # Create all agents at once
    all_agents = loader.create_all_agents(config)
    print(f&quot;Created {len(all_agents)} agents: {list(all_agents.keys())}&quot;)

asyncio.run(main())
</code></pre>
<h3 id="using-tool-registry">Using Tool Registry</h3>
<pre><code class="language-python">from rakam_systems.ai_core.config_loader import ConfigurationLoader
from rakam_systems.ai_core.interfaces.tool_registry import ToolRegistry

loader = ConfigurationLoader()
config = loader.load_from_yaml(&quot;config/agent_config.yaml&quot;)

# Get tool registry with all configured tools
registry = loader.get_tool_registry(config)

# Query tools by category
utility_tools = registry.get_tools_by_category(&quot;utility&quot;)
print(f&quot;Utility tools: {[t.name for t in utility_tools]}&quot;)

# Query tools by tag
external_tools = registry.get_tools_by_tag(&quot;external&quot;)
print(f&quot;External tools: {[t.name for t in external_tools]}&quot;)

# Get specific tool
weather_tool = registry.get_tool(&quot;get_weather&quot;)
print(f&quot;Weather tool: {weather_tool.description}&quot;)
</code></pre>
<h3 id="configuration-options-reference">Configuration Options Reference</h3>
<h4 id="model-configuration-llm_config">Model Configuration (<code>llm_config</code>)</h4>
<table>
<thead>
<tr>
<th>Option</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>model</code></td>
<td>string</td>
<td>required</td>
<td>Model identifier (e.g., <code>openai:gpt-4o</code>)</td>
</tr>
<tr>
<td><code>temperature</code></td>
<td>float</td>
<td>0.7</td>
<td>Creativity (0.0-1.0)</td>
</tr>
<tr>
<td><code>max_tokens</code></td>
<td>int</td>
<td>2000</td>
<td>Maximum response tokens</td>
</tr>
<tr>
<td><code>parallel_tool_calls</code></td>
<td>bool</td>
<td>true</td>
<td>Execute tools in parallel</td>
</tr>
<tr>
<td><code>extra_settings</code></td>
<td>dict</td>
<td>{}</td>
<td>Additional provider settings</td>
</tr>
</tbody>
</table>
<h4 id="tool-configuration">Tool Configuration</h4>
<table>
<thead>
<tr>
<th>Option</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>name</code></td>
<td>string</td>
<td>âœ“</td>
<td>Unique tool identifier</td>
</tr>
<tr>
<td><code>type</code></td>
<td>string</td>
<td>âœ“</td>
<td><code>direct</code> or <code>mcp</code></td>
</tr>
<tr>
<td><code>module</code></td>
<td>string</td>
<td>âœ“*</td>
<td>Python module path (for direct)</td>
</tr>
<tr>
<td><code>function</code></td>
<td>string</td>
<td>âœ“*</td>
<td>Function name (for direct)</td>
</tr>
<tr>
<td><code>description</code></td>
<td>string</td>
<td>âœ“</td>
<td>Tool description for LLM</td>
</tr>
<tr>
<td><code>json_schema</code></td>
<td>object</td>
<td>âœ“</td>
<td>Parameter schema</td>
</tr>
<tr>
<td><code>category</code></td>
<td>string</td>
<td>-</td>
<td>Tool category for filtering</td>
</tr>
<tr>
<td><code>tags</code></td>
<td>list</td>
<td>-</td>
<td>Tags for filtering</td>
</tr>
</tbody>
</table>
<h4 id="inline-output-type">Inline Output Type</h4>
<table>
<thead>
<tr>
<th>Option</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>name</code></td>
<td>string</td>
<td>âœ“</td>
<td>Model class name</td>
</tr>
<tr>
<td><code>description</code></td>
<td>string</td>
<td>-</td>
<td>Model description</td>
</tr>
<tr>
<td><code>fields</code></td>
<td>dict</td>
<td>âœ“</td>
<td>Field definitions</td>
</tr>
</tbody>
</table>
<p>Field options: <code>type</code>, <code>description</code>, <code>default</code>, <code>default_factory</code>, <code>required</code></p>
<hr />
<h2 id="quick-start-vector-store">Quick Start: Vector Store</h2>
<p>Create a vector store for semantic search:</p>
<h3 id="using-faiss-in-memory">Using FAISS (In-Memory)</h3>
<pre><code class="language-python">from rakam_systems.ai_vectorstore.components.vectorstore.faiss_vector_store import FaissStore
from rakam_systems.ai_vectorstore.core import Node, NodeMetadata

# Create sample documents
documents = [
    &quot;Python is a high-level programming language.&quot;,
    &quot;Machine learning is a subset of artificial intelligence.&quot;,
    &quot;Deep learning uses neural networks with multiple layers.&quot;,
    &quot;Natural language processing helps computers understand text.&quot;,
    &quot;Vector databases store data based on semantic similarity.&quot;,
]

# Create nodes
nodes = []
for i, doc in enumerate(documents):
    metadata = NodeMetadata(
        source_file_uuid=&quot;docs_001&quot;,
        position=i,
        custom={&quot;category&quot;: &quot;tech&quot;}
    )
    nodes.append(Node(content=doc, metadata=metadata))

# Initialize FAISS store
store = FaissStore(
    name=&quot;my_store&quot;,
    base_index_path=&quot;./my_indexes&quot;,
    embedding_model=&quot;Snowflake/snowflake-arctic-embed-m&quot;,
    initialising=True
)

# Create collection and add nodes
store.create_collection_from_nodes(&quot;tech_docs&quot;, nodes)

# Search
query = &quot;What is machine learning?&quot;
results, result_nodes = store.search(
    collection_name=&quot;tech_docs&quot;,
    query=query,
    distance_type=&quot;cosine&quot;,
    number=3
)

print(f&quot;Query: {query}\n&quot;)
for node_id, (metadata, content, distance) in results.items():
    print(f&quot;  [{distance:.4f}] {content}&quot;)
</code></pre>
<h3 id="using-postgresql-with-pgvector">Using PostgreSQL with pgvector</h3>
<pre><code class="language-python">import os
import django
from django.conf import settings

# Configure Django (required for PostgreSQL backend)
if not settings.configured:
    settings.configure(
        INSTALLED_APPS=[
            'django.contrib.contenttypes',
            'rakam_systems.ai_vectorstore.components.vectorstore',
        ],
        DATABASES={
            'default': {
                'ENGINE': 'django.db.backends.postgresql',
                'NAME': os.getenv('POSTGRES_DB', 'vectorstore_db'),
                'USER': os.getenv('POSTGRES_USER', 'postgres'),
                'PASSWORD': os.getenv('POSTGRES_PASSWORD', 'postgres'),
                'HOST': os.getenv('POSTGRES_HOST', 'localhost'),
                'PORT': os.getenv('POSTGRES_PORT', '5432'),
            }
        },
        DEFAULT_AUTO_FIELD='django.db.models.BigAutoField',
    )
    django.setup()

from rakam_systems.ai_vectorstore import (
    ConfigurablePgVectorStore,
    VectorStoreConfig,
    Node,
    NodeMetadata
)

# Create configuration
config = VectorStoreConfig(
    name=&quot;my_pg_store&quot;,
    embedding={
        &quot;model_type&quot;: &quot;sentence_transformer&quot;,
        &quot;model_name&quot;: &quot;Snowflake/snowflake-arctic-embed-m&quot;
    },
    search={
        &quot;similarity_metric&quot;: &quot;cosine&quot;,
        &quot;default_top_k&quot;: 5
    }
)

# Initialize store
store = ConfigurablePgVectorStore(config=config)
store.setup()

# Add documents
nodes = [
    Node(
        content=&quot;Python is great for data science.&quot;,
        metadata=NodeMetadata(source_file_uuid=&quot;doc1&quot;, position=0)
    ),
    Node(
        content=&quot;JavaScript runs in web browsers.&quot;,
        metadata=NodeMetadata(source_file_uuid=&quot;doc1&quot;, position=1)
    ),
]
store.add_nodes(nodes)

# Search
results = store.search(&quot;What language is good for data?&quot;, top_k=3)
for r in results:
    print(f&quot;[{r['score']:.4f}] {r['content']}&quot;)

# Cleanup
store.shutdown()
</code></pre>
<hr />
<h2 id="quick-start-configurable-vector-store">Quick Start: Configurable Vector Store</h2>
<p>The <code>ConfigurablePgVectorStore</code> provides a production-ready vector store with comprehensive configuration options.</p>
<h3 id="vectorstoreconfig-overview">VectorStoreConfig Overview</h3>
<pre><code class="language-python">from rakam_systems.ai_vectorstore.config import (
    VectorStoreConfig,
    EmbeddingConfig,
    DatabaseConfig,
    SearchConfig,
    IndexConfig,
    load_config
)

# Create configuration programmatically
config = VectorStoreConfig(
    name=&quot;production_store&quot;,

    # Embedding configuration
    embedding=EmbeddingConfig(
        model_type=&quot;sentence_transformer&quot;,  # or &quot;openai&quot;, &quot;cohere&quot;
        model_name=&quot;Snowflake/snowflake-arctic-embed-m&quot;,
        batch_size=128,
        normalize=True,
        # api_key loaded from env: OPENAI_API_KEY or COHERE_API_KEY
    ),

    # Database configuration (uses env vars by default)
    database=DatabaseConfig(
        host=&quot;localhost&quot;,  # or POSTGRES_HOST env var
        port=5432,         # or POSTGRES_PORT env var
        database=&quot;vectorstore_db&quot;,
        user=&quot;postgres&quot;,
        password=&quot;postgres&quot;,
    ),

    # Search configuration
    search=SearchConfig(
        similarity_metric=&quot;cosine&quot;,  # &quot;cosine&quot;, &quot;l2&quot;, &quot;dot_product&quot;
        default_top_k=5,
        enable_hybrid_search=True,   # Combine vector + keyword search
        hybrid_alpha=0.7,            # Vector weight (0.3 for keyword)
        rerank=True,
    ),

    # Indexing configuration
    index=IndexConfig(
        chunk_size=512,
        chunk_overlap=50,
        batch_insert_size=10000,
    ),

    # Additional options
    enable_caching=True,
    cache_size=1000,
    enable_logging=True,
    log_level=&quot;INFO&quot;,
)

# Validate configuration
config.validate()
</code></pre>
<h3 id="configuration-from-yaml">Configuration from YAML</h3>
<p>Create <code>config/vectorstore.yaml</code>:</p>
<pre><code class="language-yaml">name: production_vectorstore

embedding:
  model_type: sentence_transformer
  model_name: Snowflake/snowflake-arctic-embed-m
  batch_size: 128
  normalize: true

database:
  host: localhost
  port: 5432
  database: vectorstore_db
  user: postgres
  password: postgres

search:
  similarity_metric: cosine
  default_top_k: 5
  enable_hybrid_search: true
  hybrid_alpha: 0.7
  rerank: true

index:
  chunk_size: 512
  chunk_overlap: 50
  batch_insert_size: 10000

enable_caching: true
cache_size: 1000
enable_logging: true
log_level: INFO
</code></pre>
<p>Load and use:</p>
<pre><code class="language-python">from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig

# Load from YAML
config = VectorStoreConfig.from_yaml(&quot;config/vectorstore.yaml&quot;)

# Or load from JSON
config = VectorStoreConfig.from_json(&quot;config/vectorstore.json&quot;)

# Or use the helper function (auto-detects format)
from rakam_systems.ai_vectorstore.config import load_config
config = load_config(&quot;config/vectorstore.yaml&quot;)

# Create store
store = ConfigurablePgVectorStore(config=config)
store.setup()
</code></pre>
<h3 id="using-different-embedding-models">Using Different Embedding Models</h3>
<pre><code class="language-python">from rakam_systems.ai_vectorstore.config import VectorStoreConfig, EmbeddingConfig

# Local embeddings with Sentence Transformers
config_local = VectorStoreConfig(
    embedding=EmbeddingConfig(
        model_type=&quot;sentence_transformer&quot;,
        model_name=&quot;Snowflake/snowflake-arctic-embed-m&quot;,  # 768 dimensions
        # model_name=&quot;all-MiniLM-L6-v2&quot;,                  # 384 dimensions
        batch_size=128,
    )
)

# OpenAI embeddings
config_openai = VectorStoreConfig(
    embedding=EmbeddingConfig(
        model_type=&quot;openai&quot;,
        model_name=&quot;text-embedding-3-small&quot;,
        # api_key loaded from OPENAI_API_KEY env var
    )
)

# Cohere embeddings
config_cohere = VectorStoreConfig(
    embedding=EmbeddingConfig(
        model_type=&quot;cohere&quot;,
        model_name=&quot;embed-english-v3.0&quot;,
        # api_key loaded from COHERE_API_KEY env var
    )
)
</code></pre>
<h3 id="multi-model-support">Multi-Model Support</h3>
<p>Each embedding model automatically gets dedicated tables, preventing mixing of incompatible vector spaces:</p>
<pre><code class="language-python">from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig, EmbeddingConfig

# Store using MiniLM model
config_minilm = VectorStoreConfig(
    embedding=EmbeddingConfig(
        model_type=&quot;sentence_transformer&quot;,
        model_name=&quot;all-MiniLM-L6-v2&quot;  # 384D
    )
)
store_minilm = ConfigurablePgVectorStore(config=config_minilm)
# Tables: application_nodeentry_all_minilm_l6_v2

# Store using Arctic model
config_arctic = VectorStoreConfig(
    embedding=EmbeddingConfig(
        model_type=&quot;sentence_transformer&quot;,
        model_name=&quot;Snowflake/snowflake-arctic-embed-m&quot;  # 768D
    )
)
store_arctic = ConfigurablePgVectorStore(config=config_arctic)
# Tables: application_nodeentry_snowflake_arctic_embed_m

# Both can coexist without conflicts!
</code></pre>
<blockquote>
<p><strong>Important</strong>: Even if two models have the same dimensions, their vector spaces are different! Model-specific tables prevent meaningless search results from mixed embeddings.</p>
</blockquote>
<h3 id="hybrid-search">Hybrid Search</h3>
<p>Combine vector similarity with keyword search:</p>
<pre><code class="language-python">from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig

config = VectorStoreConfig()
config.search.enable_hybrid_search = True
config.search.hybrid_alpha = 0.7  # 70% vector, 30% keyword

store = ConfigurablePgVectorStore(config=config)
store.setup()

# Regular search (uses config defaults)
results = store.search(&quot;machine learning algorithms&quot;, top_k=10)

# Hybrid search with custom alpha
results = store.hybrid_search(
    query=&quot;machine learning algorithms&quot;,
    top_k=10,
    alpha=0.5  # 50/50 split
)

for r in results:
    print(f&quot;[{r['score']:.4f}] {r['content'][:100]}...&quot;)
</code></pre>
<h3 id="full-example-with-all-features">Full Example with All Features</h3>
<pre><code class="language-python">import os
import django
from django.conf import settings

# Configure Django
if not settings.configured:
    settings.configure(
        INSTALLED_APPS=[
            'django.contrib.contenttypes',
            'rakam_systems.ai_vectorstore.components.vectorstore',
        ],
        DATABASES={
            'default': {
                'ENGINE': 'django.db.backends.postgresql',
                'NAME': os.getenv('POSTGRES_DB', 'vectorstore_db'),
                'USER': os.getenv('POSTGRES_USER', 'postgres'),
                'PASSWORD': os.getenv('POSTGRES_PASSWORD', 'postgres'),
                'HOST': os.getenv('POSTGRES_HOST', 'localhost'),
                'PORT': os.getenv('POSTGRES_PORT', '5432'),
            }
        },
        DEFAULT_AUTO_FIELD='django.db.models.BigAutoField',
    )
    django.setup()

from rakam_systems.ai_vectorstore import (
    ConfigurablePgVectorStore,
    VectorStoreConfig,
    Node,
    NodeMetadata,
    VSFile
)
from rakam_systems.ai_vectorstore.components.loader import AdaptiveLoader

# Create configuration
config = VectorStoreConfig.from_yaml(&quot;config/vectorstore.yaml&quot;)

# Initialize store with context manager
with ConfigurablePgVectorStore(config=config) as store:

    # Load documents
    loader = AdaptiveLoader(config={
        &quot;chunk_size&quot;: config.index.chunk_size,
        &quot;chunk_overlap&quot;: config.index.chunk_overlap
    })

    # Load from file
    vsfile = loader.load_as_vsfile(&quot;documents/manual.pdf&quot;)
    store.add_vsfile(vsfile)

    # Or add nodes directly
    nodes = [
        Node(
            content=&quot;Python is excellent for data science and machine learning.&quot;,
            metadata=NodeMetadata(
                source_file_uuid=&quot;doc_001&quot;,
                position=0,
                custom={&quot;category&quot;: &quot;programming&quot;, &quot;topic&quot;: &quot;python&quot;}
            )
        ),
        Node(
            content=&quot;PostgreSQL with pgvector enables efficient vector similarity search.&quot;,
            metadata=NodeMetadata(
                source_file_uuid=&quot;doc_001&quot;,
                position=1,
                custom={&quot;category&quot;: &quot;database&quot;, &quot;topic&quot;: &quot;vectors&quot;}
            )
        ),
    ]
    store.add_nodes(nodes)

    # Search with filters
    results = store.search(
        query=&quot;vector database search&quot;,
        top_k=5,
        # Metadata filters can be applied here
    )

    print(&quot;Search Results:&quot;)
    for r in results:
        print(f&quot;  [{r['score']:.4f}] {r['content'][:80]}...&quot;)
        print(f&quot;           Source: {r.get('source_file_uuid', 'N/A')}&quot;)

    # Hybrid search
    print(&quot;\nHybrid Search Results:&quot;)
    hybrid_results = store.hybrid_search(
        query=&quot;PostgreSQL vector&quot;,
        top_k=5,
        alpha=0.6
    )
    for r in hybrid_results:
        print(f&quot;  [{r['score']:.4f}] {r['content'][:80]}...&quot;)

    # Get collection stats
    count = store.count()
    print(f&quot;\nTotal vectors in store: {count}&quot;)

# Store automatically cleaned up
</code></pre>
<h3 id="configuration-reference">Configuration Reference</h3>
<h4 id="embeddingconfig">EmbeddingConfig</h4>
<table>
<thead>
<tr>
<th>Option</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>model_type</code></td>
<td>str</td>
<td><code>sentence_transformer</code></td>
<td>Backend: <code>sentence_transformer</code>, <code>openai</code>, <code>cohere</code></td>
</tr>
<tr>
<td><code>model_name</code></td>
<td>str</td>
<td><code>Snowflake/snowflake-arctic-embed-m</code></td>
<td>Model identifier</td>
</tr>
<tr>
<td><code>api_key</code></td>
<td>str</td>
<td>None</td>
<td>API key (auto-loaded from env)</td>
</tr>
<tr>
<td><code>batch_size</code></td>
<td>int</td>
<td>128</td>
<td>Batch size for encoding</td>
</tr>
<tr>
<td><code>normalize</code></td>
<td>bool</td>
<td>True</td>
<td>Normalize embeddings</td>
</tr>
<tr>
<td><code>dimensions</code></td>
<td>int</td>
<td>None</td>
<td>Vector dimensions (auto-detected)</td>
</tr>
</tbody>
</table>
<h4 id="databaseconfig">DatabaseConfig</h4>
<table>
<thead>
<tr>
<th>Option</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>host</code></td>
<td>str</td>
<td><code>localhost</code></td>
<td>PostgreSQL host (or <code>POSTGRES_HOST</code> env)</td>
</tr>
<tr>
<td><code>port</code></td>
<td>int</td>
<td>5432</td>
<td>PostgreSQL port (or <code>POSTGRES_PORT</code> env)</td>
</tr>
<tr>
<td><code>database</code></td>
<td>str</td>
<td><code>vectorstore_db</code></td>
<td>Database name (or <code>POSTGRES_DB</code> env)</td>
</tr>
<tr>
<td><code>user</code></td>
<td>str</td>
<td><code>postgres</code></td>
<td>Username (or <code>POSTGRES_USER</code> env)</td>
</tr>
<tr>
<td><code>password</code></td>
<td>str</td>
<td><code>postgres</code></td>
<td>Password (or <code>POSTGRES_PASSWORD</code> env)</td>
</tr>
<tr>
<td><code>pool_size</code></td>
<td>int</td>
<td>10</td>
<td>Connection pool size</td>
</tr>
</tbody>
</table>
<h4 id="searchconfig">SearchConfig</h4>
<table>
<thead>
<tr>
<th>Option</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>similarity_metric</code></td>
<td>str</td>
<td><code>cosine</code></td>
<td>Metric: <code>cosine</code>, <code>l2</code>, <code>dot_product</code></td>
</tr>
<tr>
<td><code>default_top_k</code></td>
<td>int</td>
<td>5</td>
<td>Default results count</td>
</tr>
<tr>
<td><code>enable_hybrid_search</code></td>
<td>bool</td>
<td>True</td>
<td>Enable keyword + vector search</td>
</tr>
<tr>
<td><code>hybrid_alpha</code></td>
<td>float</td>
<td>0.7</td>
<td>Vector weight (1-alpha for keyword)</td>
</tr>
<tr>
<td><code>rerank</code></td>
<td>bool</td>
<td>True</td>
<td>Rerank results</td>
</tr>
</tbody>
</table>
<h4 id="indexconfig">IndexConfig</h4>
<table>
<thead>
<tr>
<th>Option</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>chunk_size</code></td>
<td>int</td>
<td>512</td>
<td>Chunk size in tokens</td>
</tr>
<tr>
<td><code>chunk_overlap</code></td>
<td>int</td>
<td>50</td>
<td>Overlap between chunks</td>
</tr>
<tr>
<td><code>batch_insert_size</code></td>
<td>int</td>
<td>10000</td>
<td>Batch size for inserts</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="quick-start-rag-pipeline">Quick Start: RAG Pipeline</h2>
<p>Build a complete Retrieval-Augmented Generation pipeline:</p>
<pre><code class="language-python">import asyncio
from rakam_systems.ai_agents import BaseAgent
from rakam_systems.ai_vectorstore.components.vectorstore.faiss_vector_store import FaissStore
from rakam_systems.ai_vectorstore.components.loader import AdaptiveLoader
from rakam_systems.ai_core.interfaces.tool import ToolComponent

# Step 1: Load and index documents
loader = AdaptiveLoader(config={&quot;chunk_size&quot;: 512, &quot;chunk_overlap&quot;: 50})

# Load documents (supports PDF, DOCX, TXT, MD, HTML, etc.)
# nodes = loader.load_as_nodes(&quot;path/to/document.pdf&quot;)

# For demo, use sample text
from rakam_systems.ai_vectorstore.core import Node, NodeMetadata

sample_docs = [
    &quot;Our company was founded in 2020 and specializes in AI solutions.&quot;,
    &quot;We offer three main products: AI Assistant, Data Analytics, and Automation.&quot;,
    &quot;The AI Assistant can handle customer queries 24/7 with 95% accuracy.&quot;,
    &quot;Our pricing starts at $99/month for small businesses.&quot;,
    &quot;Enterprise customers get dedicated support and custom integrations.&quot;,
    &quot;We have offices in New York, London, and Tokyo.&quot;,
]

nodes = [
    Node(content=doc, metadata=NodeMetadata(source_file_uuid=&quot;company_info&quot;, position=i))
    for i, doc in enumerate(sample_docs)
]

# Step 2: Create vector store
store = FaissStore(
    name=&quot;rag_store&quot;,
    base_index_path=&quot;./rag_indexes&quot;,
    embedding_model=&quot;Snowflake/snowflake-arctic-embed-m&quot;,
    initialising=True
)
store.create_collection_from_nodes(&quot;knowledge_base&quot;, nodes)

# Step 3: Create search tool
def search_knowledge_base(query: str, top_k: int = 3) -&gt; str:
    &quot;&quot;&quot;Search the knowledge base for relevant information.&quot;&quot;&quot;
    results, _ = store.search(
        collection_name=&quot;knowledge_base&quot;,
        query=query,
        distance_type=&quot;cosine&quot;,
        number=top_k
    )

    context = &quot;\n&quot;.join([
        f&quot;- {content}&quot; for _, (_, content, _) in results.items()
    ])
    return f&quot;Relevant information:\n{context}&quot;

search_tool = ToolComponent.from_function(
    function=search_knowledge_base,
    name=&quot;search_knowledge_base&quot;,
    description=&quot;Search the company knowledge base for information&quot;,
    json_schema={
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;query&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Search query&quot;},
            &quot;top_k&quot;: {&quot;type&quot;: &quot;integer&quot;, &quot;default&quot;: 3, &quot;description&quot;: &quot;Number of results&quot;}
        },
        &quot;required&quot;: [&quot;query&quot;],
        &quot;additionalProperties&quot;: False
    }
)

# Step 4: Create RAG agent
async def main():
    agent = BaseAgent(
        name=&quot;rag_agent&quot;,
        model=&quot;openai:gpt-4o&quot;,
        system_prompt=&quot;&quot;&quot;You are a helpful customer service agent for our company.
Use the search_knowledge_base tool to find relevant information before answering.
Always base your answers on the retrieved information.&quot;&quot;&quot;,
        tools=[search_tool]
    )

    # Ask questions
    questions = [
        &quot;When was the company founded?&quot;,
        &quot;What products do you offer?&quot;,
        &quot;How much does the service cost?&quot;,
    ]

    for question in questions:
        print(f&quot;\nQ: {question}&quot;)
        result = await agent.arun(question)
        print(f&quot;A: {result.output_text}&quot;)

asyncio.run(main())
</code></pre>
<hr />
<h2 id="quick-start-llm-gateway">Quick Start: LLM Gateway</h2>
<p>Use the LLM Gateway for direct LLM interactions:</p>
<pre><code class="language-python">from rakam_systems.ai_agents.components.llm_gateway import (
    OpenAIGateway,
    MistralGateway,
    LLMGatewayFactory,
    LLMRequest
)

# Create gateway using factory
gateway = LLMGatewayFactory.create(
    provider=&quot;openai&quot;,
    model=&quot;gpt-4o&quot;
)

# Simple text generation
request = LLMRequest(
    system_prompt=&quot;You are a helpful assistant.&quot;,
    user_prompt=&quot;Explain quantum computing in simple terms.&quot;,
    temperature=0.7,
    max_tokens=200
)

response = gateway.generate(request)
print(f&quot;Response: {response.content}&quot;)
print(f&quot;Tokens used: {response.usage}&quot;)
</code></pre>
<h3 id="structured-output-with-gateway">Structured Output with Gateway</h3>
<pre><code class="language-python">from pydantic import BaseModel
from rakam_systems.ai_agents.components.llm_gateway import OpenAIGateway, LLMRequest

class Recipe(BaseModel):
    name: str
    ingredients: list[str]
    instructions: list[str]
    prep_time_minutes: int
    servings: int

gateway = OpenAIGateway(model=&quot;gpt-4o&quot;)

request = LLMRequest(
    system_prompt=&quot;You are a chef. Create recipes based on user requests.&quot;,
    user_prompt=&quot;Give me a simple pasta recipe.&quot;
)

recipe = gateway.generate_structured(request, Recipe)
print(f&quot;Recipe: {recipe.name}&quot;)
print(f&quot;Prep time: {recipe.prep_time_minutes} minutes&quot;)
print(f&quot;Ingredients: {', '.join(recipe.ingredients)}&quot;)
</code></pre>
<h3 id="streaming-with-gateway">Streaming with Gateway</h3>
<pre><code class="language-python">from rakam_systems.ai_agents.components.llm_gateway import OpenAIGateway, LLMRequest

gateway = OpenAIGateway(model=&quot;gpt-4o&quot;)

request = LLMRequest(
    user_prompt=&quot;Write a poem about coding.&quot;,
    temperature=0.8
)

print(&quot;Poem:\n&quot;)
for chunk in gateway.stream(request):
    print(chunk, end=&quot;&quot;, flush=True)
print()
</code></pre>
<hr />
<h2 id="quick-start-document-loading">Quick Start: Document Loading</h2>
<p>Load and process various document types:</p>
<pre><code class="language-python">from rakam_systems.ai_vectorstore.components.loader import AdaptiveLoader

# Create loader
loader = AdaptiveLoader(config={
    &quot;chunk_size&quot;: 512,
    &quot;chunk_overlap&quot;: 50,
    &quot;encoding&quot;: &quot;utf-8&quot;
})

# Load as text
text = loader.load_as_text(&quot;document.pdf&quot;)
print(f&quot;Loaded {len(text)} characters&quot;)

# Load as chunks
chunks = loader.load_as_chunks(&quot;document.pdf&quot;)
print(f&quot;Created {len(chunks)} chunks&quot;)

# Load as nodes (with metadata)
nodes = loader.load_as_nodes(
    &quot;document.pdf&quot;,
    custom_metadata={&quot;category&quot;: &quot;technical&quot;, &quot;author&quot;: &quot;John&quot;}
)
print(f&quot;Created {len(nodes)} nodes&quot;)

# Load as VSFile (complete document representation)
vsfile = loader.load_as_vsfile(&quot;document.pdf&quot;)
print(f&quot;File: {vsfile.file_name}, UUID: {vsfile.uuid}&quot;)
</code></pre>
<h3 id="supported-file-types">Supported File Types</h3>
<table>
<thead>
<tr>
<th>Type</th>
<th>Extensions</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Text</strong></td>
<td><code>.txt</code>, <code>.text</code></td>
</tr>
<tr>
<td><strong>Markdown</strong></td>
<td><code>.md</code>, <code>.markdown</code></td>
</tr>
<tr>
<td><strong>PDF</strong></td>
<td><code>.pdf</code></td>
</tr>
<tr>
<td><strong>Word</strong></td>
<td><code>.docx</code>, <code>.doc</code></td>
</tr>
<tr>
<td><strong>OpenDocument</strong></td>
<td><code>.odt</code></td>
</tr>
<tr>
<td><strong>HTML</strong></td>
<td><code>.html</code>, <code>.htm</code>, <code>.xhtml</code></td>
</tr>
<tr>
<td><strong>Email</strong></td>
<td><code>.eml</code>, <code>.msg</code></td>
</tr>
<tr>
<td><strong>Data</strong></td>
<td><code>.json</code>, <code>.csv</code>, <code>.tsv</code>, <code>.xlsx</code>, <code>.xls</code></td>
</tr>
<tr>
<td><strong>Code</strong></td>
<td><code>.py</code>, <code>.js</code>, <code>.ts</code>, <code>.java</code>, <code>.cpp</code>, <code>.go</code>, <code>.rs</code>, <code>.rb</code></td>
</tr>
</tbody>
</table>
<hr />
<h2 id="quick-start-configuration-from-yaml">Quick Start: Configuration from YAML</h2>
<p>Load agents and tools from configuration files:</p>
<pre><code class="language-yaml"># config/agents.yaml
version: &quot;1.0&quot;

prompts:
  assistant:
    system_prompt: |
      You are a helpful AI assistant.
      Always be accurate and concise.

tools:
  get_time:
    name: &quot;get_time&quot;
    type: &quot;direct&quot;
    module: &quot;datetime&quot;
    function: &quot;datetime.now&quot;
    description: &quot;Get current date and time&quot;
    json_schema:
      type: &quot;object&quot;
      properties: {}

agents:
  main_agent:
    name: &quot;main_agent&quot;
    model_config:
      model: &quot;openai:gpt-4o&quot;
      temperature: 0.7
    prompt_config: &quot;assistant&quot;
    tools:
      - &quot;get_time&quot;
    enable_tracking: true
</code></pre>
<pre><code class="language-python">from rakam_systems.ai_core.config_loader import ConfigurationLoader

loader = ConfigurationLoader()

# Load configuration
config = loader.load_from_yaml(&quot;config/agents.yaml&quot;)

# Create agent from config
agent = loader.create_agent(&quot;main_agent&quot;, config)

# Use the agent
result = await agent.arun(&quot;What time is it?&quot;)
</code></pre>
<hr />
<h2 id="next-steps">Next Steps</h2>
<p>Now that you've completed the quick start:</p>
<ol>
<li><strong>Read the Full Documentation</strong></li>
<li><a href="../components/">Components Guide</a> - Detailed component documentation</li>
<li><a href="../development_guide/">Development Guide</a> - How to extend the framework</li>
<li>
<p><a href="../installation/">Installation Guide</a> - Advanced installation options</p>
</li>
<li>
<p><strong>Explore More Examples</strong></p>
</li>
<li><code>rakam_systems/examples/ai_agents_examples/</code> - Agent examples</li>
<li><code>rakam_systems/examples/ai_vectorstore_examples/</code> - Vector store examples</li>
<li>
<p><code>rakam_systems/examples/configs/</code> - Configuration examples</p>
</li>
<li>
<p><strong>Build Your Application</strong></p>
</li>
<li>Start with a simple agent</li>
<li>Add tools for your specific use case</li>
<li>Integrate vector storage for RAG</li>
<li>
<p>Scale with PostgreSQL for production</p>
</li>
<li>
<p><strong>Join the Community</strong></p>
</li>
<li><a href="https://github.com/Rakam-AI/rakam_systems">GitHub Repository</a></li>
<li>Report issues and request features</li>
<li>Contribute to the project</li>
</ol>
<hr />
<h2 id="common-patterns">Common Patterns</h2>
<h3 id="error-handling">Error Handling</h3>
<pre><code class="language-python">import asyncio
from rakam_systems.ai_agents import BaseAgent

async def main():
    agent = BaseAgent(
        name=&quot;safe_agent&quot;,
        model=&quot;openai:gpt-4o&quot;,
        system_prompt=&quot;You are helpful.&quot;
    )

    try:
        result = await agent.arun(&quot;Hello!&quot;)
        print(result.output_text)
    except Exception as e:
        print(f&quot;Error: {e}&quot;)

asyncio.run(main())
</code></pre>
<h3 id="context-manager-pattern">Context Manager Pattern</h3>
<pre><code class="language-python">from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore

# Automatic setup and cleanup
with ConfigurablePgVectorStore(config=config) as store:
    store.add_nodes(nodes)
    results = store.search(&quot;query&quot;)
# shutdown() called automatically
</code></pre>
<h3 id="async-batch-processing">Async Batch Processing</h3>
<pre><code class="language-python">import asyncio
from rakam_systems.ai_agents import BaseAgent

async def process_queries(queries: list[str]):
    agent = BaseAgent(
        name=&quot;batch_agent&quot;,
        model=&quot;openai:gpt-4o&quot;,
        system_prompt=&quot;Answer concisely.&quot;
    )

    # Process queries concurrently
    tasks = [agent.arun(q) for q in queries]
    results = await asyncio.gather(*tasks)

    return [r.output_text for r in results]

queries = [&quot;What is Python?&quot;, &quot;What is JavaScript?&quot;, &quot;What is Rust?&quot;]
answers = asyncio.run(process_queries(queries))
for q, a in zip(queries, answers):
    print(f&quot;Q: {q}\nA: {a}\n&quot;)
</code></pre>
<hr />
<p><strong>Happy Building! ðŸš€</strong></p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../start/" class="btn btn-neutral float-left" title="Getting Started"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../installation/" class="btn btn-neutral float-right" title="Installation">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../start/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../installation/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
