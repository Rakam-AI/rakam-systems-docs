{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83d\ude80 Rakam Systems \u2014 AI Systems Lifecycle Framework From architecture to deployment \u2014 one unified toolkit to design, build, evaluate, deploy, and monitor AI systems. Rakam Systems is an end-to-end Python framework that provides production-ready templates, SDKs, CLIs, and automation for building full AI systems\u2014not just models or agents. Focus on what matters: your system logic and your business needs. Rakam Systems provides everything else: architecture scaffolds, evaluation pipelines, data collection templates, model development tooling, deployment-ready microservices, and integrated observability. \u26a1\ufe0f Build AI Systems End-to-End \ud83e\udde9 Design \u2192 \ud83d\udcca Evaluate \u2192 \ud83d\udce5 Collect \u2192 \ud83e\uddea Develop \u2192 \ud83d\ude80 Serve \u2192 \ud83d\udce6 Deploy \u2192 \ud83d\udd0e Manage All in a single library. \u2728 Key Pillars Stage What Rakam Systems Automates Design your Systems Architecture templates, diagram generation, SDK-first system definitions Evaluate your Systems Dataset builders, metric definitions, evaluation pipelines Collect your Data Auto-generated CSVs/templates based on evaluation requirements Develop your Systems Reusable components, model wrappers, orchestration patterns Serve your Systems FastAPI microservices, Docker templates, vector DB connectors Deploy your Systems IaC provisioning, CI/CD pipelines, environment automation Manage your Systems Monitoring, drift detection, observability dashboards \ud83e\udde9 System Lifecycle Overview Rakam Systems implements a holistic AI Systems Lifecycle , enabling a seamless path from design to production. 1. \ud83e\udde9 Design your Systems Create a clear, reproducible architecture: Define system components using the Architecture SDK Sync with Draw.io using exported files Generate architecture diagrams programmatically 2. \ud83d\udcca Evaluate your Systems Build measurable, reliable AI: Define metrics Build evaluation datasets Structure evaluation scenarios Run evaluation pipelines with a single command 3. \ud83d\udce5 Collect your Data Your evaluation items become data templates: Auto-generate CSVs and annotation formats Pre-fill metadata Validate incoming data 4. \ud83e\uddea Develop your Systems Use Rakam Systems' component library or create your own: Pre-built modules for ranking, routing, LLM orchestration Reusable patterns for ML pipelines Model development utilities 5. \ud83d\ude80 Serve your Systems Out-of-the-box microservices: FastAPI templates with auth, rate limiting, tracing Optional UI playground Dockerized services with env-based configuration 6. \ud83d\udce6 Deploy your Systems Provision infrastructure and CI/CD using the CLI: Deploy to cloud environments Terraform integration Automatic pipelines for testing, building, and releasing 7. \ud83d\udd0e Manage your Systems Keep your system reliable after launch: Monitoring dashboards Latency and performance metrics Data drift and quality checks Alerts & logs automatically integrated \ud83d\udd27 Requirements Python 3.10+ Docker (optional for serving) Terraform (optional for deployment) \ud83d\udcda Documentation Comprehensive documentation is available: Getting Started Guide \u2013 Build your first system Architecture Templates \u2013 README for each module/template/system Evaluation SDK Reference Component Development Guide Serving & API Templates Overview Deployment Guide Monitoring & Observability Guide \ud83c\udf1f Community & Extensions We're building an ecosystem of reusable system templates. Contribute your own architecture, evaluation suites, or components! Community Examples Add-on Components \ud83e\udd1d Contributing Contributions are welcome! Please see the Contributing Guide for instructions on improving the framework. \ud83d\udcac Feedback & Support If you have issues, questions, or feature requests: Open a GitHub issue Join the community discussions Share ideas and feedback Help us shape the future of AI system development. \ud83d\udcdc Disclaimer This library provides scaffolding and automation for AI system development. You are responsible for reviewing and managing any infrastructure resources deployed using the CLI.","title":"Home"},{"location":"#rakam-systems-ai-systems-lifecycle-framework","text":"From architecture to deployment \u2014 one unified toolkit to design, build, evaluate, deploy, and monitor AI systems. Rakam Systems is an end-to-end Python framework that provides production-ready templates, SDKs, CLIs, and automation for building full AI systems\u2014not just models or agents. Focus on what matters: your system logic and your business needs. Rakam Systems provides everything else: architecture scaffolds, evaluation pipelines, data collection templates, model development tooling, deployment-ready microservices, and integrated observability.","title":"\ud83d\ude80 Rakam Systems \u2014 AI Systems Lifecycle Framework"},{"location":"#build-ai-systems-end-to-end","text":"\ud83e\udde9 Design \u2192 \ud83d\udcca Evaluate \u2192 \ud83d\udce5 Collect \u2192 \ud83e\uddea Develop \u2192 \ud83d\ude80 Serve \u2192 \ud83d\udce6 Deploy \u2192 \ud83d\udd0e Manage All in a single library.","title":"\u26a1\ufe0f Build AI Systems End-to-End"},{"location":"#key-pillars","text":"Stage What Rakam Systems Automates Design your Systems Architecture templates, diagram generation, SDK-first system definitions Evaluate your Systems Dataset builders, metric definitions, evaluation pipelines Collect your Data Auto-generated CSVs/templates based on evaluation requirements Develop your Systems Reusable components, model wrappers, orchestration patterns Serve your Systems FastAPI microservices, Docker templates, vector DB connectors Deploy your Systems IaC provisioning, CI/CD pipelines, environment automation Manage your Systems Monitoring, drift detection, observability dashboards","title":"\u2728 Key Pillars"},{"location":"#system-lifecycle-overview","text":"Rakam Systems implements a holistic AI Systems Lifecycle , enabling a seamless path from design to production.","title":"\ud83e\udde9 System Lifecycle Overview"},{"location":"#1-design-your-systems","text":"Create a clear, reproducible architecture: Define system components using the Architecture SDK Sync with Draw.io using exported files Generate architecture diagrams programmatically","title":"1. \ud83e\udde9 Design your Systems"},{"location":"#2-evaluate-your-systems","text":"Build measurable, reliable AI: Define metrics Build evaluation datasets Structure evaluation scenarios Run evaluation pipelines with a single command","title":"2. \ud83d\udcca Evaluate your Systems"},{"location":"#3-collect-your-data","text":"Your evaluation items become data templates: Auto-generate CSVs and annotation formats Pre-fill metadata Validate incoming data","title":"3. \ud83d\udce5 Collect your Data"},{"location":"#4-develop-your-systems","text":"Use Rakam Systems' component library or create your own: Pre-built modules for ranking, routing, LLM orchestration Reusable patterns for ML pipelines Model development utilities","title":"4. \ud83e\uddea Develop your Systems"},{"location":"#5-serve-your-systems","text":"Out-of-the-box microservices: FastAPI templates with auth, rate limiting, tracing Optional UI playground Dockerized services with env-based configuration","title":"5. \ud83d\ude80 Serve your Systems"},{"location":"#6-deploy-your-systems","text":"Provision infrastructure and CI/CD using the CLI: Deploy to cloud environments Terraform integration Automatic pipelines for testing, building, and releasing","title":"6. \ud83d\udce6 Deploy your Systems"},{"location":"#7-manage-your-systems","text":"Keep your system reliable after launch: Monitoring dashboards Latency and performance metrics Data drift and quality checks Alerts & logs automatically integrated","title":"7. \ud83d\udd0e Manage your Systems"},{"location":"#requirements","text":"Python 3.10+ Docker (optional for serving) Terraform (optional for deployment)","title":"\ud83d\udd27 Requirements"},{"location":"#documentation","text":"Comprehensive documentation is available: Getting Started Guide \u2013 Build your first system Architecture Templates \u2013 README for each module/template/system Evaluation SDK Reference Component Development Guide Serving & API Templates Overview Deployment Guide Monitoring & Observability Guide","title":"\ud83d\udcda Documentation"},{"location":"#community-extensions","text":"We're building an ecosystem of reusable system templates. Contribute your own architecture, evaluation suites, or components! Community Examples Add-on Components","title":"\ud83c\udf1f Community &amp; Extensions"},{"location":"#contributing","text":"Contributions are welcome! Please see the Contributing Guide for instructions on improving the framework.","title":"\ud83e\udd1d Contributing"},{"location":"#feedback-support","text":"If you have issues, questions, or feature requests: Open a GitHub issue Join the community discussions Share ideas and feedback Help us shape the future of AI system development.","title":"\ud83d\udcac Feedback &amp; Support"},{"location":"#disclaimer","text":"This library provides scaffolding and automation for AI system development. You are responsible for reviewing and managing any infrastructure resources deployed using the CLI.","title":"\ud83d\udcdc Disclaimer"},{"location":"components/","text":"Rakam Systems Components Documentation Rakam Systems is a modular AI framework designed to build production-ready AI applications. It provides a comprehensive set of components for building AI agents, vector stores, and LLM-powered applications. \ud83d\udcd1 Table of Contents \ud83c\udfd7\ufe0f Architecture Overview \ud83e\uddf1 Core Module ( ai_core ) \ud83e\udd16 Agents Module ( ai_agents ) \ud83d\udd0d Vector Store Module ( ai_vectorstore ) \ud83d\udee0\ufe0f Utilities Module ( ai_utils ) \u2699\ufe0f Configuration System \ud83d\ude80 Quick Start Examples \ud83c\udf0d Environment Variables \u2705 Best Practices \ud83d\udcda Further Reading \ud83c\udfd7\ufe0f Architecture Overview Rakam Systems is organized into four main modules: rakam_systems/ \u251c\u2500\u2500 ai_core/ # Core abstractions, interfaces, and base classes \u251c\u2500\u2500 ai_agents/ # Agent implementations and LLM gateways \u251c\u2500\u2500 ai_vectorstore/ # Vector storage, embeddings, and document loading \u251c\u2500\u2500 ai_utils/ # Logging, metrics, and tracing utilities \u2514\u2500\u2500 examples/ # Usage examples and configuration templates Design Principles Component-Based Architecture : All components extend BaseComponent with lifecycle management ( setup() , shutdown() ) Interface-Driven : Abstract interfaces define contracts for extensibility Configuration-First : YAML/JSON configuration support for all components Provider-Agnostic : Support for multiple LLM providers, embedding models, and vector stores Core Module ( ai_core ) The core module provides foundational abstractions used throughout the system. BaseComponent The base class for all components, providing lifecycle management and evaluation capabilities. from rakam_systems.ai_core.base import BaseComponent class BaseComponent(ABC): \"\"\" Base class with: - name and config attributes - setup()/shutdown() lifecycle hooks - __call__ for auto-setup execution - Context manager support - Built-in evaluation harness \"\"\" def __init__(self, name: str, config: Optional[Dict] = None): self.name = name self.config = config or {} self.initialized = False def setup(self) -> None: \"\"\"Initialize heavy resources - override in subclasses.\"\"\" self.initialized = True def shutdown(self) -> None: \"\"\"Release resources - override in subclasses.\"\"\" self.initialized = False @abstractmethod def run(self, *args, **kwargs) -> Any: \"\"\"Execute the primary operation.\"\"\" raise NotImplementedError Interfaces Located in ai_core/interfaces/ , these define the contracts for various component types: AgentComponent from rakam_systems.ai_core.interfaces.agent import AgentComponent, AgentInput, AgentOutput class AgentInput: \"\"\"Input DTO for agents.\"\"\" input_text: str context: Dict[str, Any] class AgentOutput: \"\"\"Output DTO for agents.\"\"\" output_text: str metadata: Dict[str, Any] output: Optional[Any] # Structured output when output_type is used class AgentComponent(BaseComponent, ABC): \"\"\"Abstract agent interface with streaming and async support.\"\"\" def run(input_data, deps=None, model_settings=None) -> AgentOutput async def arun(input_data, deps=None, model_settings=None) -> AgentOutput def stream(input_data, deps=None) -> Iterator[str] async def astream(input_data, deps=None) -> AsyncIterator[str] ToolComponent from rakam_systems.ai_core.interfaces.tool import ToolComponent class ToolComponent(BaseComponent, ABC): \"\"\" Base class for callable tools, compatible with Pydantic AI. Attributes: name: Unique tool name description: Human-readable description function: The callable function json_schema: JSON schema for parameters takes_ctx: Whether tool takes context as first argument \"\"\" @classmethod def from_function(cls, function, name, description, json_schema, takes_ctx=False): \"\"\"Create a ToolComponent from a standalone function.\"\"\" ToolRegistry Central registry for managing tools across the system: from rakam_systems.ai_core.interfaces.tool_registry import ToolRegistry, ToolMode registry = ToolRegistry() # Register a direct tool registry.register_direct_tool( name=\"calculate\", function=lambda x, y: x + y, description=\"Add two numbers\", json_schema={...}, category=\"math\", tags=[\"arithmetic\"] ) # Register an MCP tool registry.register_mcp_tool( name=\"search\", mcp_server=\"search_server\", mcp_tool_name=\"web_search\", description=\"Search the web\" ) # Query tools tools = registry.get_tools_by_category(\"math\") tools = registry.get_tools_by_tag(\"arithmetic\") tools = registry.get_tools_by_mode(ToolMode.DIRECT) LLMGateway from rakam_systems.ai_core.interfaces.llm_gateway import LLMGateway, LLMRequest, LLMResponse class LLMRequest(BaseModel): system_prompt: Optional[str] user_prompt: str temperature: Optional[float] max_tokens: Optional[int] extra_params: Dict[str, Any] class LLMResponse(BaseModel): content: str parsed_content: Optional[Any] usage: Optional[Dict[str, Any]] model: Optional[str] finish_reason: Optional[str] class LLMGateway(BaseComponent, ABC): \"\"\"Abstract LLM gateway for provider-agnostic LLM interactions.\"\"\" def generate(request: LLMRequest) -> LLMResponse def generate_structured(request: LLMRequest, schema: Type[T]) -> T def stream(request: LLMRequest) -> Iterator[str] def count_tokens(text: str, model: str = None) -> int VectorStore from rakam_systems.ai_core.interfaces.vectorstore import VectorStore class VectorStore(BaseComponent, ABC): \"\"\"Abstract vector store interface.\"\"\" def add(vectors: List[List[float]], metadatas: List[Dict]) -> Any def query(vector: List[float], top_k: int = 5) -> List[Dict] def count() -> Optional[int] Loader from rakam_systems.ai_core.interfaces.loader import Loader class Loader(BaseComponent, ABC): \"\"\"Abstract document loader interface.\"\"\" def load_as_text(source: Union[str, Path]) -> str def load_as_chunks(source: Union[str, Path]) -> List[str] def load_as_nodes(source, source_id=None, custom_metadata=None) -> List[Node] def load_as_vsfile(file_path, custom_metadata=None) -> VSFile Tracking System Built-in input/output tracking for debugging and evaluation: from rakam_systems.ai_core.tracking import TrackingManager, track_method, TrackingMixin class MyAgent(TrackingMixin, BaseAgent): @track_method() async def arun(self, input_data, deps=None): return await super().arun(input_data, deps) # Enable tracking agent.enable_tracking(output_dir=\"./tracking\") # Export tracking data agent.export_tracking_data(format='csv') agent.export_tracking_data(format='json') # Get statistics stats = agent.get_tracking_statistics() Configuration Loader Load agent configurations from YAML files: from rakam_systems.ai_core.config_loader import ConfigurationLoader loader = ConfigurationLoader() config = loader.load_from_yaml(\"agent_config.yaml\") # Create agents from config agent = loader.create_agent(\"my_agent\", config) all_agents = loader.create_all_agents(config) # Get tool registry registry = loader.get_tool_registry(config) # Validate configuration is_valid, errors = loader.validate_config(\"config.yaml\") \ud83e\udd16 Agents Module ( ai_agents ) BaseAgent The main agent implementation using Pydantic AI: from rakam_systems.ai_agents import BaseAgent from rakam_systems.ai_core.interfaces.agent import AgentInput, AgentOutput, ModelSettings agent = BaseAgent( name=\"my_agent\", model=\"openai:gpt-4o\", system_prompt=\"You are a helpful assistant.\", tools=[my_tool], # Optional tools output_type=MyOutputModel, # Optional structured output enable_tracking=True # Optional tracking ) # Async inference (required for Pydantic AI) result = await agent.arun(\"What is AI?\") print(result.output_text) # With dependencies result = await agent.arun(\"Hello\", deps={\"user_id\": \"123\"}) # With model settings settings = ModelSettings(temperature=0.5, max_tokens=1000) result = await agent.arun(\"Explain quantum computing\", model_settings=settings) # Streaming async for chunk in agent.astream(\"Tell me a story\"): print(chunk, end=\"\") Dynamic System Prompts from datetime import date from pydantic_ai import RunContext @agent.dynamic_system_prompt def add_date() -> str: return f\"Today's date is {date.today()}.\" @agent.dynamic_system_prompt def add_user_context(ctx: RunContext[str]) -> str: return f\"The user's name is {ctx.deps}.\" LLM Gateways OpenAI Gateway from rakam_systems.ai_agents import OpenAIGateway, LLMRequest gateway = OpenAIGateway( model=\"gpt-4o\", api_key=\"...\", # Or use OPENAI_API_KEY env var default_temperature=0.7 ) # Text generation request = LLMRequest( system_prompt=\"You are a helpful assistant\", user_prompt=\"What is AI?\", temperature=0.7 ) response = gateway.generate(request) print(response.content) # Structured output from pydantic import BaseModel class Answer(BaseModel): answer: str confidence: float result = gateway.generate_structured(request, Answer) print(result.answer, result.confidence) # Streaming for chunk in gateway.stream(request): print(chunk, end=\"\") # Token counting token_count = gateway.count_tokens(\"Hello, world!\") Mistral Gateway from rakam_systems.ai_agents import MistralGateway gateway = MistralGateway( model=\"mistral-large-latest\", api_key=\"...\" # Or use MISTRAL_API_KEY env var ) Gateway Factory from rakam_systems.ai_agents import LLMGatewayFactory, get_llm_gateway # Using factory gateway = LLMGatewayFactory.create( provider=\"openai\", model=\"gpt-4o\", api_key=\"...\" ) # Using convenience function gateway = get_llm_gateway(provider=\"openai\", model=\"gpt-4o\") Chat History JSON Chat History from rakam_systems.ai_agents.components.chat_history import JSONChatHistory history = JSONChatHistory(config={\"storage_path\": \"./chat_history.json\"}) # Add messages history.add_message(\"chat123\", {\"role\": \"user\", \"content\": \"Hello!\"}) history.add_message(\"chat123\", {\"role\": \"assistant\", \"content\": \"Hi there!\"}) # Get history messages = history.get_chat_history(\"chat123\") readable = history.get_readable_chat_history(\"chat123\") # Pydantic AI integration message_history = history.get_message_history(\"chat123\") result = await agent.run(\"Hello\", message_history=message_history) history.save_messages(\"chat123\", result.all_messages()) # Manage chats all_chats = history.get_all_chat_ids() history.delete_chat_history(\"chat123\") history.clear_all() SQL Chat History For production deployments with database-backed storage. \ud83d\udd0d Vector Store Module ( ai_vectorstore ) Core Data Structures from rakam_systems.ai_vectorstore.core import Node, NodeMetadata, VSFile # VSFile - Represents a document source vsfile = VSFile(file_path=\"/path/to/document.pdf\") print(vsfile.uuid, vsfile.file_name, vsfile.mime_type) # NodeMetadata - Metadata for document chunks metadata = NodeMetadata( source_file_uuid=str(vsfile.uuid), position=0, # Page number or chunk position custom={\"author\": \"John\", \"date\": \"2024-01-01\"} ) # Node - A chunk with content and metadata node = Node(content=\"Document content here...\", metadata=metadata) node.embedding = [0.1, 0.2, 0.3, ...] # Set after embedding ConfigurablePgVectorStore Enhanced PostgreSQL vector store with full configuration support: from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig # From configuration object config = VectorStoreConfig() store = ConfigurablePgVectorStore(config=config) # From YAML file store = ConfigurablePgVectorStore(config=\"vectorstore_config.yaml\") # From dictionary store = ConfigurablePgVectorStore(config={ \"name\": \"my_store\", \"embedding\": { \"model_type\": \"sentence_transformer\", \"model_name\": \"Snowflake/snowflake-arctic-embed-m\" }, \"search\": { \"similarity_metric\": \"cosine\", \"enable_hybrid_search\": True, \"hybrid_alpha\": 0.7 } }) # Setup (initializes embedding model, database tables) store.setup() # Add documents store.add_nodes(nodes) store.add_vsfile(vsfile) # Search results = store.search(\"What is machine learning?\", top_k=5) results = store.hybrid_search(\"machine learning\", top_k=10, alpha=0.7) # Update vectors store.update_vector(node_id, new_embedding) # Cleanup store.shutdown() Multi-Model Support Each embedding model automatically gets dedicated tables: # Using different models - each gets its own tables store_minilm = ConfigurablePgVectorStore(config=config_minilm) store_mpnet = ConfigurablePgVectorStore(config=config_mpnet) # Table names are based on model names: # - application_nodeentry_all_minilm_l6_v2 # - application_nodeentry_snowflake_arctic_embed_m # Disable model-specific tables if needed (not recommended) store = ConfigurablePgVectorStore( config=config, use_dimension_specific_tables=False ) ConfigurableEmbeddings Multi-backend embedding model with unified interface: from rakam_systems.ai_vectorstore import ConfigurableEmbeddings, create_embedding_model # Using Sentence Transformers (local) embeddings = ConfigurableEmbeddings(config={ \"model_type\": \"sentence_transformer\", \"model_name\": \"Snowflake/snowflake-arctic-embed-m\", \"batch_size\": 128, \"normalize\": True }) # Using OpenAI embeddings = ConfigurableEmbeddings(config={ \"model_type\": \"openai\", \"model_name\": \"text-embedding-3-small\", \"api_key\": \"...\" # Or use OPENAI_API_KEY }) # Using Cohere embeddings = ConfigurableEmbeddings(config={ \"model_type\": \"cohere\", \"model_name\": \"embed-english-v3.0\", \"api_key\": \"...\" # Or use COHERE_API_KEY }) embeddings.setup() # Encode texts vectors = embeddings.run([\"Hello world\", \"How are you?\"]) query_vector = embeddings.encode_query(\"What is AI?\") doc_vectors = embeddings.encode_documents(documents) # Get dimension dim = embeddings.embedding_dimension Factory Function embeddings = create_embedding_model( model_type=\"sentence_transformer\", model_name=\"all-MiniLM-L6-v2\", batch_size=64 ) AdaptiveLoader Automatically detects and processes various file types: from rakam_systems.ai_vectorstore import AdaptiveLoader, create_adaptive_loader loader = AdaptiveLoader(config={ \"encoding\": \"utf-8\", \"chunk_size\": 512, \"chunk_overlap\": 50 }) # Supported file types: # - Text: .txt, .text # - Markdown: .md, .markdown # - Documents: .pdf, .docx, .doc, .odt # - Email: .eml, .msg # - Data: .json, .csv, .tsv, .xlsx, .xls # - HTML: .html, .htm, .xhtml # - Code: .py, .js, .ts, .java, .cpp, .go, .rs, .rb, etc. # Load as single text text = loader.load_as_text(\"document.pdf\") # Load as chunks chunks = loader.load_as_chunks(\"document.pdf\") # Load as nodes (with metadata) nodes = loader.load_as_nodes(\"document.pdf\", custom_metadata={\"category\": \"science\"}) # Load as VSFile vsfile = loader.load_as_vsfile(\"document.pdf\") # Also handles raw text chunks = loader.load_as_chunks(\"This is raw text content...\") Factory Function loader = create_adaptive_loader( chunk_size=1024, chunk_overlap=100, encoding='utf-8' ) Specialized Loaders Located in ai_vectorstore/components/loader/ : Loader File Types Features PdfLoader .pdf Text extraction, page-aware chunking DocLoader .docx , .doc Microsoft Word documents OdtLoader .odt OpenDocument Text MdLoader .md Markdown with structure preservation HtmlLoader .html , .htm HTML parsing and text extraction EmlLoader .eml , .msg Email files with attachments TabularLoader .csv , .tsv , .xlsx Tabular data processing CodeLoader .py , .js , etc. Code-aware chunking TextChunker Sentence-based text chunking using Chonkie: from rakam_systems.ai_vectorstore.components.chunker import TextChunker, create_text_chunker chunker = TextChunker( chunk_size=512, # Tokens per chunk chunk_overlap=50, # Overlap in tokens min_sentences_per_chunk=1, tokenizer=\"character\" # Or \"gpt2\", HuggingFace tokenizer ) chunks = chunker.chunk_text(\"Long document text...\") # Returns: [{\"text\": \"...\", \"token_count\": 100, \"start_index\": 0, \"end_index\": 500}, ...] # Process multiple documents all_chunks = chunker.run([\"doc1 text\", \"doc2 text\"]) \ud83d\udee0\ufe0f Utilities Module ( ai_utils ) Logging from rakam_systems.ai_utils import logging logger = logging.getLogger(__name__) logger.info(\"Processing document...\") logger.debug(\"Detailed debug info\") logger.error(\"An error occurred\") Metrics and Tracing For production monitoring and observability. \u2699\ufe0f Configuration System VectorStoreConfig from rakam_systems.ai_vectorstore.config import ( VectorStoreConfig, EmbeddingConfig, DatabaseConfig, SearchConfig, IndexConfig, load_config ) # Programmatic configuration config = VectorStoreConfig( name=\"my_vectorstore\", embedding=EmbeddingConfig( model_type=\"sentence_transformer\", model_name=\"Snowflake/snowflake-arctic-embed-m\", batch_size=128, normalize=True ), database=DatabaseConfig( host=\"localhost\", port=5432, database=\"vectorstore_db\", user=\"postgres\", password=\"postgres\" ), search=SearchConfig( similarity_metric=\"cosine\", default_top_k=5, enable_hybrid_search=True, hybrid_alpha=0.7 ), index=IndexConfig( chunk_size=512, chunk_overlap=50, batch_insert_size=10000 ) ) # From YAML file config = VectorStoreConfig.from_yaml(\"config.yaml\") # From JSON file config = VectorStoreConfig.from_json(\"config.json\") # From dictionary config = VectorStoreConfig.from_dict(config_dict) # Validation config.validate() # Save configuration config.save_yaml(\"output_config.yaml\") config.save_json(\"output_config.json\") YAML Configuration Example # vectorstore_config.yaml name: production_vectorstore embedding: model_type: sentence_transformer model_name: Snowflake/snowflake-arctic-embed-m batch_size: 128 normalize: true database: host: localhost port: 5432 database: vectorstore_db user: postgres password: postgres search: similarity_metric: cosine default_top_k: 5 enable_hybrid_search: true hybrid_alpha: 0.7 index: chunk_size: 512 chunk_overlap: 50 batch_insert_size: 10000 enable_caching: true cache_size: 1000 enable_logging: true log_level: INFO Agent Configuration Example # agent_config.yaml agents: my_agent: name: my_agent llm_config: model: openai:gpt-4o temperature: 0.7 max_tokens: 2000 parallel_tool_calls: true prompt_config: default_prompt tools: - search_tool - calculator deps_type: myapp.models.AgentDeps output_type: name: AgentOutput fields: answer: type: str description: The answer confidence: type: float description: Confidence score enable_tracking: true prompts: default_prompt: system_prompt: | You are a helpful AI assistant. Always provide accurate and helpful responses. tools: search_tool: name: search_tool type: direct module: myapp.tools function: search description: Search for information json_schema: type: object properties: query: type: string required: [query] calculator: name: calculator type: direct module: myapp.tools function: calculate description: Perform calculations \ud83d\ude80 Quick Start Examples Basic Agent import asyncio from rakam_systems.ai_agents import BaseAgent async def main(): agent = BaseAgent( name=\"assistant\", model=\"openai:gpt-4o\", system_prompt=\"You are a helpful assistant.\" ) result = await agent.arun(\"What is the capital of France?\") print(result.output_text) asyncio.run(main()) Agent with Tools import asyncio from rakam_systems.ai_agents import BaseAgent from rakam_systems.ai_core.interfaces.tool import ToolComponent def get_weather(city: str) -> str: return f\"The weather in {city} is sunny, 25\u00b0C\" weather_tool = ToolComponent.from_function( function=get_weather, name=\"get_weather\", description=\"Get the current weather for a city\", json_schema={ \"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\"}}, \"required\": [\"city\"] } ) async def main(): agent = BaseAgent( name=\"weather_assistant\", model=\"openai:gpt-4o\", system_prompt=\"You help users with weather information.\", tools=[weather_tool] ) result = await agent.arun(\"What's the weather in Paris?\") print(result.output_text) asyncio.run(main()) Document Search Pipeline from rakam_systems.ai_vectorstore import ( ConfigurablePgVectorStore, VectorStoreConfig, AdaptiveLoader ) # Configure vector store config = VectorStoreConfig() store = ConfigurablePgVectorStore(config=config) store.setup() # Load documents loader = AdaptiveLoader(config={\"chunk_size\": 512}) nodes = loader.load_as_nodes(\"documents/research_paper.pdf\") # Add to vector store store.add_nodes(nodes) # Search results = store.search(\"What are the main findings?\", top_k=5) for result in results: print(f\"Score: {result['score']:.4f}\") print(f\"Content: {result['content'][:200]}...\") print(\"---\") store.shutdown() Full RAG Pipeline import asyncio from rakam_systems.ai_agents import BaseAgent from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore, AdaptiveLoader, VectorStoreConfig from rakam_systems.ai_core.interfaces.tool import ToolComponent # Setup vector store config = VectorStoreConfig() store = ConfigurablePgVectorStore(config=config) store.setup() # Index documents loader = AdaptiveLoader() for doc_path in [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]: nodes = loader.load_as_nodes(doc_path) store.add_nodes(nodes) # Create search tool def search_documents(query: str, top_k: int = 5) -> str: results = store.search(query, top_k=top_k) return \"\\n\\n\".join([r['content'] for r in results]) search_tool = ToolComponent.from_function( function=search_documents, name=\"search_documents\", description=\"Search the document database\", json_schema={ \"type\": \"object\", \"properties\": { \"query\": {\"type\": \"string\", \"description\": \"Search query\"}, \"top_k\": {\"type\": \"integer\", \"description\": \"Number of results\"} }, \"required\": [\"query\"] } ) # Create RAG agent async def main(): agent = BaseAgent( name=\"rag_agent\", model=\"openai:gpt-4o\", system_prompt=\"\"\"You are a helpful assistant with access to a document database. Use the search_documents tool to find relevant information before answering questions.\"\"\", tools=[search_tool] ) result = await agent.arun(\"What are the key points from the documents?\") print(result.output_text) asyncio.run(main()) store.shutdown() Environment Variables The system supports the following environment variables: Variable Description Used By OPENAI_API_KEY OpenAI API key OpenAIGateway, ConfigurableEmbeddings MISTRAL_API_KEY Mistral API key MistralGateway COHERE_API_KEY Cohere API key ConfigurableEmbeddings POSTGRES_HOST PostgreSQL host DatabaseConfig POSTGRES_PORT PostgreSQL port DatabaseConfig POSTGRES_DB PostgreSQL database DatabaseConfig POSTGRES_USER PostgreSQL user DatabaseConfig POSTGRES_PASSWORD PostgreSQL password DatabaseConfig \u2705 Best Practices Always use context managers or explicit setup() / shutdown() for proper resource management Use configuration files for production deployments instead of hardcoded values Enable tracking during development for debugging and evaluation Use model-specific tables (default) to prevent mixing incompatible vector spaces Batch operations when processing large document collections Use async methods ( arun , astream ) for agents as they are powered by Pydantic AI Validate configurations before deployment using config.validate() or loader.validate_config() \ud83d\udcda Further Reading Example configurations: rakam_systems/examples/configs/ Agent examples: rakam_systems/examples/ai_agents_examples/ Vector store examples: rakam_systems/examples/ai_vectorstore_examples/ Loader documentation: rakam_systems/ai_vectorstore/components/loader/docs/ Architecture documentation: rakam_systems/ai_vectorstore/docs/ARCHITECTURE.md","title":"Components"},{"location":"components/#rakam-systems-components-documentation","text":"Rakam Systems is a modular AI framework designed to build production-ready AI applications. It provides a comprehensive set of components for building AI agents, vector stores, and LLM-powered applications.","title":"Rakam Systems Components Documentation"},{"location":"components/#table-of-contents","text":"\ud83c\udfd7\ufe0f Architecture Overview \ud83e\uddf1 Core Module ( ai_core ) \ud83e\udd16 Agents Module ( ai_agents ) \ud83d\udd0d Vector Store Module ( ai_vectorstore ) \ud83d\udee0\ufe0f Utilities Module ( ai_utils ) \u2699\ufe0f Configuration System \ud83d\ude80 Quick Start Examples \ud83c\udf0d Environment Variables \u2705 Best Practices \ud83d\udcda Further Reading","title":"\ud83d\udcd1 Table of Contents"},{"location":"components/#architecture-overview","text":"Rakam Systems is organized into four main modules: rakam_systems/ \u251c\u2500\u2500 ai_core/ # Core abstractions, interfaces, and base classes \u251c\u2500\u2500 ai_agents/ # Agent implementations and LLM gateways \u251c\u2500\u2500 ai_vectorstore/ # Vector storage, embeddings, and document loading \u251c\u2500\u2500 ai_utils/ # Logging, metrics, and tracing utilities \u2514\u2500\u2500 examples/ # Usage examples and configuration templates","title":"\ud83c\udfd7\ufe0f Architecture Overview"},{"location":"components/#design-principles","text":"Component-Based Architecture : All components extend BaseComponent with lifecycle management ( setup() , shutdown() ) Interface-Driven : Abstract interfaces define contracts for extensibility Configuration-First : YAML/JSON configuration support for all components Provider-Agnostic : Support for multiple LLM providers, embedding models, and vector stores","title":"Design Principles"},{"location":"components/#core-module-ai_core","text":"The core module provides foundational abstractions used throughout the system.","title":"Core Module (ai_core)"},{"location":"components/#basecomponent","text":"The base class for all components, providing lifecycle management and evaluation capabilities. from rakam_systems.ai_core.base import BaseComponent class BaseComponent(ABC): \"\"\" Base class with: - name and config attributes - setup()/shutdown() lifecycle hooks - __call__ for auto-setup execution - Context manager support - Built-in evaluation harness \"\"\" def __init__(self, name: str, config: Optional[Dict] = None): self.name = name self.config = config or {} self.initialized = False def setup(self) -> None: \"\"\"Initialize heavy resources - override in subclasses.\"\"\" self.initialized = True def shutdown(self) -> None: \"\"\"Release resources - override in subclasses.\"\"\" self.initialized = False @abstractmethod def run(self, *args, **kwargs) -> Any: \"\"\"Execute the primary operation.\"\"\" raise NotImplementedError","title":"BaseComponent"},{"location":"components/#interfaces","text":"Located in ai_core/interfaces/ , these define the contracts for various component types:","title":"Interfaces"},{"location":"components/#agentcomponent","text":"from rakam_systems.ai_core.interfaces.agent import AgentComponent, AgentInput, AgentOutput class AgentInput: \"\"\"Input DTO for agents.\"\"\" input_text: str context: Dict[str, Any] class AgentOutput: \"\"\"Output DTO for agents.\"\"\" output_text: str metadata: Dict[str, Any] output: Optional[Any] # Structured output when output_type is used class AgentComponent(BaseComponent, ABC): \"\"\"Abstract agent interface with streaming and async support.\"\"\" def run(input_data, deps=None, model_settings=None) -> AgentOutput async def arun(input_data, deps=None, model_settings=None) -> AgentOutput def stream(input_data, deps=None) -> Iterator[str] async def astream(input_data, deps=None) -> AsyncIterator[str]","title":"AgentComponent"},{"location":"components/#toolcomponent","text":"from rakam_systems.ai_core.interfaces.tool import ToolComponent class ToolComponent(BaseComponent, ABC): \"\"\" Base class for callable tools, compatible with Pydantic AI. Attributes: name: Unique tool name description: Human-readable description function: The callable function json_schema: JSON schema for parameters takes_ctx: Whether tool takes context as first argument \"\"\" @classmethod def from_function(cls, function, name, description, json_schema, takes_ctx=False): \"\"\"Create a ToolComponent from a standalone function.\"\"\"","title":"ToolComponent"},{"location":"components/#toolregistry","text":"Central registry for managing tools across the system: from rakam_systems.ai_core.interfaces.tool_registry import ToolRegistry, ToolMode registry = ToolRegistry() # Register a direct tool registry.register_direct_tool( name=\"calculate\", function=lambda x, y: x + y, description=\"Add two numbers\", json_schema={...}, category=\"math\", tags=[\"arithmetic\"] ) # Register an MCP tool registry.register_mcp_tool( name=\"search\", mcp_server=\"search_server\", mcp_tool_name=\"web_search\", description=\"Search the web\" ) # Query tools tools = registry.get_tools_by_category(\"math\") tools = registry.get_tools_by_tag(\"arithmetic\") tools = registry.get_tools_by_mode(ToolMode.DIRECT)","title":"ToolRegistry"},{"location":"components/#llmgateway","text":"from rakam_systems.ai_core.interfaces.llm_gateway import LLMGateway, LLMRequest, LLMResponse class LLMRequest(BaseModel): system_prompt: Optional[str] user_prompt: str temperature: Optional[float] max_tokens: Optional[int] extra_params: Dict[str, Any] class LLMResponse(BaseModel): content: str parsed_content: Optional[Any] usage: Optional[Dict[str, Any]] model: Optional[str] finish_reason: Optional[str] class LLMGateway(BaseComponent, ABC): \"\"\"Abstract LLM gateway for provider-agnostic LLM interactions.\"\"\" def generate(request: LLMRequest) -> LLMResponse def generate_structured(request: LLMRequest, schema: Type[T]) -> T def stream(request: LLMRequest) -> Iterator[str] def count_tokens(text: str, model: str = None) -> int","title":"LLMGateway"},{"location":"components/#vectorstore","text":"from rakam_systems.ai_core.interfaces.vectorstore import VectorStore class VectorStore(BaseComponent, ABC): \"\"\"Abstract vector store interface.\"\"\" def add(vectors: List[List[float]], metadatas: List[Dict]) -> Any def query(vector: List[float], top_k: int = 5) -> List[Dict] def count() -> Optional[int]","title":"VectorStore"},{"location":"components/#loader","text":"from rakam_systems.ai_core.interfaces.loader import Loader class Loader(BaseComponent, ABC): \"\"\"Abstract document loader interface.\"\"\" def load_as_text(source: Union[str, Path]) -> str def load_as_chunks(source: Union[str, Path]) -> List[str] def load_as_nodes(source, source_id=None, custom_metadata=None) -> List[Node] def load_as_vsfile(file_path, custom_metadata=None) -> VSFile","title":"Loader"},{"location":"components/#tracking-system","text":"Built-in input/output tracking for debugging and evaluation: from rakam_systems.ai_core.tracking import TrackingManager, track_method, TrackingMixin class MyAgent(TrackingMixin, BaseAgent): @track_method() async def arun(self, input_data, deps=None): return await super().arun(input_data, deps) # Enable tracking agent.enable_tracking(output_dir=\"./tracking\") # Export tracking data agent.export_tracking_data(format='csv') agent.export_tracking_data(format='json') # Get statistics stats = agent.get_tracking_statistics()","title":"Tracking System"},{"location":"components/#configuration-loader","text":"Load agent configurations from YAML files: from rakam_systems.ai_core.config_loader import ConfigurationLoader loader = ConfigurationLoader() config = loader.load_from_yaml(\"agent_config.yaml\") # Create agents from config agent = loader.create_agent(\"my_agent\", config) all_agents = loader.create_all_agents(config) # Get tool registry registry = loader.get_tool_registry(config) # Validate configuration is_valid, errors = loader.validate_config(\"config.yaml\")","title":"Configuration Loader"},{"location":"components/#agents-module-ai_agents","text":"","title":"\ud83e\udd16 Agents Module (ai_agents)"},{"location":"components/#baseagent","text":"The main agent implementation using Pydantic AI: from rakam_systems.ai_agents import BaseAgent from rakam_systems.ai_core.interfaces.agent import AgentInput, AgentOutput, ModelSettings agent = BaseAgent( name=\"my_agent\", model=\"openai:gpt-4o\", system_prompt=\"You are a helpful assistant.\", tools=[my_tool], # Optional tools output_type=MyOutputModel, # Optional structured output enable_tracking=True # Optional tracking ) # Async inference (required for Pydantic AI) result = await agent.arun(\"What is AI?\") print(result.output_text) # With dependencies result = await agent.arun(\"Hello\", deps={\"user_id\": \"123\"}) # With model settings settings = ModelSettings(temperature=0.5, max_tokens=1000) result = await agent.arun(\"Explain quantum computing\", model_settings=settings) # Streaming async for chunk in agent.astream(\"Tell me a story\"): print(chunk, end=\"\")","title":"BaseAgent"},{"location":"components/#dynamic-system-prompts","text":"from datetime import date from pydantic_ai import RunContext @agent.dynamic_system_prompt def add_date() -> str: return f\"Today's date is {date.today()}.\" @agent.dynamic_system_prompt def add_user_context(ctx: RunContext[str]) -> str: return f\"The user's name is {ctx.deps}.\"","title":"Dynamic System Prompts"},{"location":"components/#llm-gateways","text":"","title":"LLM Gateways"},{"location":"components/#openai-gateway","text":"from rakam_systems.ai_agents import OpenAIGateway, LLMRequest gateway = OpenAIGateway( model=\"gpt-4o\", api_key=\"...\", # Or use OPENAI_API_KEY env var default_temperature=0.7 ) # Text generation request = LLMRequest( system_prompt=\"You are a helpful assistant\", user_prompt=\"What is AI?\", temperature=0.7 ) response = gateway.generate(request) print(response.content) # Structured output from pydantic import BaseModel class Answer(BaseModel): answer: str confidence: float result = gateway.generate_structured(request, Answer) print(result.answer, result.confidence) # Streaming for chunk in gateway.stream(request): print(chunk, end=\"\") # Token counting token_count = gateway.count_tokens(\"Hello, world!\")","title":"OpenAI Gateway"},{"location":"components/#mistral-gateway","text":"from rakam_systems.ai_agents import MistralGateway gateway = MistralGateway( model=\"mistral-large-latest\", api_key=\"...\" # Or use MISTRAL_API_KEY env var )","title":"Mistral Gateway"},{"location":"components/#gateway-factory","text":"from rakam_systems.ai_agents import LLMGatewayFactory, get_llm_gateway # Using factory gateway = LLMGatewayFactory.create( provider=\"openai\", model=\"gpt-4o\", api_key=\"...\" ) # Using convenience function gateway = get_llm_gateway(provider=\"openai\", model=\"gpt-4o\")","title":"Gateway Factory"},{"location":"components/#chat-history","text":"","title":"Chat History"},{"location":"components/#json-chat-history","text":"from rakam_systems.ai_agents.components.chat_history import JSONChatHistory history = JSONChatHistory(config={\"storage_path\": \"./chat_history.json\"}) # Add messages history.add_message(\"chat123\", {\"role\": \"user\", \"content\": \"Hello!\"}) history.add_message(\"chat123\", {\"role\": \"assistant\", \"content\": \"Hi there!\"}) # Get history messages = history.get_chat_history(\"chat123\") readable = history.get_readable_chat_history(\"chat123\") # Pydantic AI integration message_history = history.get_message_history(\"chat123\") result = await agent.run(\"Hello\", message_history=message_history) history.save_messages(\"chat123\", result.all_messages()) # Manage chats all_chats = history.get_all_chat_ids() history.delete_chat_history(\"chat123\") history.clear_all()","title":"JSON Chat History"},{"location":"components/#sql-chat-history","text":"For production deployments with database-backed storage.","title":"SQL Chat History"},{"location":"components/#vector-store-module-ai_vectorstore","text":"","title":"\ud83d\udd0d Vector Store Module (ai_vectorstore)"},{"location":"components/#core-data-structures","text":"from rakam_systems.ai_vectorstore.core import Node, NodeMetadata, VSFile # VSFile - Represents a document source vsfile = VSFile(file_path=\"/path/to/document.pdf\") print(vsfile.uuid, vsfile.file_name, vsfile.mime_type) # NodeMetadata - Metadata for document chunks metadata = NodeMetadata( source_file_uuid=str(vsfile.uuid), position=0, # Page number or chunk position custom={\"author\": \"John\", \"date\": \"2024-01-01\"} ) # Node - A chunk with content and metadata node = Node(content=\"Document content here...\", metadata=metadata) node.embedding = [0.1, 0.2, 0.3, ...] # Set after embedding","title":"Core Data Structures"},{"location":"components/#configurablepgvectorstore","text":"Enhanced PostgreSQL vector store with full configuration support: from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig # From configuration object config = VectorStoreConfig() store = ConfigurablePgVectorStore(config=config) # From YAML file store = ConfigurablePgVectorStore(config=\"vectorstore_config.yaml\") # From dictionary store = ConfigurablePgVectorStore(config={ \"name\": \"my_store\", \"embedding\": { \"model_type\": \"sentence_transformer\", \"model_name\": \"Snowflake/snowflake-arctic-embed-m\" }, \"search\": { \"similarity_metric\": \"cosine\", \"enable_hybrid_search\": True, \"hybrid_alpha\": 0.7 } }) # Setup (initializes embedding model, database tables) store.setup() # Add documents store.add_nodes(nodes) store.add_vsfile(vsfile) # Search results = store.search(\"What is machine learning?\", top_k=5) results = store.hybrid_search(\"machine learning\", top_k=10, alpha=0.7) # Update vectors store.update_vector(node_id, new_embedding) # Cleanup store.shutdown()","title":"ConfigurablePgVectorStore"},{"location":"components/#multi-model-support","text":"Each embedding model automatically gets dedicated tables: # Using different models - each gets its own tables store_minilm = ConfigurablePgVectorStore(config=config_minilm) store_mpnet = ConfigurablePgVectorStore(config=config_mpnet) # Table names are based on model names: # - application_nodeentry_all_minilm_l6_v2 # - application_nodeentry_snowflake_arctic_embed_m # Disable model-specific tables if needed (not recommended) store = ConfigurablePgVectorStore( config=config, use_dimension_specific_tables=False )","title":"Multi-Model Support"},{"location":"components/#configurableembeddings","text":"Multi-backend embedding model with unified interface: from rakam_systems.ai_vectorstore import ConfigurableEmbeddings, create_embedding_model # Using Sentence Transformers (local) embeddings = ConfigurableEmbeddings(config={ \"model_type\": \"sentence_transformer\", \"model_name\": \"Snowflake/snowflake-arctic-embed-m\", \"batch_size\": 128, \"normalize\": True }) # Using OpenAI embeddings = ConfigurableEmbeddings(config={ \"model_type\": \"openai\", \"model_name\": \"text-embedding-3-small\", \"api_key\": \"...\" # Or use OPENAI_API_KEY }) # Using Cohere embeddings = ConfigurableEmbeddings(config={ \"model_type\": \"cohere\", \"model_name\": \"embed-english-v3.0\", \"api_key\": \"...\" # Or use COHERE_API_KEY }) embeddings.setup() # Encode texts vectors = embeddings.run([\"Hello world\", \"How are you?\"]) query_vector = embeddings.encode_query(\"What is AI?\") doc_vectors = embeddings.encode_documents(documents) # Get dimension dim = embeddings.embedding_dimension","title":"ConfigurableEmbeddings"},{"location":"components/#factory-function","text":"embeddings = create_embedding_model( model_type=\"sentence_transformer\", model_name=\"all-MiniLM-L6-v2\", batch_size=64 )","title":"Factory Function"},{"location":"components/#adaptiveloader","text":"Automatically detects and processes various file types: from rakam_systems.ai_vectorstore import AdaptiveLoader, create_adaptive_loader loader = AdaptiveLoader(config={ \"encoding\": \"utf-8\", \"chunk_size\": 512, \"chunk_overlap\": 50 }) # Supported file types: # - Text: .txt, .text # - Markdown: .md, .markdown # - Documents: .pdf, .docx, .doc, .odt # - Email: .eml, .msg # - Data: .json, .csv, .tsv, .xlsx, .xls # - HTML: .html, .htm, .xhtml # - Code: .py, .js, .ts, .java, .cpp, .go, .rs, .rb, etc. # Load as single text text = loader.load_as_text(\"document.pdf\") # Load as chunks chunks = loader.load_as_chunks(\"document.pdf\") # Load as nodes (with metadata) nodes = loader.load_as_nodes(\"document.pdf\", custom_metadata={\"category\": \"science\"}) # Load as VSFile vsfile = loader.load_as_vsfile(\"document.pdf\") # Also handles raw text chunks = loader.load_as_chunks(\"This is raw text content...\")","title":"AdaptiveLoader"},{"location":"components/#factory-function_1","text":"loader = create_adaptive_loader( chunk_size=1024, chunk_overlap=100, encoding='utf-8' )","title":"Factory Function"},{"location":"components/#specialized-loaders","text":"Located in ai_vectorstore/components/loader/ : Loader File Types Features PdfLoader .pdf Text extraction, page-aware chunking DocLoader .docx , .doc Microsoft Word documents OdtLoader .odt OpenDocument Text MdLoader .md Markdown with structure preservation HtmlLoader .html , .htm HTML parsing and text extraction EmlLoader .eml , .msg Email files with attachments TabularLoader .csv , .tsv , .xlsx Tabular data processing CodeLoader .py , .js , etc. Code-aware chunking","title":"Specialized Loaders"},{"location":"components/#textchunker","text":"Sentence-based text chunking using Chonkie: from rakam_systems.ai_vectorstore.components.chunker import TextChunker, create_text_chunker chunker = TextChunker( chunk_size=512, # Tokens per chunk chunk_overlap=50, # Overlap in tokens min_sentences_per_chunk=1, tokenizer=\"character\" # Or \"gpt2\", HuggingFace tokenizer ) chunks = chunker.chunk_text(\"Long document text...\") # Returns: [{\"text\": \"...\", \"token_count\": 100, \"start_index\": 0, \"end_index\": 500}, ...] # Process multiple documents all_chunks = chunker.run([\"doc1 text\", \"doc2 text\"])","title":"TextChunker"},{"location":"components/#utilities-module-ai_utils","text":"","title":"\ud83d\udee0\ufe0f Utilities Module (ai_utils)"},{"location":"components/#logging","text":"from rakam_systems.ai_utils import logging logger = logging.getLogger(__name__) logger.info(\"Processing document...\") logger.debug(\"Detailed debug info\") logger.error(\"An error occurred\")","title":"Logging"},{"location":"components/#metrics-and-tracing","text":"For production monitoring and observability.","title":"Metrics and Tracing"},{"location":"components/#configuration-system","text":"","title":"\u2699\ufe0f Configuration System"},{"location":"components/#vectorstoreconfig","text":"from rakam_systems.ai_vectorstore.config import ( VectorStoreConfig, EmbeddingConfig, DatabaseConfig, SearchConfig, IndexConfig, load_config ) # Programmatic configuration config = VectorStoreConfig( name=\"my_vectorstore\", embedding=EmbeddingConfig( model_type=\"sentence_transformer\", model_name=\"Snowflake/snowflake-arctic-embed-m\", batch_size=128, normalize=True ), database=DatabaseConfig( host=\"localhost\", port=5432, database=\"vectorstore_db\", user=\"postgres\", password=\"postgres\" ), search=SearchConfig( similarity_metric=\"cosine\", default_top_k=5, enable_hybrid_search=True, hybrid_alpha=0.7 ), index=IndexConfig( chunk_size=512, chunk_overlap=50, batch_insert_size=10000 ) ) # From YAML file config = VectorStoreConfig.from_yaml(\"config.yaml\") # From JSON file config = VectorStoreConfig.from_json(\"config.json\") # From dictionary config = VectorStoreConfig.from_dict(config_dict) # Validation config.validate() # Save configuration config.save_yaml(\"output_config.yaml\") config.save_json(\"output_config.json\")","title":"VectorStoreConfig"},{"location":"components/#yaml-configuration-example","text":"# vectorstore_config.yaml name: production_vectorstore embedding: model_type: sentence_transformer model_name: Snowflake/snowflake-arctic-embed-m batch_size: 128 normalize: true database: host: localhost port: 5432 database: vectorstore_db user: postgres password: postgres search: similarity_metric: cosine default_top_k: 5 enable_hybrid_search: true hybrid_alpha: 0.7 index: chunk_size: 512 chunk_overlap: 50 batch_insert_size: 10000 enable_caching: true cache_size: 1000 enable_logging: true log_level: INFO","title":"YAML Configuration Example"},{"location":"components/#agent-configuration-example","text":"# agent_config.yaml agents: my_agent: name: my_agent llm_config: model: openai:gpt-4o temperature: 0.7 max_tokens: 2000 parallel_tool_calls: true prompt_config: default_prompt tools: - search_tool - calculator deps_type: myapp.models.AgentDeps output_type: name: AgentOutput fields: answer: type: str description: The answer confidence: type: float description: Confidence score enable_tracking: true prompts: default_prompt: system_prompt: | You are a helpful AI assistant. Always provide accurate and helpful responses. tools: search_tool: name: search_tool type: direct module: myapp.tools function: search description: Search for information json_schema: type: object properties: query: type: string required: [query] calculator: name: calculator type: direct module: myapp.tools function: calculate description: Perform calculations","title":"Agent Configuration Example"},{"location":"components/#quick-start-examples","text":"","title":"\ud83d\ude80 Quick Start Examples"},{"location":"components/#basic-agent","text":"import asyncio from rakam_systems.ai_agents import BaseAgent async def main(): agent = BaseAgent( name=\"assistant\", model=\"openai:gpt-4o\", system_prompt=\"You are a helpful assistant.\" ) result = await agent.arun(\"What is the capital of France?\") print(result.output_text) asyncio.run(main())","title":"Basic Agent"},{"location":"components/#agent-with-tools","text":"import asyncio from rakam_systems.ai_agents import BaseAgent from rakam_systems.ai_core.interfaces.tool import ToolComponent def get_weather(city: str) -> str: return f\"The weather in {city} is sunny, 25\u00b0C\" weather_tool = ToolComponent.from_function( function=get_weather, name=\"get_weather\", description=\"Get the current weather for a city\", json_schema={ \"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\"}}, \"required\": [\"city\"] } ) async def main(): agent = BaseAgent( name=\"weather_assistant\", model=\"openai:gpt-4o\", system_prompt=\"You help users with weather information.\", tools=[weather_tool] ) result = await agent.arun(\"What's the weather in Paris?\") print(result.output_text) asyncio.run(main())","title":"Agent with Tools"},{"location":"components/#document-search-pipeline","text":"from rakam_systems.ai_vectorstore import ( ConfigurablePgVectorStore, VectorStoreConfig, AdaptiveLoader ) # Configure vector store config = VectorStoreConfig() store = ConfigurablePgVectorStore(config=config) store.setup() # Load documents loader = AdaptiveLoader(config={\"chunk_size\": 512}) nodes = loader.load_as_nodes(\"documents/research_paper.pdf\") # Add to vector store store.add_nodes(nodes) # Search results = store.search(\"What are the main findings?\", top_k=5) for result in results: print(f\"Score: {result['score']:.4f}\") print(f\"Content: {result['content'][:200]}...\") print(\"---\") store.shutdown()","title":"Document Search Pipeline"},{"location":"components/#full-rag-pipeline","text":"import asyncio from rakam_systems.ai_agents import BaseAgent from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore, AdaptiveLoader, VectorStoreConfig from rakam_systems.ai_core.interfaces.tool import ToolComponent # Setup vector store config = VectorStoreConfig() store = ConfigurablePgVectorStore(config=config) store.setup() # Index documents loader = AdaptiveLoader() for doc_path in [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]: nodes = loader.load_as_nodes(doc_path) store.add_nodes(nodes) # Create search tool def search_documents(query: str, top_k: int = 5) -> str: results = store.search(query, top_k=top_k) return \"\\n\\n\".join([r['content'] for r in results]) search_tool = ToolComponent.from_function( function=search_documents, name=\"search_documents\", description=\"Search the document database\", json_schema={ \"type\": \"object\", \"properties\": { \"query\": {\"type\": \"string\", \"description\": \"Search query\"}, \"top_k\": {\"type\": \"integer\", \"description\": \"Number of results\"} }, \"required\": [\"query\"] } ) # Create RAG agent async def main(): agent = BaseAgent( name=\"rag_agent\", model=\"openai:gpt-4o\", system_prompt=\"\"\"You are a helpful assistant with access to a document database. Use the search_documents tool to find relevant information before answering questions.\"\"\", tools=[search_tool] ) result = await agent.arun(\"What are the key points from the documents?\") print(result.output_text) asyncio.run(main()) store.shutdown()","title":"Full RAG Pipeline"},{"location":"components/#environment-variables","text":"The system supports the following environment variables: Variable Description Used By OPENAI_API_KEY OpenAI API key OpenAIGateway, ConfigurableEmbeddings MISTRAL_API_KEY Mistral API key MistralGateway COHERE_API_KEY Cohere API key ConfigurableEmbeddings POSTGRES_HOST PostgreSQL host DatabaseConfig POSTGRES_PORT PostgreSQL port DatabaseConfig POSTGRES_DB PostgreSQL database DatabaseConfig POSTGRES_USER PostgreSQL user DatabaseConfig POSTGRES_PASSWORD PostgreSQL password DatabaseConfig","title":"Environment Variables"},{"location":"components/#best-practices","text":"Always use context managers or explicit setup() / shutdown() for proper resource management Use configuration files for production deployments instead of hardcoded values Enable tracking during development for debugging and evaluation Use model-specific tables (default) to prevent mixing incompatible vector spaces Batch operations when processing large document collections Use async methods ( arun , astream ) for agents as they are powered by Pydantic AI Validate configurations before deployment using config.validate() or loader.validate_config()","title":"\u2705 Best Practices"},{"location":"components/#further-reading","text":"Example configurations: rakam_systems/examples/configs/ Agent examples: rakam_systems/examples/ai_agents_examples/ Vector store examples: rakam_systems/examples/ai_vectorstore_examples/ Loader documentation: rakam_systems/ai_vectorstore/components/loader/docs/ Architecture documentation: rakam_systems/ai_vectorstore/docs/ARCHITECTURE.md","title":"\ud83d\udcda Further Reading"},{"location":"contributing/","text":"\ud83e\udd1d Contributing to Rakam Systems Rakam Systems \u2014 an end-to-end AI Systems Lifecycle Framework. Rakam Systems is built in layers to maximize reuse, maintainability, and clarity. To contribute effectively, please follow the guidelines below. \ud83c\udfd7\ufe0f Contribution Levels Rakam Systems has four contribution layers. Each layer has its own review expectations, coding standards, and acceptance criteria. Level Description Who usually contributes Level 1 \u2014 Syntax Typing, formatting, docstrings, naming consistency Everyone Level 2 \u2014 Components Reusable blocks: tools, gateways, metrics, RAG primitives AI Team Level 3 \u2014 Pipelines Higher-level flows: evaluation runners, monitoring handlers, data flows Everyone Level 4 \u2014 Templates Project generators, system templates, agent templates Everyone Below is a detailed explanation of how to contribute at each level. \ud83d\udd39 Level 1 \u2014 Syntax Contributions (Formatting, typing, naming, documentation) These contributions focus on code quality, clarity, and internal consistency. They are essential for readability and long-term maintainability. \u2714\ufe0f What counts as a Level-1 contribution Fixing typos Update DSL (Domain Specific Language) Adding missing type hints Renaming variables/functions for consistency Improving documentations Adding small unit tests Fixing imports or unused code Refactoring long functions into readable blocks (no behavior change) Adding new syntax \ud83d\udee0\ufe0f How to contribute Ensure your changes do not modify behavior (unless it's a bugfix). Follow the project conventions: Black for formatting Ruff for linting mypy for type checking Run: pre-commit run --all-files pytest Submit a PR with a clear title: example chores: Renamed run_evaluation function to evaluate \ud83d\udcdd Review expectations: 1 reviewer required Changes should be safe, isolated, and reversible \ud83d\udd39 Level 2 \u2014 Component Contributions (Metrics, gateways, tools, utilities, RAG primitives, memory blocks, policies) Components are the building blocks of Rakam Systems. They should be generic, reusable, strongly typed, and easily composable. Contributions may involve improving an existing component or creating a new one. \u2714\ufe0f What counts as a Level-2 contribution A new metric (e.g., retrieval_hit_rate, hallucination_score) A new LLM Gateway (Bedrock, Azure OpenAI, Together AI\u2026) New tool types (search tool, web tool, SQL tool\u2026) \ud83d\udee0\ufe0f How to contribute In case of updating component develop new feature in the client's project Identify the base compoonent from rakam_system to inherent from. create new class that inheret from choosen base class Implement new core logic switch to rakam_system repo: checkout from main copy and adapt new implementaion from the first step push, open pr and request review # e.g. of new gateway implemented in the client's project from __future__ import annotations from abc import ABC, abstractmethod from typing import Any, Dict, Iterator, Optional, Type, TypeVar from pydantic import BaseModel from rakam_systems.rakam_systems.ai_core.interfaces.llm_gateway import ( LLMGateway, LLMRequest, LLMResponse, ) T = TypeVar(\"T\", bound=BaseModel) class AnthropicGateway(LLMGateway): \"\"\" ... \"\"\" def __init__( self, name: str = \"llm_gateway\", ... ): ... @abstractmethod def generate( self, request: LLMRequest, ) -> LLMResponse: \"\"\"Generate a response from the LLM. Args: request: Standardized LLM request Returns: Standardized LLM response \"\"\" raise NotImplementedError @abstractmethod def generate_structured( self, request: LLMRequest, schema: Type[T], ) -> T: \"\"\"Generate structured output conforming to a Pydantic schema. Args: request: Standardized LLM request schema: Pydantic model class to parse response into Returns: Instance of the schema class \"\"\" raise NotImplementedError def stream( self, request: LLMRequest, ) -> Iterator[str]: \"\"\"Stream token/segment responses. Args: request: Standardized LLM request Yields: String chunks from the LLM \"\"\" ... @abstractmethod def count_tokens( self, text: str, model: Optional[str] = None, ) -> int: \"\"\"Count tokens in text. Args: text: Text to count tokens for model: Model name to determine encoding Returns: Number of tokens \"\"\" raise NotImplementedError # Legacy methods for backward compatibility def run(self, prompt: str, **kwargs: Any) -> str: \"\"\"Legacy synchronous text completion.\"\"\" ... In case of creating a new component in client's project develop new component logic write evaluation adapt new logic to become rakam component switch to rakam_system repo: create new module under rakam_system copy and adapt code from first step ( try to follow examples of other existing components) push, open pr and request review \ud83d\udd39 Level 3 \u2014 Pipeline Contributions (Evaluation pipelines, monitoring flows, data ingestion, orchestration) Pipelines are end-to-end flows. They orchestrate multiple components together. \u2714\ufe0f What counts as a Level-3 New evaluation pipeline Improvements to EvalService: run \u2192 test case \u2192 entry \u2192 metrics \u2192 reasons \u2192 logs Data preparation pipelines \ud83d\udee0\ufe0f How to contribute In case of updating component develop new pipeline in the client's project Identify the base compoonent from rakam_system to inherent from. create new class that inheret from choosen base class Implement new pipeline logic switch to rakam_system repo: checkout from main copy and adapt new implementaion from the first step push, open pr and request review \ud83d\udd39 Level 4 \u2014 Template Contributions (System templates, agent templates, scaffolds, generators) Templates define the starting point of entire AI systems. \u2714\ufe0f What counts as a Level-4 contribution A full agent template: Support Agent RAG Agent OCR Agent ... A new service template (FastAPI, Django ...) A deployment template (Terraform/K8s) Improvements to project generation Integrate Monitoring features \ud83d\udee0\ufe0f How to contribute update template (e.g. versions upgrade, update available rakam_system components, bug fixes...) push, open pr and request review","title":"Contributing"},{"location":"contributing/#contributing-to-rakam-systems","text":"Rakam Systems \u2014 an end-to-end AI Systems Lifecycle Framework. Rakam Systems is built in layers to maximize reuse, maintainability, and clarity. To contribute effectively, please follow the guidelines below.","title":"\ud83e\udd1d Contributing to Rakam Systems"},{"location":"contributing/#contribution-levels","text":"Rakam Systems has four contribution layers. Each layer has its own review expectations, coding standards, and acceptance criteria. Level Description Who usually contributes Level 1 \u2014 Syntax Typing, formatting, docstrings, naming consistency Everyone Level 2 \u2014 Components Reusable blocks: tools, gateways, metrics, RAG primitives AI Team Level 3 \u2014 Pipelines Higher-level flows: evaluation runners, monitoring handlers, data flows Everyone Level 4 \u2014 Templates Project generators, system templates, agent templates Everyone Below is a detailed explanation of how to contribute at each level.","title":"\ud83c\udfd7\ufe0f Contribution Levels"},{"location":"contributing/#level-1-syntax-contributions","text":"(Formatting, typing, naming, documentation) These contributions focus on code quality, clarity, and internal consistency. They are essential for readability and long-term maintainability.","title":"\ud83d\udd39 Level 1 \u2014 Syntax Contributions"},{"location":"contributing/#what-counts-as-a-level-1-contribution","text":"Fixing typos Update DSL (Domain Specific Language) Adding missing type hints Renaming variables/functions for consistency Improving documentations Adding small unit tests Fixing imports or unused code Refactoring long functions into readable blocks (no behavior change) Adding new syntax","title":"\u2714\ufe0f What counts as a Level-1 contribution"},{"location":"contributing/#how-to-contribute","text":"Ensure your changes do not modify behavior (unless it's a bugfix). Follow the project conventions: Black for formatting Ruff for linting mypy for type checking Run: pre-commit run --all-files pytest Submit a PR with a clear title: example chores: Renamed run_evaluation function to evaluate","title":"\ud83d\udee0\ufe0f How to contribute"},{"location":"contributing/#review-expectations","text":"1 reviewer required Changes should be safe, isolated, and reversible","title":"\ud83d\udcdd Review expectations:"},{"location":"contributing/#level-2-component-contributions","text":"(Metrics, gateways, tools, utilities, RAG primitives, memory blocks, policies) Components are the building blocks of Rakam Systems. They should be generic, reusable, strongly typed, and easily composable. Contributions may involve improving an existing component or creating a new one.","title":"\ud83d\udd39 Level 2 \u2014 Component Contributions"},{"location":"contributing/#what-counts-as-a-level-2-contribution","text":"A new metric (e.g., retrieval_hit_rate, hallucination_score) A new LLM Gateway (Bedrock, Azure OpenAI, Together AI\u2026) New tool types (search tool, web tool, SQL tool\u2026)","title":"\u2714\ufe0f What counts as a Level-2 contribution"},{"location":"contributing/#how-to-contribute_1","text":"","title":"\ud83d\udee0\ufe0f How to contribute"},{"location":"contributing/#in-case-of-updating-component","text":"develop new feature in the client's project Identify the base compoonent from rakam_system to inherent from. create new class that inheret from choosen base class Implement new core logic switch to rakam_system repo: checkout from main copy and adapt new implementaion from the first step push, open pr and request review # e.g. of new gateway implemented in the client's project from __future__ import annotations from abc import ABC, abstractmethod from typing import Any, Dict, Iterator, Optional, Type, TypeVar from pydantic import BaseModel from rakam_systems.rakam_systems.ai_core.interfaces.llm_gateway import ( LLMGateway, LLMRequest, LLMResponse, ) T = TypeVar(\"T\", bound=BaseModel) class AnthropicGateway(LLMGateway): \"\"\" ... \"\"\" def __init__( self, name: str = \"llm_gateway\", ... ): ... @abstractmethod def generate( self, request: LLMRequest, ) -> LLMResponse: \"\"\"Generate a response from the LLM. Args: request: Standardized LLM request Returns: Standardized LLM response \"\"\" raise NotImplementedError @abstractmethod def generate_structured( self, request: LLMRequest, schema: Type[T], ) -> T: \"\"\"Generate structured output conforming to a Pydantic schema. Args: request: Standardized LLM request schema: Pydantic model class to parse response into Returns: Instance of the schema class \"\"\" raise NotImplementedError def stream( self, request: LLMRequest, ) -> Iterator[str]: \"\"\"Stream token/segment responses. Args: request: Standardized LLM request Yields: String chunks from the LLM \"\"\" ... @abstractmethod def count_tokens( self, text: str, model: Optional[str] = None, ) -> int: \"\"\"Count tokens in text. Args: text: Text to count tokens for model: Model name to determine encoding Returns: Number of tokens \"\"\" raise NotImplementedError # Legacy methods for backward compatibility def run(self, prompt: str, **kwargs: Any) -> str: \"\"\"Legacy synchronous text completion.\"\"\" ...","title":"In case of updating component"},{"location":"contributing/#in-case-of-creating-a-new-component","text":"in client's project develop new component logic write evaluation adapt new logic to become rakam component switch to rakam_system repo: create new module under rakam_system copy and adapt code from first step ( try to follow examples of other existing components) push, open pr and request review","title":"In case of creating a new component"},{"location":"contributing/#level-3-pipeline-contributions","text":"(Evaluation pipelines, monitoring flows, data ingestion, orchestration) Pipelines are end-to-end flows. They orchestrate multiple components together.","title":"\ud83d\udd39 Level 3 \u2014 Pipeline Contributions"},{"location":"contributing/#what-counts-as-a-level-3","text":"New evaluation pipeline Improvements to EvalService: run \u2192 test case \u2192 entry \u2192 metrics \u2192 reasons \u2192 logs Data preparation pipelines","title":"\u2714\ufe0f What counts as a Level-3"},{"location":"contributing/#how-to-contribute_2","text":"","title":"\ud83d\udee0\ufe0f How to contribute"},{"location":"contributing/#in-case-of-updating-component_1","text":"develop new pipeline in the client's project Identify the base compoonent from rakam_system to inherent from. create new class that inheret from choosen base class Implement new pipeline logic switch to rakam_system repo: checkout from main copy and adapt new implementaion from the first step push, open pr and request review","title":"In case of updating component"},{"location":"contributing/#level-4-template-contributions","text":"(System templates, agent templates, scaffolds, generators) Templates define the starting point of entire AI systems.","title":"\ud83d\udd39 Level 4 \u2014 Template Contributions"},{"location":"contributing/#what-counts-as-a-level-4-contribution","text":"A full agent template: Support Agent RAG Agent OCR Agent ... A new service template (FastAPI, Django ...) A deployment template (Terraform/K8s) Improvements to project generation Integrate Monitoring features","title":"\u2714\ufe0f What counts as a Level-4 contribution"},{"location":"contributing/#how-to-contribute_3","text":"update template (e.g. versions upgrade, update available rakam_system components, bug fixes...) push, open pr and request review","title":"\ud83d\udee0\ufe0f How to contribute"},{"location":"deployment/","text":"","title":"Deployment & Operation"},{"location":"development_guide/","text":"Rakam Systems Development Guide This guide provides comprehensive instructions for developers who want to contribute to or extend the rakam_systems framework. It covers architecture patterns, component development, testing strategies, and best practices. \ud83d\udcd1 Table of Contents Getting Started Architecture Overview Core Concepts Developing Components Creating Custom Agents Building Tools Implementing Vector Stores Creating Document Loaders Configuration System Testing Guidelines Code Style & Conventions Publishing & Distribution Getting Started Development Environment Setup # Clone the repository git clone <repository_url> cd rakam_systems/app/rakam_systems # Create a virtual environment python -m venv venv source venv/bin/activate # On Windows: venv\\Scripts\\activate # Install with all dependencies including dev tools pip install -e \".[complete]\" # Setup pre-commit hooks pre-commit install # Verify installation python -c \"from rakam_systems.ai_core.base import BaseComponent; print('\u2705 Setup complete!')\" Project Structure rakam_systems/ \u251c\u2500\u2500 rakam_systems/ # Main package \u2502 \u251c\u2500\u2500 ai_core/ # Core abstractions & interfaces \u2502 \u2502 \u251c\u2500\u2500 base.py # BaseComponent class \u2502 \u2502 \u251c\u2500\u2500 interfaces/ # Abstract interfaces \u2502 \u2502 \u251c\u2500\u2500 config_loader.py # Configuration system \u2502 \u2502 \u251c\u2500\u2500 tracking.py # Input/output tracking \u2502 \u2502 \u2514\u2500\u2500 mcp/ # MCP server support \u2502 \u251c\u2500\u2500 ai_agents/ # Agent implementations \u2502 \u2502 \u251c\u2500\u2500 components/ # Agent components \u2502 \u2502 \u2502 \u251c\u2500\u2500 base_agent.py # BaseAgent implementation \u2502 \u2502 \u2502 \u251c\u2500\u2500 llm_gateway/ # LLM provider gateways \u2502 \u2502 \u2502 \u251c\u2500\u2500 chat_history/ # Chat history backends \u2502 \u2502 \u2502 \u2514\u2500\u2500 tools/ # Built-in tools \u2502 \u2502 \u2514\u2500\u2500 server/ # MCP server for agents \u2502 \u251c\u2500\u2500 ai_vectorstore/ # Vector store implementations \u2502 \u2502 \u251c\u2500\u2500 core.py # Node, VSFile data structures \u2502 \u2502 \u251c\u2500\u2500 config.py # VectorStoreConfig \u2502 \u2502 \u2514\u2500\u2500 components/ # Vector store components \u2502 \u2502 \u251c\u2500\u2500 vectorstore/ # Store implementations \u2502 \u2502 \u251c\u2500\u2500 embedding_model/ # Embedding models \u2502 \u2502 \u251c\u2500\u2500 loader/ # Document loaders \u2502 \u2502 \u2514\u2500\u2500 chunker/ # Text chunkers \u2502 \u251c\u2500\u2500 ai_utils/ # Utilities (logging, metrics) \u2502 \u2514\u2500\u2500 examples/ # Usage examples \u251c\u2500\u2500 tests/ # Test suite \u251c\u2500\u2500 docs/ # Documentation \u251c\u2500\u2500 pyproject.toml # Package configuration \u2514\u2500\u2500 requirements.txt # Locked dependencies Architecture Overview Design Principles Component-Based Architecture : All functionality is encapsulated in components that extend BaseComponent Interface-Driven Development : Abstract interfaces define contracts; implementations are swappable Configuration-First : YAML/JSON configuration support for all components Provider-Agnostic : Support multiple backends (LLMs, embeddings, vector stores) Lifecycle Management : Components have setup() and shutdown() hooks for resource management Component Hierarchy BaseComponent (ai_core/base.py) \u251c\u2500\u2500 AgentComponent (ai_core/interfaces/agent.py) \u2502 \u2514\u2500\u2500 BaseAgent (ai_agents/components/base_agent.py) \u251c\u2500\u2500 ToolComponent (ai_core/interfaces/tool.py) \u2502 \u2514\u2500\u2500 FunctionToolComponent \u251c\u2500\u2500 LLMGateway (ai_core/interfaces/llm_gateway.py) \u2502 \u251c\u2500\u2500 OpenAIGateway \u2502 \u2514\u2500\u2500 MistralGateway \u251c\u2500\u2500 VectorStore (ai_core/interfaces/vectorstore.py) \u2502 \u251c\u2500\u2500 ConfigurablePgVectorStore \u2502 \u2514\u2500\u2500 FAISSVectorStore \u251c\u2500\u2500 EmbeddingModel (ai_core/interfaces/embedding_model.py) \u2502 \u2514\u2500\u2500 ConfigurableEmbeddings \u251c\u2500\u2500 Loader (ai_core/interfaces/loader.py) \u2502 \u251c\u2500\u2500 AdaptiveLoader \u2502 \u251c\u2500\u2500 PdfLoader, DocLoader, etc. \u2514\u2500\u2500 Chunker (ai_core/interfaces/chunker.py) \u2514\u2500\u2500 TextChunker Core Concepts BaseComponent Every component in the system extends BaseComponent , which provides: Lifecycle management : setup() and shutdown() methods Auto-initialization : __call__ automatically calls setup() if needed Context manager support : Use with statement for automatic cleanup Evaluation harness : Built-in method for testing components from abc import abstractmethod from typing import Any, Dict, Optional from rakam_systems.ai_core.base import BaseComponent class MyComponent(BaseComponent): \"\"\"Example custom component.\"\"\" def __init__(self, name: str, config: Optional[Dict[str, Any]] = None): super().__init__(name, config) self.heavy_resource = None def setup(self) -> None: \"\"\"Initialize heavy resources.\"\"\" super().setup() # Sets self.initialized = True # Initialize expensive resources here self.heavy_resource = self._load_model() def shutdown(self) -> None: \"\"\"Release resources.\"\"\" if self.heavy_resource: self.heavy_resource = None super().shutdown() # Sets self.initialized = False @abstractmethod def run(self, *args, **kwargs) -> Any: \"\"\"Execute the primary operation.\"\"\" raise NotImplementedError def _load_model(self): \"\"\"Private method to load model.\"\"\" return \"loaded_model\" Usage Patterns # Pattern 1: Manual lifecycle component = MyComponent(\"my_component\") component.setup() result = component.run(input_data) component.shutdown() # Pattern 2: Auto-setup via __call__ component = MyComponent(\"my_component\") result = component(input_data) # setup() called automatically # Pattern 3: Context manager (recommended) with MyComponent(\"my_component\") as component: result = component.run(input_data) # shutdown() called automatically Interfaces Interfaces define contracts that implementations must follow. They are located in ai_core/interfaces/ : Interface Purpose Key Methods AgentComponent AI agents run() , arun() , stream() , astream() ToolComponent Callable tools run() , acall() LLMGateway LLM providers generate() , stream() , count_tokens() VectorStore Vector storage add() , query() , count() EmbeddingModel Text embeddings run() , encode_query() , encode_documents() Loader Document loading load_as_text() , load_as_chunks() , load_as_nodes() Chunker Text chunking run() , chunk_text() Developing Components Step 1: Choose the Right Interface Determine which interface your component should implement: # For a new LLM provider from rakam_systems.ai_core.interfaces.llm_gateway import LLMGateway # For a new vector store backend from rakam_systems.ai_core.interfaces.vectorstore import VectorStore # For a new document loader from rakam_systems.ai_core.interfaces.loader import Loader # For a custom tool from rakam_systems.ai_core.interfaces.tool import ToolComponent Step 2: Implement Required Methods from typing import List, Dict, Any, Optional from rakam_systems.ai_core.interfaces.vectorstore import VectorStore class MyCustomVectorStore(VectorStore): \"\"\"Custom vector store implementation.\"\"\" def __init__( self, name: str = \"my_vectorstore\", config: Optional[Dict[str, Any]] = None ): super().__init__(name, config) self.index = None self.dimension = config.get(\"dimension\", 384) if config else 384 def setup(self) -> None: \"\"\"Initialize the vector index.\"\"\" super().setup() # Initialize your backend here self.index = self._create_index() def shutdown(self) -> None: \"\"\"Clean up resources.\"\"\" if self.index: self._save_index() self.index = None super().shutdown() def run(self, *args, **kwargs) -> Any: \"\"\"Default run delegates to query.\"\"\" return self.query(*args, **kwargs) def add( self, vectors: List[List[float]], metadatas: List[Dict[str, Any]] ) -> Any: \"\"\"Add vectors to the store.\"\"\" if not self.initialized: self.setup() # Implementation here return {\"added\": len(vectors)} def query( self, vector: List[float], top_k: int = 5 ) -> List[Dict[str, Any]]: \"\"\"Query similar vectors.\"\"\" if not self.initialized: self.setup() # Implementation here return [{\"id\": i, \"score\": 0.9} for i in range(top_k)] def count(self) -> Optional[int]: \"\"\"Return total vector count.\"\"\" return len(self.index) if self.index else 0 def _create_index(self): \"\"\"Private method to create index.\"\"\" return {} def _save_index(self): \"\"\"Private method to save index.\"\"\" pass Step 3: Add Configuration Support Support both programmatic and YAML/JSON configuration: from pydantic import BaseModel, Field from typing import Optional import yaml class MyVectorStoreConfig(BaseModel): \"\"\"Configuration for MyCustomVectorStore.\"\"\" name: str = \"my_vectorstore\" dimension: int = Field(default=384, ge=1) backend: str = Field(default=\"memory\", pattern=\"^(memory|disk|redis)$\") cache_size: int = Field(default=1000, ge=0) @classmethod def from_yaml(cls, path: str) -> \"MyVectorStoreConfig\": \"\"\"Load configuration from YAML file.\"\"\" with open(path, 'r') as f: data = yaml.safe_load(f) return cls(**data) def to_dict(self) -> Dict[str, Any]: \"\"\"Convert to dictionary.\"\"\" return self.model_dump() class MyCustomVectorStore(VectorStore): \"\"\"Custom vector store with config support.\"\"\" def __init__( self, config: Optional[MyVectorStoreConfig | Dict | str] = None ): # Handle different config types if isinstance(config, str): config = MyVectorStoreConfig.from_yaml(config) elif isinstance(config, dict): config = MyVectorStoreConfig(**config) elif config is None: config = MyVectorStoreConfig() super().__init__(config.name, config.to_dict()) self.config_obj = config Creating Custom Agents Basic Agent import asyncio from typing import Optional, List, Any from pydantic import BaseModel from rakam_systems.ai_agents.components import BaseAgent from rakam_systems.ai_core.interfaces.agent import AgentInput, AgentOutput from rakam_systems.ai_core.interfaces.tool import ToolComponent class MyCustomAgent(BaseAgent): \"\"\"Custom agent with specialized behavior.\"\"\" def __init__( self, name: str = \"custom_agent\", model: str = \"openai:gpt-4o\", system_prompt: str = \"You are a helpful assistant.\", tools: Optional[List[ToolComponent]] = None, output_type: Optional[type[BaseModel]] = None, **kwargs ): super().__init__( name=name, model=model, system_prompt=system_prompt, tools=tools, output_type=output_type, **kwargs ) async def arun( self, input_data: str | AgentInput, deps: Any = None, **kwargs ) -> AgentOutput: \"\"\"Override to add custom preprocessing/postprocessing.\"\"\" # Preprocess if isinstance(input_data, str): input_data = self._preprocess(input_data) # Call parent implementation result = await super().arun(input_data, deps, **kwargs) # Postprocess result = self._postprocess(result) return result def _preprocess(self, text: str) -> str: \"\"\"Add preprocessing logic.\"\"\" # Example: Add context or modify input return f\"[User Query] {text}\" def _postprocess(self, result: AgentOutput) -> AgentOutput: \"\"\"Add postprocessing logic.\"\"\" # Example: Format output return result Agent with Structured Output from pydantic import BaseModel, Field from typing import List class AnalysisResult(BaseModel): \"\"\"Structured output for analysis agent.\"\"\" summary: str = Field(description=\"Brief summary of the analysis\") key_points: List[str] = Field(description=\"Key points extracted\") confidence: float = Field(ge=0.0, le=1.0, description=\"Confidence score\") needs_review: bool = Field(default=False, description=\"Whether human review is needed\") # Create agent with structured output analysis_agent = BaseAgent( name=\"analysis_agent\", model=\"openai:gpt-4o\", system_prompt=\"You analyze text and provide structured analysis.\", output_type=AnalysisResult # Enforces structured output ) # Usage async def analyze(text: str) -> AnalysisResult: result = await analysis_agent.arun(f\"Analyze this: {text}\") return result.output # Typed as AnalysisResult Agent with Dynamic System Prompts from datetime import date from pydantic_ai import RunContext agent = BaseAgent( name=\"dynamic_agent\", model=\"openai:gpt-4o\", system_prompt=\"You are a helpful assistant.\" ) # Add dynamic prompts that are evaluated at runtime @agent.dynamic_system_prompt def add_date_context() -> str: \"\"\"Add current date to system prompt.\"\"\" return f\"Today's date is {date.today().strftime('%B %d, %Y')}.\" @agent.dynamic_system_prompt def add_user_context(ctx: RunContext[dict]) -> str: \"\"\"Add user-specific context from dependencies.\"\"\" if ctx.deps and \"user_name\" in ctx.deps: return f\"You are assisting {ctx.deps['user_name']}.\" return \"\" # Usage with dependencies result = await agent.arun( \"What day is it?\", deps={\"user_name\": \"Alice\"} ) Building Tools Method 1: Class-Based Tool from typing import Any, Dict from rakam_systems.ai_core.interfaces.tool import ToolComponent class WeatherTool(ToolComponent): \"\"\"Tool to get weather information.\"\"\" def __init__(self, api_key: str = None): super().__init__( name=\"get_weather\", description=\"Get current weather for a location\", json_schema={ \"type\": \"object\", \"properties\": { \"city\": { \"type\": \"string\", \"description\": \"City name\" }, \"units\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"default\": \"celsius\" } }, \"required\": [\"city\"], \"additionalProperties\": False } ) self.api_key = api_key def run(self, city: str, units: str = \"celsius\") -> str: \"\"\"Fetch weather data.\"\"\" # Implementation here return f\"Weather in {city}: 22\u00b0{units[0].upper()}, Sunny\" Method 2: Function-Based Tool (Recommended for Simple Tools) from rakam_systems.ai_core.interfaces.tool import ToolComponent def calculate_bmi(weight_kg: float, height_m: float) -> dict: \"\"\"Calculate BMI given weight and height.\"\"\" bmi = weight_kg / (height_m ** 2) category = ( \"Underweight\" if bmi < 18.5 else \"Normal\" if bmi < 25 else \"Overweight\" if bmi < 30 else \"Obese\" ) return {\"bmi\": round(bmi, 2), \"category\": category} # Create tool from function bmi_tool = ToolComponent.from_function( function=calculate_bmi, name=\"calculate_bmi\", description=\"Calculate Body Mass Index from weight and height\", json_schema={ \"type\": \"object\", \"properties\": { \"weight_kg\": { \"type\": \"number\", \"description\": \"Weight in kilograms\" }, \"height_m\": { \"type\": \"number\", \"description\": \"Height in meters\" } }, \"required\": [\"weight_kg\", \"height_m\"], \"additionalProperties\": False } ) Method 3: Async Tool import aiohttp from rakam_systems.ai_core.interfaces.tool import ToolComponent class AsyncSearchTool(ToolComponent): \"\"\"Async tool for web search.\"\"\" def __init__(self): super().__init__( name=\"web_search\", description=\"Search the web for information\", json_schema={ \"type\": \"object\", \"properties\": { \"query\": {\"type\": \"string\", \"description\": \"Search query\"} }, \"required\": [\"query\"] } ) async def run(self, query: str) -> str: \"\"\"Perform async web search.\"\"\" async with aiohttp.ClientSession() as session: async with session.get(f\"https://api.search.com?q={query}\") as resp: data = await resp.json() return data.get(\"results\", []) # acall is automatically handled for async run methods Registering Tools with ToolRegistry from rakam_systems.ai_core.interfaces.tool_registry import ToolRegistry, ToolMode registry = ToolRegistry() # Register tools registry.register_direct_tool( name=\"weather\", function=get_weather_fn, description=\"Get weather information\", json_schema={...}, category=\"utility\", tags=[\"weather\", \"external\"] ) registry.register_direct_tool( name=\"calculator\", function=calculate_fn, description=\"Perform calculations\", json_schema={...}, category=\"math\", tags=[\"calculation\"] ) # Query tools math_tools = registry.get_tools_by_category(\"math\") external_tools = registry.get_tools_by_tag(\"external\") all_direct_tools = registry.get_tools_by_mode(ToolMode.DIRECT) Implementing Vector Stores Custom Vector Store from typing import List, Dict, Any, Optional from rakam_systems.ai_core.interfaces.vectorstore import VectorStore from rakam_systems.ai_vectorstore.core import Node, NodeMetadata class RedisVectorStore(VectorStore): \"\"\"Redis-backed vector store implementation.\"\"\" def __init__( self, name: str = \"redis_vectorstore\", host: str = \"localhost\", port: int = 6379, index_name: str = \"vectors\", dimension: int = 384, config: Optional[Dict] = None ): super().__init__(name, config or {}) self.host = host self.port = port self.index_name = index_name self.dimension = dimension self.client = None def setup(self) -> None: \"\"\"Connect to Redis and create index.\"\"\" super().setup() import redis from redis.commands.search.field import VectorField, TextField from redis.commands.search.indexDefinition import IndexDefinition, IndexType self.client = redis.Redis(host=self.host, port=self.port) # Create vector index try: self.client.ft(self.index_name).create_index( fields=[ VectorField(\"embedding\", \"HNSW\", { \"TYPE\": \"FLOAT32\", \"DIM\": self.dimension, \"DISTANCE_METRIC\": \"COSINE\" }), TextField(\"content\"), ], definition=IndexDefinition( prefix=[f\"{self.index_name}:\"], index_type=IndexType.HASH ) ) except Exception: pass # Index already exists def shutdown(self) -> None: \"\"\"Close Redis connection.\"\"\" if self.client: self.client.close() self.client = None super().shutdown() def run(self, *args, **kwargs) -> Any: return self.query(*args, **kwargs) def add( self, vectors: List[List[float]], metadatas: List[Dict[str, Any]] ) -> Dict[str, Any]: \"\"\"Add vectors to Redis.\"\"\" if not self.initialized: self.setup() import numpy as np added = 0 for i, (vector, metadata) in enumerate(zip(vectors, metadatas)): doc_id = metadata.get(\"id\", f\"doc_{i}\") self.client.hset( f\"{self.index_name}:{doc_id}\", mapping={ \"embedding\": np.array(vector, dtype=np.float32).tobytes(), \"content\": metadata.get(\"content\", \"\"), **metadata } ) added += 1 return {\"added\": added} def query( self, vector: List[float], top_k: int = 5 ) -> List[Dict[str, Any]]: \"\"\"Search for similar vectors.\"\"\" if not self.initialized: self.setup() import numpy as np from redis.commands.search.query import Query query_vector = np.array(vector, dtype=np.float32).tobytes() q = ( Query(f\"*=>[KNN {top_k} @embedding $vec AS score]\") .sort_by(\"score\") .return_fields(\"content\", \"score\") .dialect(2) ) results = self.client.ft(self.index_name).search( q, query_params={\"vec\": query_vector} ) return [ { \"id\": doc.id, \"content\": doc.content, \"score\": float(doc.score) } for doc in results.docs ] def count(self) -> Optional[int]: \"\"\"Count total documents.\"\"\" if not self.client: return 0 info = self.client.ft(self.index_name).info() return int(info.get(\"num_docs\", 0)) Creating Document Loaders Custom Loader from typing import List, Union, Optional, Dict, Any from pathlib import Path from rakam_systems.ai_core.interfaces.loader import Loader from rakam_systems.ai_vectorstore.core import Node, NodeMetadata, VSFile class XMLLoader(Loader): \"\"\"Loader for XML documents.\"\"\" def __init__( self, name: str = \"xml_loader\", config: Optional[Dict[str, Any]] = None ): super().__init__(name, config or {}) self.chunk_size = self.config.get(\"chunk_size\", 512) self.encoding = self.config.get(\"encoding\", \"utf-8\") def run(self, source: Union[str, Path]) -> List[str]: \"\"\"Default run delegates to load_as_chunks.\"\"\" return self.load_as_chunks(source) def load_as_text(self, source: Union[str, Path]) -> str: \"\"\"Load XML as plain text (elements removed).\"\"\" import xml.etree.ElementTree as ET tree = ET.parse(source) root = tree.getroot() # Extract all text content texts = [] for elem in root.iter(): if elem.text: texts.append(elem.text.strip()) if elem.tail: texts.append(elem.tail.strip()) return \" \".join(filter(None, texts)) def load_as_chunks(self, source: Union[str, Path]) -> List[str]: \"\"\"Load XML and split into chunks.\"\"\" text = self.load_as_text(source) # Simple chunking by character count chunks = [] for i in range(0, len(text), self.chunk_size): chunk = text[i:i + self.chunk_size] if chunk.strip(): chunks.append(chunk) return chunks def load_as_nodes( self, source: Union[str, Path], source_id: Optional[str] = None, custom_metadata: Optional[Dict[str, Any]] = None ) -> List[Node]: \"\"\"Load XML as nodes with metadata.\"\"\" source_path = Path(source) chunks = self.load_as_chunks(source) nodes = [] for i, chunk in enumerate(chunks): metadata = NodeMetadata( source_file_uuid=source_id or str(source_path), position=i, custom=custom_metadata or {} ) nodes.append(Node(content=chunk, metadata=metadata)) return nodes def load_as_vsfile( self, file_path: Union[str, Path], custom_metadata: Optional[Dict[str, Any]] = None ) -> VSFile: \"\"\"Load XML as VSFile with nodes attached.\"\"\" vsfile = VSFile(file_path=str(file_path)) vsfile.nodes = self.load_as_nodes( file_path, source_id=str(vsfile.uuid), custom_metadata=custom_metadata ) return vsfile Integrating with AdaptiveLoader To make your loader work with AdaptiveLoader , register the file extension: from rakam_systems.ai_vectorstore.components.loader import AdaptiveLoader class EnhancedAdaptiveLoader(AdaptiveLoader): \"\"\"AdaptiveLoader with XML support.\"\"\" EXTENSION_LOADERS = { **AdaptiveLoader.EXTENSION_LOADERS, \".xml\": XMLLoader, \".xhtml\": XMLLoader, } Configuration System YAML Configuration The framework supports comprehensive YAML configuration: # config/agent_config.yaml version: \"1.0\" # Prompt library prompts: assistant: name: \"assistant\" system_prompt: | You are a helpful AI assistant. Always be accurate and helpful. skills: - \"Information retrieval\" - \"Task automation\" # Tool library tools: search: name: \"search\" type: \"direct\" module: \"myapp.tools\" function: \"search_documents\" description: \"Search documents\" json_schema: type: \"object\" properties: query: type: \"string\" required: [\"query\"] # Agent configurations agents: main_agent: name: \"main_agent\" model_config: model: \"openai:gpt-4o\" temperature: 0.7 max_tokens: 2000 prompt_config: \"assistant\" tools: - \"search\" enable_tracking: true Loading Configuration from rakam_systems.ai_core.config_loader import ConfigurationLoader loader = ConfigurationLoader() # Load from YAML config = loader.load_from_yaml(\"config/agent_config.yaml\") # Create single agent agent = loader.create_agent(\"main_agent\", config) # Create all agents all_agents = loader.create_all_agents(config) # Get tool registry registry = loader.get_tool_registry(config) # Validate configuration is_valid, errors = loader.validate_config(\"config/agent_config.yaml\") if not is_valid: for error in errors: print(f\"Config error: {error}\") Testing Guidelines Unit Test Structure # tests/test_my_component.py import pytest from typing import List, Dict, Any from rakam_systems.ai_core.interfaces.vectorstore import VectorStore class DummyVectorStore(VectorStore): \"\"\"Mock implementation for testing.\"\"\" def __init__(self): super().__init__(\"dummy_store\") self.data = [] def run(self, *args, **kwargs): return self.query(*args, **kwargs) def add(self, vectors: List[List[float]], metadatas: List[Dict]) -> Dict: self.data.extend(zip(vectors, metadatas)) return {\"added\": len(vectors)} def query(self, vector: List[float], top_k: int = 5) -> List[Dict]: return [{\"id\": i, \"score\": 0.9} for i in range(min(top_k, len(self.data)))] def count(self) -> int: return len(self.data) class TestVectorStore: \"\"\"Test suite for VectorStore implementations.\"\"\" @pytest.fixture def store(self): \"\"\"Create a fresh store for each test.\"\"\" store = DummyVectorStore() yield store store.shutdown() def test_add_vectors(self, store): \"\"\"Test adding vectors to store.\"\"\" vectors = [[0.1, 0.2], [0.3, 0.4]] metadatas = [{\"id\": 1}, {\"id\": 2}] result = store.add(vectors, metadatas) assert result[\"added\"] == 2 assert store.count() == 2 def test_query_vectors(self, store): \"\"\"Test querying vectors.\"\"\" store.add([[0.1, 0.2]], [{\"id\": 1}]) results = store.query([0.1, 0.2], top_k=5) assert len(results) <= 5 assert all(\"score\" in r for r in results) def test_lifecycle(self, store): \"\"\"Test setup/shutdown lifecycle.\"\"\" assert not store.initialized store.setup() assert store.initialized store.shutdown() assert not store.initialized def test_evaluate_harness(self, store): \"\"\"Test built-in evaluation harness.\"\"\" store.add([[0.1]], [{\"id\": 1}]) test_cases = { \"query\": [ {\"args\": [[0.1, 0.2]], \"kwargs\": {\"top_k\": 3}} ] } results = store.evaluate( methods=[\"query\"], test_cases=test_cases, verbose=False ) assert \"query\" in results assert results[\"query\"][0][\"success\"] Async Test Structure import pytest import asyncio @pytest.fixture def event_loop(): \"\"\"Create event loop for async tests.\"\"\" loop = asyncio.new_event_loop() yield loop loop.close() @pytest.mark.asyncio async def test_async_agent(): \"\"\"Test async agent methods.\"\"\" from rakam_systems.ai_agents.components import BaseAgent agent = BaseAgent( name=\"test_agent\", model=\"openai:gpt-4o-mini\", system_prompt=\"You are a test assistant.\" ) result = await agent.arun(\"Say 'hello'\") assert result.output_text is not None assert len(result.output_text) > 0 Running Tests # Run all tests pytest # Run with coverage pytest --cov=rakam_systems --cov-report=html # Run specific test file pytest tests/test_my_component.py # Run tests matching pattern pytest -k \"test_vector\" # Run async tests pytest -v tests/test_async_*.py Code Style & Conventions Python Style Follow PEP 8 with these project-specific conventions: # Imports: Standard library, third-party, local (separated by blank lines) from __future__ import annotations import asyncio from typing import Any, Dict, List, Optional import numpy as np from pydantic import BaseModel from rakam_systems.ai_core.base import BaseComponent # Class docstrings: Brief description + Attributes + Methods class MyComponent(BaseComponent): \"\"\"Brief description of the component. This component does X, Y, and Z. Use it when you need to... Attributes: name: Component identifier config: Configuration dictionary some_param: Description of parameter Example: ```python component = MyComponent(\"my_comp\", config={\"key\": \"value\"}) with component: result = component.run(input_data) ``` \"\"\" # Method docstrings: Brief description + Args + Returns + Raises def my_method(self, param1: str, param2: int = 10) -> Dict[str, Any]: \"\"\"Brief description of what this method does. Args: param1: Description of param1 param2: Description of param2 (default: 10) Returns: Dictionary containing: - key1: Description - key2: Description Raises: ValueError: When param1 is empty RuntimeError: When operation fails \"\"\" Type Hints Always use type hints: from typing import Any, Dict, List, Optional, Union, TypeVar, Generic T = TypeVar(\"T\") class GenericComponent(BaseComponent, Generic[T]): def run(self, data: T) -> T: ... def process_items( items: List[Dict[str, Any]], filter_fn: Optional[callable] = None, *, limit: int = 100 ) -> List[Dict[str, Any]]: ... Formatting Tools # Format code with black black rakam_systems/ # Lint with ruff ruff check rakam_systems/ # Type check (optional) mypy rakam_systems/ Publishing & Distribution Version Management Update version in pyproject.toml : [project] name = \"rakam-systems\" version = \"0.2.4\" # Update this Building the Package # Install build tools pip install build twine # Build distribution python -m build # Check distribution twine check dist/* Publishing to PyPI # Upload to TestPyPI first twine upload --repository testpypi dist/* # Test installation from TestPyPI pip install --index-url https://test.pypi.org/simple/ rakam-systems # Upload to PyPI twine upload dist/* Creating a Release Update version in pyproject.toml Update CHANGELOG.md Commit changes: git commit -m \"Release v0.2.4\" Tag release: git tag v0.2.4 Push: git push && git push --tags Build and publish to PyPI Additional Resources Component Documentation : docs/components.md Installation Guide : INSTALLATION.md Example Configurations : rakam_systems/examples/configs/ Agent Examples : rakam_systems/examples/ai_agents_examples/ Vector Store Examples : rakam_systems/examples/ai_vectorstore_examples/ Getting Help GitHub Issues : Report bugs or request features Documentation : Check component-specific READMEs Examples : Explore the examples/ directory for working code Happy Developing! \ud83d\ude80","title":"Development Guid"},{"location":"development_guide/#rakam-systems-development-guide","text":"This guide provides comprehensive instructions for developers who want to contribute to or extend the rakam_systems framework. It covers architecture patterns, component development, testing strategies, and best practices.","title":"Rakam Systems Development Guide"},{"location":"development_guide/#table-of-contents","text":"Getting Started Architecture Overview Core Concepts Developing Components Creating Custom Agents Building Tools Implementing Vector Stores Creating Document Loaders Configuration System Testing Guidelines Code Style & Conventions Publishing & Distribution","title":"\ud83d\udcd1 Table of Contents"},{"location":"development_guide/#getting-started","text":"","title":"Getting Started"},{"location":"development_guide/#development-environment-setup","text":"# Clone the repository git clone <repository_url> cd rakam_systems/app/rakam_systems # Create a virtual environment python -m venv venv source venv/bin/activate # On Windows: venv\\Scripts\\activate # Install with all dependencies including dev tools pip install -e \".[complete]\" # Setup pre-commit hooks pre-commit install # Verify installation python -c \"from rakam_systems.ai_core.base import BaseComponent; print('\u2705 Setup complete!')\"","title":"Development Environment Setup"},{"location":"development_guide/#project-structure","text":"rakam_systems/ \u251c\u2500\u2500 rakam_systems/ # Main package \u2502 \u251c\u2500\u2500 ai_core/ # Core abstractions & interfaces \u2502 \u2502 \u251c\u2500\u2500 base.py # BaseComponent class \u2502 \u2502 \u251c\u2500\u2500 interfaces/ # Abstract interfaces \u2502 \u2502 \u251c\u2500\u2500 config_loader.py # Configuration system \u2502 \u2502 \u251c\u2500\u2500 tracking.py # Input/output tracking \u2502 \u2502 \u2514\u2500\u2500 mcp/ # MCP server support \u2502 \u251c\u2500\u2500 ai_agents/ # Agent implementations \u2502 \u2502 \u251c\u2500\u2500 components/ # Agent components \u2502 \u2502 \u2502 \u251c\u2500\u2500 base_agent.py # BaseAgent implementation \u2502 \u2502 \u2502 \u251c\u2500\u2500 llm_gateway/ # LLM provider gateways \u2502 \u2502 \u2502 \u251c\u2500\u2500 chat_history/ # Chat history backends \u2502 \u2502 \u2502 \u2514\u2500\u2500 tools/ # Built-in tools \u2502 \u2502 \u2514\u2500\u2500 server/ # MCP server for agents \u2502 \u251c\u2500\u2500 ai_vectorstore/ # Vector store implementations \u2502 \u2502 \u251c\u2500\u2500 core.py # Node, VSFile data structures \u2502 \u2502 \u251c\u2500\u2500 config.py # VectorStoreConfig \u2502 \u2502 \u2514\u2500\u2500 components/ # Vector store components \u2502 \u2502 \u251c\u2500\u2500 vectorstore/ # Store implementations \u2502 \u2502 \u251c\u2500\u2500 embedding_model/ # Embedding models \u2502 \u2502 \u251c\u2500\u2500 loader/ # Document loaders \u2502 \u2502 \u2514\u2500\u2500 chunker/ # Text chunkers \u2502 \u251c\u2500\u2500 ai_utils/ # Utilities (logging, metrics) \u2502 \u2514\u2500\u2500 examples/ # Usage examples \u251c\u2500\u2500 tests/ # Test suite \u251c\u2500\u2500 docs/ # Documentation \u251c\u2500\u2500 pyproject.toml # Package configuration \u2514\u2500\u2500 requirements.txt # Locked dependencies","title":"Project Structure"},{"location":"development_guide/#architecture-overview","text":"","title":"Architecture Overview"},{"location":"development_guide/#design-principles","text":"Component-Based Architecture : All functionality is encapsulated in components that extend BaseComponent Interface-Driven Development : Abstract interfaces define contracts; implementations are swappable Configuration-First : YAML/JSON configuration support for all components Provider-Agnostic : Support multiple backends (LLMs, embeddings, vector stores) Lifecycle Management : Components have setup() and shutdown() hooks for resource management","title":"Design Principles"},{"location":"development_guide/#component-hierarchy","text":"BaseComponent (ai_core/base.py) \u251c\u2500\u2500 AgentComponent (ai_core/interfaces/agent.py) \u2502 \u2514\u2500\u2500 BaseAgent (ai_agents/components/base_agent.py) \u251c\u2500\u2500 ToolComponent (ai_core/interfaces/tool.py) \u2502 \u2514\u2500\u2500 FunctionToolComponent \u251c\u2500\u2500 LLMGateway (ai_core/interfaces/llm_gateway.py) \u2502 \u251c\u2500\u2500 OpenAIGateway \u2502 \u2514\u2500\u2500 MistralGateway \u251c\u2500\u2500 VectorStore (ai_core/interfaces/vectorstore.py) \u2502 \u251c\u2500\u2500 ConfigurablePgVectorStore \u2502 \u2514\u2500\u2500 FAISSVectorStore \u251c\u2500\u2500 EmbeddingModel (ai_core/interfaces/embedding_model.py) \u2502 \u2514\u2500\u2500 ConfigurableEmbeddings \u251c\u2500\u2500 Loader (ai_core/interfaces/loader.py) \u2502 \u251c\u2500\u2500 AdaptiveLoader \u2502 \u251c\u2500\u2500 PdfLoader, DocLoader, etc. \u2514\u2500\u2500 Chunker (ai_core/interfaces/chunker.py) \u2514\u2500\u2500 TextChunker","title":"Component Hierarchy"},{"location":"development_guide/#core-concepts","text":"","title":"Core Concepts"},{"location":"development_guide/#basecomponent","text":"Every component in the system extends BaseComponent , which provides: Lifecycle management : setup() and shutdown() methods Auto-initialization : __call__ automatically calls setup() if needed Context manager support : Use with statement for automatic cleanup Evaluation harness : Built-in method for testing components from abc import abstractmethod from typing import Any, Dict, Optional from rakam_systems.ai_core.base import BaseComponent class MyComponent(BaseComponent): \"\"\"Example custom component.\"\"\" def __init__(self, name: str, config: Optional[Dict[str, Any]] = None): super().__init__(name, config) self.heavy_resource = None def setup(self) -> None: \"\"\"Initialize heavy resources.\"\"\" super().setup() # Sets self.initialized = True # Initialize expensive resources here self.heavy_resource = self._load_model() def shutdown(self) -> None: \"\"\"Release resources.\"\"\" if self.heavy_resource: self.heavy_resource = None super().shutdown() # Sets self.initialized = False @abstractmethod def run(self, *args, **kwargs) -> Any: \"\"\"Execute the primary operation.\"\"\" raise NotImplementedError def _load_model(self): \"\"\"Private method to load model.\"\"\" return \"loaded_model\"","title":"BaseComponent"},{"location":"development_guide/#usage-patterns","text":"# Pattern 1: Manual lifecycle component = MyComponent(\"my_component\") component.setup() result = component.run(input_data) component.shutdown() # Pattern 2: Auto-setup via __call__ component = MyComponent(\"my_component\") result = component(input_data) # setup() called automatically # Pattern 3: Context manager (recommended) with MyComponent(\"my_component\") as component: result = component.run(input_data) # shutdown() called automatically","title":"Usage Patterns"},{"location":"development_guide/#interfaces","text":"Interfaces define contracts that implementations must follow. They are located in ai_core/interfaces/ : Interface Purpose Key Methods AgentComponent AI agents run() , arun() , stream() , astream() ToolComponent Callable tools run() , acall() LLMGateway LLM providers generate() , stream() , count_tokens() VectorStore Vector storage add() , query() , count() EmbeddingModel Text embeddings run() , encode_query() , encode_documents() Loader Document loading load_as_text() , load_as_chunks() , load_as_nodes() Chunker Text chunking run() , chunk_text()","title":"Interfaces"},{"location":"development_guide/#developing-components","text":"","title":"Developing Components"},{"location":"development_guide/#step-1-choose-the-right-interface","text":"Determine which interface your component should implement: # For a new LLM provider from rakam_systems.ai_core.interfaces.llm_gateway import LLMGateway # For a new vector store backend from rakam_systems.ai_core.interfaces.vectorstore import VectorStore # For a new document loader from rakam_systems.ai_core.interfaces.loader import Loader # For a custom tool from rakam_systems.ai_core.interfaces.tool import ToolComponent","title":"Step 1: Choose the Right Interface"},{"location":"development_guide/#step-2-implement-required-methods","text":"from typing import List, Dict, Any, Optional from rakam_systems.ai_core.interfaces.vectorstore import VectorStore class MyCustomVectorStore(VectorStore): \"\"\"Custom vector store implementation.\"\"\" def __init__( self, name: str = \"my_vectorstore\", config: Optional[Dict[str, Any]] = None ): super().__init__(name, config) self.index = None self.dimension = config.get(\"dimension\", 384) if config else 384 def setup(self) -> None: \"\"\"Initialize the vector index.\"\"\" super().setup() # Initialize your backend here self.index = self._create_index() def shutdown(self) -> None: \"\"\"Clean up resources.\"\"\" if self.index: self._save_index() self.index = None super().shutdown() def run(self, *args, **kwargs) -> Any: \"\"\"Default run delegates to query.\"\"\" return self.query(*args, **kwargs) def add( self, vectors: List[List[float]], metadatas: List[Dict[str, Any]] ) -> Any: \"\"\"Add vectors to the store.\"\"\" if not self.initialized: self.setup() # Implementation here return {\"added\": len(vectors)} def query( self, vector: List[float], top_k: int = 5 ) -> List[Dict[str, Any]]: \"\"\"Query similar vectors.\"\"\" if not self.initialized: self.setup() # Implementation here return [{\"id\": i, \"score\": 0.9} for i in range(top_k)] def count(self) -> Optional[int]: \"\"\"Return total vector count.\"\"\" return len(self.index) if self.index else 0 def _create_index(self): \"\"\"Private method to create index.\"\"\" return {} def _save_index(self): \"\"\"Private method to save index.\"\"\" pass","title":"Step 2: Implement Required Methods"},{"location":"development_guide/#step-3-add-configuration-support","text":"Support both programmatic and YAML/JSON configuration: from pydantic import BaseModel, Field from typing import Optional import yaml class MyVectorStoreConfig(BaseModel): \"\"\"Configuration for MyCustomVectorStore.\"\"\" name: str = \"my_vectorstore\" dimension: int = Field(default=384, ge=1) backend: str = Field(default=\"memory\", pattern=\"^(memory|disk|redis)$\") cache_size: int = Field(default=1000, ge=0) @classmethod def from_yaml(cls, path: str) -> \"MyVectorStoreConfig\": \"\"\"Load configuration from YAML file.\"\"\" with open(path, 'r') as f: data = yaml.safe_load(f) return cls(**data) def to_dict(self) -> Dict[str, Any]: \"\"\"Convert to dictionary.\"\"\" return self.model_dump() class MyCustomVectorStore(VectorStore): \"\"\"Custom vector store with config support.\"\"\" def __init__( self, config: Optional[MyVectorStoreConfig | Dict | str] = None ): # Handle different config types if isinstance(config, str): config = MyVectorStoreConfig.from_yaml(config) elif isinstance(config, dict): config = MyVectorStoreConfig(**config) elif config is None: config = MyVectorStoreConfig() super().__init__(config.name, config.to_dict()) self.config_obj = config","title":"Step 3: Add Configuration Support"},{"location":"development_guide/#creating-custom-agents","text":"","title":"Creating Custom Agents"},{"location":"development_guide/#basic-agent","text":"import asyncio from typing import Optional, List, Any from pydantic import BaseModel from rakam_systems.ai_agents.components import BaseAgent from rakam_systems.ai_core.interfaces.agent import AgentInput, AgentOutput from rakam_systems.ai_core.interfaces.tool import ToolComponent class MyCustomAgent(BaseAgent): \"\"\"Custom agent with specialized behavior.\"\"\" def __init__( self, name: str = \"custom_agent\", model: str = \"openai:gpt-4o\", system_prompt: str = \"You are a helpful assistant.\", tools: Optional[List[ToolComponent]] = None, output_type: Optional[type[BaseModel]] = None, **kwargs ): super().__init__( name=name, model=model, system_prompt=system_prompt, tools=tools, output_type=output_type, **kwargs ) async def arun( self, input_data: str | AgentInput, deps: Any = None, **kwargs ) -> AgentOutput: \"\"\"Override to add custom preprocessing/postprocessing.\"\"\" # Preprocess if isinstance(input_data, str): input_data = self._preprocess(input_data) # Call parent implementation result = await super().arun(input_data, deps, **kwargs) # Postprocess result = self._postprocess(result) return result def _preprocess(self, text: str) -> str: \"\"\"Add preprocessing logic.\"\"\" # Example: Add context or modify input return f\"[User Query] {text}\" def _postprocess(self, result: AgentOutput) -> AgentOutput: \"\"\"Add postprocessing logic.\"\"\" # Example: Format output return result","title":"Basic Agent"},{"location":"development_guide/#agent-with-structured-output","text":"from pydantic import BaseModel, Field from typing import List class AnalysisResult(BaseModel): \"\"\"Structured output for analysis agent.\"\"\" summary: str = Field(description=\"Brief summary of the analysis\") key_points: List[str] = Field(description=\"Key points extracted\") confidence: float = Field(ge=0.0, le=1.0, description=\"Confidence score\") needs_review: bool = Field(default=False, description=\"Whether human review is needed\") # Create agent with structured output analysis_agent = BaseAgent( name=\"analysis_agent\", model=\"openai:gpt-4o\", system_prompt=\"You analyze text and provide structured analysis.\", output_type=AnalysisResult # Enforces structured output ) # Usage async def analyze(text: str) -> AnalysisResult: result = await analysis_agent.arun(f\"Analyze this: {text}\") return result.output # Typed as AnalysisResult","title":"Agent with Structured Output"},{"location":"development_guide/#agent-with-dynamic-system-prompts","text":"from datetime import date from pydantic_ai import RunContext agent = BaseAgent( name=\"dynamic_agent\", model=\"openai:gpt-4o\", system_prompt=\"You are a helpful assistant.\" ) # Add dynamic prompts that are evaluated at runtime @agent.dynamic_system_prompt def add_date_context() -> str: \"\"\"Add current date to system prompt.\"\"\" return f\"Today's date is {date.today().strftime('%B %d, %Y')}.\" @agent.dynamic_system_prompt def add_user_context(ctx: RunContext[dict]) -> str: \"\"\"Add user-specific context from dependencies.\"\"\" if ctx.deps and \"user_name\" in ctx.deps: return f\"You are assisting {ctx.deps['user_name']}.\" return \"\" # Usage with dependencies result = await agent.arun( \"What day is it?\", deps={\"user_name\": \"Alice\"} )","title":"Agent with Dynamic System Prompts"},{"location":"development_guide/#building-tools","text":"","title":"Building Tools"},{"location":"development_guide/#method-1-class-based-tool","text":"from typing import Any, Dict from rakam_systems.ai_core.interfaces.tool import ToolComponent class WeatherTool(ToolComponent): \"\"\"Tool to get weather information.\"\"\" def __init__(self, api_key: str = None): super().__init__( name=\"get_weather\", description=\"Get current weather for a location\", json_schema={ \"type\": \"object\", \"properties\": { \"city\": { \"type\": \"string\", \"description\": \"City name\" }, \"units\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"default\": \"celsius\" } }, \"required\": [\"city\"], \"additionalProperties\": False } ) self.api_key = api_key def run(self, city: str, units: str = \"celsius\") -> str: \"\"\"Fetch weather data.\"\"\" # Implementation here return f\"Weather in {city}: 22\u00b0{units[0].upper()}, Sunny\"","title":"Method 1: Class-Based Tool"},{"location":"development_guide/#method-2-function-based-tool-recommended-for-simple-tools","text":"from rakam_systems.ai_core.interfaces.tool import ToolComponent def calculate_bmi(weight_kg: float, height_m: float) -> dict: \"\"\"Calculate BMI given weight and height.\"\"\" bmi = weight_kg / (height_m ** 2) category = ( \"Underweight\" if bmi < 18.5 else \"Normal\" if bmi < 25 else \"Overweight\" if bmi < 30 else \"Obese\" ) return {\"bmi\": round(bmi, 2), \"category\": category} # Create tool from function bmi_tool = ToolComponent.from_function( function=calculate_bmi, name=\"calculate_bmi\", description=\"Calculate Body Mass Index from weight and height\", json_schema={ \"type\": \"object\", \"properties\": { \"weight_kg\": { \"type\": \"number\", \"description\": \"Weight in kilograms\" }, \"height_m\": { \"type\": \"number\", \"description\": \"Height in meters\" } }, \"required\": [\"weight_kg\", \"height_m\"], \"additionalProperties\": False } )","title":"Method 2: Function-Based Tool (Recommended for Simple Tools)"},{"location":"development_guide/#method-3-async-tool","text":"import aiohttp from rakam_systems.ai_core.interfaces.tool import ToolComponent class AsyncSearchTool(ToolComponent): \"\"\"Async tool for web search.\"\"\" def __init__(self): super().__init__( name=\"web_search\", description=\"Search the web for information\", json_schema={ \"type\": \"object\", \"properties\": { \"query\": {\"type\": \"string\", \"description\": \"Search query\"} }, \"required\": [\"query\"] } ) async def run(self, query: str) -> str: \"\"\"Perform async web search.\"\"\" async with aiohttp.ClientSession() as session: async with session.get(f\"https://api.search.com?q={query}\") as resp: data = await resp.json() return data.get(\"results\", []) # acall is automatically handled for async run methods","title":"Method 3: Async Tool"},{"location":"development_guide/#registering-tools-with-toolregistry","text":"from rakam_systems.ai_core.interfaces.tool_registry import ToolRegistry, ToolMode registry = ToolRegistry() # Register tools registry.register_direct_tool( name=\"weather\", function=get_weather_fn, description=\"Get weather information\", json_schema={...}, category=\"utility\", tags=[\"weather\", \"external\"] ) registry.register_direct_tool( name=\"calculator\", function=calculate_fn, description=\"Perform calculations\", json_schema={...}, category=\"math\", tags=[\"calculation\"] ) # Query tools math_tools = registry.get_tools_by_category(\"math\") external_tools = registry.get_tools_by_tag(\"external\") all_direct_tools = registry.get_tools_by_mode(ToolMode.DIRECT)","title":"Registering Tools with ToolRegistry"},{"location":"development_guide/#implementing-vector-stores","text":"","title":"Implementing Vector Stores"},{"location":"development_guide/#custom-vector-store","text":"from typing import List, Dict, Any, Optional from rakam_systems.ai_core.interfaces.vectorstore import VectorStore from rakam_systems.ai_vectorstore.core import Node, NodeMetadata class RedisVectorStore(VectorStore): \"\"\"Redis-backed vector store implementation.\"\"\" def __init__( self, name: str = \"redis_vectorstore\", host: str = \"localhost\", port: int = 6379, index_name: str = \"vectors\", dimension: int = 384, config: Optional[Dict] = None ): super().__init__(name, config or {}) self.host = host self.port = port self.index_name = index_name self.dimension = dimension self.client = None def setup(self) -> None: \"\"\"Connect to Redis and create index.\"\"\" super().setup() import redis from redis.commands.search.field import VectorField, TextField from redis.commands.search.indexDefinition import IndexDefinition, IndexType self.client = redis.Redis(host=self.host, port=self.port) # Create vector index try: self.client.ft(self.index_name).create_index( fields=[ VectorField(\"embedding\", \"HNSW\", { \"TYPE\": \"FLOAT32\", \"DIM\": self.dimension, \"DISTANCE_METRIC\": \"COSINE\" }), TextField(\"content\"), ], definition=IndexDefinition( prefix=[f\"{self.index_name}:\"], index_type=IndexType.HASH ) ) except Exception: pass # Index already exists def shutdown(self) -> None: \"\"\"Close Redis connection.\"\"\" if self.client: self.client.close() self.client = None super().shutdown() def run(self, *args, **kwargs) -> Any: return self.query(*args, **kwargs) def add( self, vectors: List[List[float]], metadatas: List[Dict[str, Any]] ) -> Dict[str, Any]: \"\"\"Add vectors to Redis.\"\"\" if not self.initialized: self.setup() import numpy as np added = 0 for i, (vector, metadata) in enumerate(zip(vectors, metadatas)): doc_id = metadata.get(\"id\", f\"doc_{i}\") self.client.hset( f\"{self.index_name}:{doc_id}\", mapping={ \"embedding\": np.array(vector, dtype=np.float32).tobytes(), \"content\": metadata.get(\"content\", \"\"), **metadata } ) added += 1 return {\"added\": added} def query( self, vector: List[float], top_k: int = 5 ) -> List[Dict[str, Any]]: \"\"\"Search for similar vectors.\"\"\" if not self.initialized: self.setup() import numpy as np from redis.commands.search.query import Query query_vector = np.array(vector, dtype=np.float32).tobytes() q = ( Query(f\"*=>[KNN {top_k} @embedding $vec AS score]\") .sort_by(\"score\") .return_fields(\"content\", \"score\") .dialect(2) ) results = self.client.ft(self.index_name).search( q, query_params={\"vec\": query_vector} ) return [ { \"id\": doc.id, \"content\": doc.content, \"score\": float(doc.score) } for doc in results.docs ] def count(self) -> Optional[int]: \"\"\"Count total documents.\"\"\" if not self.client: return 0 info = self.client.ft(self.index_name).info() return int(info.get(\"num_docs\", 0))","title":"Custom Vector Store"},{"location":"development_guide/#creating-document-loaders","text":"","title":"Creating Document Loaders"},{"location":"development_guide/#custom-loader","text":"from typing import List, Union, Optional, Dict, Any from pathlib import Path from rakam_systems.ai_core.interfaces.loader import Loader from rakam_systems.ai_vectorstore.core import Node, NodeMetadata, VSFile class XMLLoader(Loader): \"\"\"Loader for XML documents.\"\"\" def __init__( self, name: str = \"xml_loader\", config: Optional[Dict[str, Any]] = None ): super().__init__(name, config or {}) self.chunk_size = self.config.get(\"chunk_size\", 512) self.encoding = self.config.get(\"encoding\", \"utf-8\") def run(self, source: Union[str, Path]) -> List[str]: \"\"\"Default run delegates to load_as_chunks.\"\"\" return self.load_as_chunks(source) def load_as_text(self, source: Union[str, Path]) -> str: \"\"\"Load XML as plain text (elements removed).\"\"\" import xml.etree.ElementTree as ET tree = ET.parse(source) root = tree.getroot() # Extract all text content texts = [] for elem in root.iter(): if elem.text: texts.append(elem.text.strip()) if elem.tail: texts.append(elem.tail.strip()) return \" \".join(filter(None, texts)) def load_as_chunks(self, source: Union[str, Path]) -> List[str]: \"\"\"Load XML and split into chunks.\"\"\" text = self.load_as_text(source) # Simple chunking by character count chunks = [] for i in range(0, len(text), self.chunk_size): chunk = text[i:i + self.chunk_size] if chunk.strip(): chunks.append(chunk) return chunks def load_as_nodes( self, source: Union[str, Path], source_id: Optional[str] = None, custom_metadata: Optional[Dict[str, Any]] = None ) -> List[Node]: \"\"\"Load XML as nodes with metadata.\"\"\" source_path = Path(source) chunks = self.load_as_chunks(source) nodes = [] for i, chunk in enumerate(chunks): metadata = NodeMetadata( source_file_uuid=source_id or str(source_path), position=i, custom=custom_metadata or {} ) nodes.append(Node(content=chunk, metadata=metadata)) return nodes def load_as_vsfile( self, file_path: Union[str, Path], custom_metadata: Optional[Dict[str, Any]] = None ) -> VSFile: \"\"\"Load XML as VSFile with nodes attached.\"\"\" vsfile = VSFile(file_path=str(file_path)) vsfile.nodes = self.load_as_nodes( file_path, source_id=str(vsfile.uuid), custom_metadata=custom_metadata ) return vsfile","title":"Custom Loader"},{"location":"development_guide/#integrating-with-adaptiveloader","text":"To make your loader work with AdaptiveLoader , register the file extension: from rakam_systems.ai_vectorstore.components.loader import AdaptiveLoader class EnhancedAdaptiveLoader(AdaptiveLoader): \"\"\"AdaptiveLoader with XML support.\"\"\" EXTENSION_LOADERS = { **AdaptiveLoader.EXTENSION_LOADERS, \".xml\": XMLLoader, \".xhtml\": XMLLoader, }","title":"Integrating with AdaptiveLoader"},{"location":"development_guide/#configuration-system","text":"","title":"Configuration System"},{"location":"development_guide/#yaml-configuration","text":"The framework supports comprehensive YAML configuration: # config/agent_config.yaml version: \"1.0\" # Prompt library prompts: assistant: name: \"assistant\" system_prompt: | You are a helpful AI assistant. Always be accurate and helpful. skills: - \"Information retrieval\" - \"Task automation\" # Tool library tools: search: name: \"search\" type: \"direct\" module: \"myapp.tools\" function: \"search_documents\" description: \"Search documents\" json_schema: type: \"object\" properties: query: type: \"string\" required: [\"query\"] # Agent configurations agents: main_agent: name: \"main_agent\" model_config: model: \"openai:gpt-4o\" temperature: 0.7 max_tokens: 2000 prompt_config: \"assistant\" tools: - \"search\" enable_tracking: true","title":"YAML Configuration"},{"location":"development_guide/#loading-configuration","text":"from rakam_systems.ai_core.config_loader import ConfigurationLoader loader = ConfigurationLoader() # Load from YAML config = loader.load_from_yaml(\"config/agent_config.yaml\") # Create single agent agent = loader.create_agent(\"main_agent\", config) # Create all agents all_agents = loader.create_all_agents(config) # Get tool registry registry = loader.get_tool_registry(config) # Validate configuration is_valid, errors = loader.validate_config(\"config/agent_config.yaml\") if not is_valid: for error in errors: print(f\"Config error: {error}\")","title":"Loading Configuration"},{"location":"development_guide/#testing-guidelines","text":"","title":"Testing Guidelines"},{"location":"development_guide/#unit-test-structure","text":"# tests/test_my_component.py import pytest from typing import List, Dict, Any from rakam_systems.ai_core.interfaces.vectorstore import VectorStore class DummyVectorStore(VectorStore): \"\"\"Mock implementation for testing.\"\"\" def __init__(self): super().__init__(\"dummy_store\") self.data = [] def run(self, *args, **kwargs): return self.query(*args, **kwargs) def add(self, vectors: List[List[float]], metadatas: List[Dict]) -> Dict: self.data.extend(zip(vectors, metadatas)) return {\"added\": len(vectors)} def query(self, vector: List[float], top_k: int = 5) -> List[Dict]: return [{\"id\": i, \"score\": 0.9} for i in range(min(top_k, len(self.data)))] def count(self) -> int: return len(self.data) class TestVectorStore: \"\"\"Test suite for VectorStore implementations.\"\"\" @pytest.fixture def store(self): \"\"\"Create a fresh store for each test.\"\"\" store = DummyVectorStore() yield store store.shutdown() def test_add_vectors(self, store): \"\"\"Test adding vectors to store.\"\"\" vectors = [[0.1, 0.2], [0.3, 0.4]] metadatas = [{\"id\": 1}, {\"id\": 2}] result = store.add(vectors, metadatas) assert result[\"added\"] == 2 assert store.count() == 2 def test_query_vectors(self, store): \"\"\"Test querying vectors.\"\"\" store.add([[0.1, 0.2]], [{\"id\": 1}]) results = store.query([0.1, 0.2], top_k=5) assert len(results) <= 5 assert all(\"score\" in r for r in results) def test_lifecycle(self, store): \"\"\"Test setup/shutdown lifecycle.\"\"\" assert not store.initialized store.setup() assert store.initialized store.shutdown() assert not store.initialized def test_evaluate_harness(self, store): \"\"\"Test built-in evaluation harness.\"\"\" store.add([[0.1]], [{\"id\": 1}]) test_cases = { \"query\": [ {\"args\": [[0.1, 0.2]], \"kwargs\": {\"top_k\": 3}} ] } results = store.evaluate( methods=[\"query\"], test_cases=test_cases, verbose=False ) assert \"query\" in results assert results[\"query\"][0][\"success\"]","title":"Unit Test Structure"},{"location":"development_guide/#async-test-structure","text":"import pytest import asyncio @pytest.fixture def event_loop(): \"\"\"Create event loop for async tests.\"\"\" loop = asyncio.new_event_loop() yield loop loop.close() @pytest.mark.asyncio async def test_async_agent(): \"\"\"Test async agent methods.\"\"\" from rakam_systems.ai_agents.components import BaseAgent agent = BaseAgent( name=\"test_agent\", model=\"openai:gpt-4o-mini\", system_prompt=\"You are a test assistant.\" ) result = await agent.arun(\"Say 'hello'\") assert result.output_text is not None assert len(result.output_text) > 0","title":"Async Test Structure"},{"location":"development_guide/#running-tests","text":"# Run all tests pytest # Run with coverage pytest --cov=rakam_systems --cov-report=html # Run specific test file pytest tests/test_my_component.py # Run tests matching pattern pytest -k \"test_vector\" # Run async tests pytest -v tests/test_async_*.py","title":"Running Tests"},{"location":"development_guide/#code-style-conventions","text":"","title":"Code Style &amp; Conventions"},{"location":"development_guide/#python-style","text":"Follow PEP 8 with these project-specific conventions: # Imports: Standard library, third-party, local (separated by blank lines) from __future__ import annotations import asyncio from typing import Any, Dict, List, Optional import numpy as np from pydantic import BaseModel from rakam_systems.ai_core.base import BaseComponent # Class docstrings: Brief description + Attributes + Methods class MyComponent(BaseComponent): \"\"\"Brief description of the component. This component does X, Y, and Z. Use it when you need to... Attributes: name: Component identifier config: Configuration dictionary some_param: Description of parameter Example: ```python component = MyComponent(\"my_comp\", config={\"key\": \"value\"}) with component: result = component.run(input_data) ``` \"\"\" # Method docstrings: Brief description + Args + Returns + Raises def my_method(self, param1: str, param2: int = 10) -> Dict[str, Any]: \"\"\"Brief description of what this method does. Args: param1: Description of param1 param2: Description of param2 (default: 10) Returns: Dictionary containing: - key1: Description - key2: Description Raises: ValueError: When param1 is empty RuntimeError: When operation fails \"\"\"","title":"Python Style"},{"location":"development_guide/#type-hints","text":"Always use type hints: from typing import Any, Dict, List, Optional, Union, TypeVar, Generic T = TypeVar(\"T\") class GenericComponent(BaseComponent, Generic[T]): def run(self, data: T) -> T: ... def process_items( items: List[Dict[str, Any]], filter_fn: Optional[callable] = None, *, limit: int = 100 ) -> List[Dict[str, Any]]: ...","title":"Type Hints"},{"location":"development_guide/#formatting-tools","text":"# Format code with black black rakam_systems/ # Lint with ruff ruff check rakam_systems/ # Type check (optional) mypy rakam_systems/","title":"Formatting Tools"},{"location":"development_guide/#publishing-distribution","text":"","title":"Publishing &amp; Distribution"},{"location":"development_guide/#version-management","text":"Update version in pyproject.toml : [project] name = \"rakam-systems\" version = \"0.2.4\" # Update this","title":"Version Management"},{"location":"development_guide/#building-the-package","text":"# Install build tools pip install build twine # Build distribution python -m build # Check distribution twine check dist/*","title":"Building the Package"},{"location":"development_guide/#publishing-to-pypi","text":"# Upload to TestPyPI first twine upload --repository testpypi dist/* # Test installation from TestPyPI pip install --index-url https://test.pypi.org/simple/ rakam-systems # Upload to PyPI twine upload dist/*","title":"Publishing to PyPI"},{"location":"development_guide/#creating-a-release","text":"Update version in pyproject.toml Update CHANGELOG.md Commit changes: git commit -m \"Release v0.2.4\" Tag release: git tag v0.2.4 Push: git push && git push --tags Build and publish to PyPI","title":"Creating a Release"},{"location":"development_guide/#additional-resources","text":"Component Documentation : docs/components.md Installation Guide : INSTALLATION.md Example Configurations : rakam_systems/examples/configs/ Agent Examples : rakam_systems/examples/ai_agents_examples/ Vector Store Examples : rakam_systems/examples/ai_vectorstore_examples/","title":"Additional Resources"},{"location":"development_guide/#getting-help","text":"GitHub Issues : Report bugs or request features Documentation : Check component-specific READMEs Examples : Explore the examples/ directory for working code Happy Developing! \ud83d\ude80","title":"Getting Help"},{"location":"gradio/","text":"","title":"Gradio"},{"location":"installation/","text":"Rakam Systems Installation Guide Complete installation instructions for the rakam_systems modular AI framework. \ud83d\udcd1 Table of Contents Quick Start System Requirements Modular Installation Installation Recipes Environment Setup Verification Docker Setup Troubleshooting Quick Start Option 1: Install Everything # Navigate to the package directory cd app/rakam_systems # Install all features pip install -e \".[all]\" Option 2: Install Specific Modules # AI Agents only pip install -e \".[ai-agents]\" # Vector Store only pip install -e \".[ai-vectorstore]\" # LLM Gateway only pip install -e \".[llm-gateway]\" # Multiple modules pip install -e \".[ai-agents,ai-vectorstore]\" Option 3: Development Setup # Full installation with dev tools pip install -e \".[complete]\" # Setup pre-commit hooks pre-commit install System Requirements Minimum Requirements Requirement Minimum Recommended Python 3.10 3.11+ RAM 4 GB 8 GB+ Disk Space 1 GB 5 GB+ OS Linux, macOS, Windows Linux, macOS Module-Specific Requirements AI Vectorstore (with local embeddings) RAM : 8 GB+ (embedding models are loaded into memory) Disk : 5 GB+ (for downloading model weights) GPU : Optional but recommended for faster inference PostgreSQL : 12+ with pgvector extension (for persistent storage) AI Agents RAM : 4 GB minimum Network : Internet access for LLM API calls API Keys : OpenAI and/or Mistral API keys Modular Installation Rakam Systems uses a modular architecture. Install only what you need: Core Package (Minimal) pip install -e . Includes: - pydantic - Data validation - pyyaml - YAML configuration - python-dotenv - Environment variables - colorlog - Logging - requests - HTTP client AI Vectorstore pip install -e \".[ai-vectorstore]\" Includes: Package Purpose sentence-transformers Local embedding models faiss-cpu Vector similarity search psycopg2-binary PostgreSQL driver pgvector PostgreSQL vector extension django ORM for database operations torch Deep learning backend pymupdf PDF processing python-docx DOCX processing beautifulsoup4 HTML parsing chonkie Text chunking docling Advanced document processing Use Cases: - Semantic search applications - RAG (Retrieval-Augmented Generation) - Document Q&A systems - Knowledge base management AI Agents pip install -e \".[ai-agents]\" Includes: Package Purpose pydantic-ai Agent framework mistralai Mistral AI provider tiktoken Token counting Use Cases: - Building AI agents with tools - Multi-step reasoning systems - Conversational AI - Structured output generation LLM Gateway pip install -e \".[llm-gateway]\" Includes: Package Purpose openai OpenAI API client mistralai Mistral AI client tiktoken Token counting Use Cases: - Multi-provider LLM abstraction - Chat interfaces - Text generation - Structured output Document Loaders pip install -e \".[loaders]\" Includes: Package Purpose beautifulsoup4 HTML parsing python-docx Word documents pymupdf PDF documents python-magic File type detection playwright Web scraping odfpy ODT files openpyxl Excel files docling Advanced document parsing docling-core Core document processing docling-ibm-models IBM document models docling-parse Document parsing engine Development Tools pip install -e \".[dev]\" Includes: Package Purpose pytest Testing framework pytest-asyncio Async test support black Code formatting ruff Linting pre-commit Git hooks Combining Modules Install multiple modules in a single command: # Agents + Vector Store pip install -e \".[ai-agents,ai-vectorstore]\" # Everything except dev tools pip install -e \".[all]\" # Everything including dev tools pip install -e \".[complete]\" Installation Recipes Recipe 1: RAG Application Build a Retrieval-Augmented Generation system: pip install -e \".[ai-vectorstore,ai-agents]\" This gives you: - Document loading and chunking - Vector storage (FAISS or PostgreSQL) - Embedding models - AI agents for query processing Recipe 2: Simple Chatbot Build a conversational AI without vector storage: pip install -e \".[ai-agents]\" Recipe 3: Document Processing Pipeline Process and index documents without AI agents: pip install -e \".[ai-vectorstore]\" Recipe 4: LLM Abstraction Layer Use multiple LLM providers with a unified interface: pip install -e \".[llm-gateway]\" Recipe 5: Full Development Environment Complete setup for contributing: pip install -e \".[complete]\" pre-commit install Environment Setup API Keys Create a .env file in your project root: # OpenAI (for GPT models and embeddings) OPENAI_API_KEY=sk-your-openai-key # Mistral AI (for Mistral models) MISTRAL_API_KEY=your-mistral-key # Cohere (for Cohere embeddings) COHERE_API_KEY=your-cohere-key Load in your code: from dotenv import load_dotenv load_dotenv() PostgreSQL with pgvector Option 1: Docker (Recommended) docker run -d \\ --name postgres-vectorstore \\ -e POSTGRES_PASSWORD=postgres \\ -e POSTGRES_DB=vectorstore_db \\ -p 5432:5432 \\ pgvector/pgvector:pg16 Option 2: Docker Compose Create docker-compose.yml : version: '3.8' services: postgres: image: pgvector/pgvector:pg16 environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: postgres POSTGRES_DB: vectorstore_db ports: - \"5432:5432\" volumes: - postgres_data:/var/lib/postgresql/data volumes: postgres_data: Run: docker compose up -d Environment Variables for PostgreSQL export POSTGRES_HOST=localhost export POSTGRES_PORT=5432 export POSTGRES_DB=vectorstore_db export POSTGRES_USER=postgres export POSTGRES_PASSWORD=postgres Django Configuration For PostgreSQL-backed vector stores, configure Django: import os import django from django.conf import settings if not settings.configured: settings.configure( INSTALLED_APPS=[ 'django.contrib.contenttypes', 'rakam_systems.ai_vectorstore.components.vectorstore', ], DATABASES={ 'default': { 'ENGINE': 'django.db.backends.postgresql', 'NAME': os.getenv('POSTGRES_DB', 'vectorstore_db'), 'USER': os.getenv('POSTGRES_USER', 'postgres'), 'PASSWORD': os.getenv('POSTGRES_PASSWORD', 'postgres'), 'HOST': os.getenv('POSTGRES_HOST', 'localhost'), 'PORT': os.getenv('POSTGRES_PORT', '5432'), } }, DEFAULT_AUTO_FIELD='django.db.models.BigAutoField', ) django.setup() Verification Verify Core Installation # Test core imports from rakam_systems.ai_core.base import BaseComponent from rakam_systems.ai_core.interfaces import ToolComponent, VectorStore print(\"\u2705 Core installed successfully!\") Verify AI Agents try: from rakam_systems.ai_agents import BaseAgent from rakam_systems.ai_core.interfaces.agent import AgentInput, AgentOutput print(\"\u2705 AI Agents installed successfully!\") except ImportError as e: print(f\"\u274c AI Agents not installed: {e}\") Verify AI Vectorstore try: from rakam_systems.ai_vectorstore import ( VectorStoreConfig, Node, VSFile, ConfigurableEmbeddings ) print(\"\u2705 AI Vectorstore installed successfully!\") except ImportError as e: print(f\"\u274c AI Vectorstore not installed: {e}\") Verify LLM Gateway try: from rakam_systems.ai_agents.components.llm_gateway import ( OpenAIGateway, MistralGateway, LLMGatewayFactory ) print(\"\u2705 LLM Gateway installed successfully!\") except ImportError as e: print(f\"\u274c LLM Gateway not installed: {e}\") Full Verification Script #!/usr/bin/env python3 \"\"\"Verify rakam_systems installation.\"\"\" def check_module(name: str, import_fn): \"\"\"Check if a module is properly installed.\"\"\" try: import_fn() print(f\"\u2705 {name}\") return True except ImportError as e: print(f\"\u274c {name}: {e}\") return False def main(): print(\"=\" * 50) print(\"Rakam Systems Installation Verification\") print(\"=\" * 50) results = [] # Core (always required) results.append(check_module( \"Core\", lambda: __import__('rakam_systems.ai_core.base', fromlist=['BaseComponent']) )) # AI Agents results.append(check_module( \"AI Agents\", lambda: __import__('rakam_systems.ai_agents', fromlist=['BaseAgent']) )) # AI Vectorstore results.append(check_module( \"AI Vectorstore\", lambda: __import__('rakam_systems.ai_vectorstore', fromlist=['VectorStoreConfig']) )) # LLM Gateway results.append(check_module( \"LLM Gateway\", lambda: __import__('rakam_systems.ai_agents.components.llm_gateway', fromlist=['OpenAIGateway']) )) print(\"=\" * 50) passed = sum(results) total = len(results) print(f\"Result: {passed}/{total} modules installed\") if passed == total: print(\"\ud83c\udf89 All modules installed successfully!\") else: print(\"\u26a0\ufe0f Some modules are missing. Install them with:\") print(' pip install -e \".[all]\"') if __name__ == \"__main__\": main() Docker Setup Development with Docker Create a Dockerfile : FROM python:3.11-slim WORKDIR /app # Install system dependencies RUN apt-get update && apt-get install -y \\ libmagic1 \\ libpq-dev \\ gcc \\ && rm -rf /var/lib/apt/lists/* # Copy package files COPY app/rakam_systems /app/rakam_systems # Install the package WORKDIR /app/rakam_systems RUN pip install -e \".[all]\" # Set working directory back WORKDIR /app # Default command CMD [\"python\"] Docker Compose for Full Stack version: '3.8' services: app: build: . volumes: - .:/app environment: - OPENAI_API_KEY=${OPENAI_API_KEY} - POSTGRES_HOST=postgres - POSTGRES_PORT=5432 - POSTGRES_DB=vectorstore_db - POSTGRES_USER=postgres - POSTGRES_PASSWORD=postgres depends_on: - postgres postgres: image: pgvector/pgvector:pg16 environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: postgres POSTGRES_DB: vectorstore_db ports: - \"5432:5432\" volumes: - postgres_data:/var/lib/postgresql/data volumes: postgres_data: Troubleshooting Common Issues 1. ModuleNotFoundError ModuleNotFoundError: No module named 'rakam_systems' Solution: Install from the correct directory: cd app/rakam_systems pip install -e . 2. Missing Optional Dependencies ImportError: cannot import name 'BaseAgent' from 'rakam_systems.ai_agents' Solution: Install the required module: pip install -e \".[ai-agents]\" 3. Django Not Configured django.core.exceptions.ImproperlyConfigured: Requested setting INSTALLED_APPS... Solution: Configure Django before importing Django-dependent components: import django from django.conf import settings settings.configure( INSTALLED_APPS=['rakam_systems.ai_vectorstore.components.vectorstore'], DATABASES={'default': {...}} ) django.setup() # Now import Django-dependent components from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore 4. PyTorch Installation Issues PyTorch is large (~2GB). For CPU-only installation: pip install torch --index-url https://download.pytorch.org/whl/cpu pip install -e \".[ai-vectorstore]\" For CUDA support: pip install torch --index-url https://download.pytorch.org/whl/cu118 pip install -e \".[ai-vectorstore]\" 5. FAISS GPU Support Replace CPU version with GPU version: pip uninstall faiss-cpu pip install faiss-gpu 6. libmagic Not Found On macOS: brew install libmagic On Ubuntu/Debian: apt-get install libmagic1 On Windows: pip install python-magic-bin 7. PostgreSQL Connection Refused Ensure PostgreSQL is running: # Check if running docker ps | grep postgres # Start if not running docker start postgres-vectorstore # Or start with docker compose docker compose up -d postgres 8. Permission Errors Use a virtual environment: python -m venv venv source venv/bin/activate # Linux/macOS # or .\\venv\\Scripts\\activate # Windows pip install -e \".[all]\" Getting Help Documentation : See docs/ directory Examples : Check rakam_systems/examples/ Issues : GitHub Issues Upgrading Upgrade to Latest Version cd app/rakam_systems git pull pip install -e \".[all]\" --upgrade Regenerate Locked Dependencies pip install -e \".[complete]\" pip freeze > requirements.txt Uninstallation pip uninstall rakam-systems To remove all dependencies: pip uninstall rakam-systems pip freeze | xargs pip uninstall -y # Be careful: removes ALL packages Next Steps After installation: Read the Documentation Components Guide Development Guide Explore Examples rakam_systems/examples/ai_agents_examples/ rakam_systems/examples/ai_vectorstore_examples/ Try Quick Start Code import asyncio from rakam_systems.ai_agents import BaseAgent async def main(): agent = BaseAgent( name=\"my_agent\", model=\"openai:gpt-4o\", system_prompt=\"You are a helpful assistant.\" ) result = await agent.arun(\"Hello, world!\") print(result.output_text) asyncio.run(main()) Summary of Commands # Core only pip install -e . # AI Agents pip install -e \".[ai-agents]\" # AI Vectorstore pip install -e \".[ai-vectorstore]\" # LLM Gateway pip install -e \".[llm-gateway]\" # Document Loaders pip install -e \".[loaders]\" # All features pip install -e \".[all]\" # All features + dev tools pip install -e \".[complete]\" # Custom combination pip install -e \".[ai-agents,ai-vectorstore,dev]\" License: Apache 2.0 Support: GitHub Repository","title":"Installation"},{"location":"installation/#rakam-systems-installation-guide","text":"Complete installation instructions for the rakam_systems modular AI framework.","title":"Rakam Systems Installation Guide"},{"location":"installation/#table-of-contents","text":"Quick Start System Requirements Modular Installation Installation Recipes Environment Setup Verification Docker Setup Troubleshooting","title":"\ud83d\udcd1 Table of Contents"},{"location":"installation/#quick-start","text":"","title":"Quick Start"},{"location":"installation/#option-1-install-everything","text":"# Navigate to the package directory cd app/rakam_systems # Install all features pip install -e \".[all]\"","title":"Option 1: Install Everything"},{"location":"installation/#option-2-install-specific-modules","text":"# AI Agents only pip install -e \".[ai-agents]\" # Vector Store only pip install -e \".[ai-vectorstore]\" # LLM Gateway only pip install -e \".[llm-gateway]\" # Multiple modules pip install -e \".[ai-agents,ai-vectorstore]\"","title":"Option 2: Install Specific Modules"},{"location":"installation/#option-3-development-setup","text":"# Full installation with dev tools pip install -e \".[complete]\" # Setup pre-commit hooks pre-commit install","title":"Option 3: Development Setup"},{"location":"installation/#system-requirements","text":"","title":"System Requirements"},{"location":"installation/#minimum-requirements","text":"Requirement Minimum Recommended Python 3.10 3.11+ RAM 4 GB 8 GB+ Disk Space 1 GB 5 GB+ OS Linux, macOS, Windows Linux, macOS","title":"Minimum Requirements"},{"location":"installation/#module-specific-requirements","text":"","title":"Module-Specific Requirements"},{"location":"installation/#ai-vectorstore-with-local-embeddings","text":"RAM : 8 GB+ (embedding models are loaded into memory) Disk : 5 GB+ (for downloading model weights) GPU : Optional but recommended for faster inference PostgreSQL : 12+ with pgvector extension (for persistent storage)","title":"AI Vectorstore (with local embeddings)"},{"location":"installation/#ai-agents","text":"RAM : 4 GB minimum Network : Internet access for LLM API calls API Keys : OpenAI and/or Mistral API keys","title":"AI Agents"},{"location":"installation/#modular-installation","text":"Rakam Systems uses a modular architecture. Install only what you need:","title":"Modular Installation"},{"location":"installation/#core-package-minimal","text":"pip install -e . Includes: - pydantic - Data validation - pyyaml - YAML configuration - python-dotenv - Environment variables - colorlog - Logging - requests - HTTP client","title":"Core Package (Minimal)"},{"location":"installation/#ai-vectorstore","text":"pip install -e \".[ai-vectorstore]\" Includes: Package Purpose sentence-transformers Local embedding models faiss-cpu Vector similarity search psycopg2-binary PostgreSQL driver pgvector PostgreSQL vector extension django ORM for database operations torch Deep learning backend pymupdf PDF processing python-docx DOCX processing beautifulsoup4 HTML parsing chonkie Text chunking docling Advanced document processing Use Cases: - Semantic search applications - RAG (Retrieval-Augmented Generation) - Document Q&A systems - Knowledge base management","title":"AI Vectorstore"},{"location":"installation/#ai-agents_1","text":"pip install -e \".[ai-agents]\" Includes: Package Purpose pydantic-ai Agent framework mistralai Mistral AI provider tiktoken Token counting Use Cases: - Building AI agents with tools - Multi-step reasoning systems - Conversational AI - Structured output generation","title":"AI Agents"},{"location":"installation/#llm-gateway","text":"pip install -e \".[llm-gateway]\" Includes: Package Purpose openai OpenAI API client mistralai Mistral AI client tiktoken Token counting Use Cases: - Multi-provider LLM abstraction - Chat interfaces - Text generation - Structured output","title":"LLM Gateway"},{"location":"installation/#document-loaders","text":"pip install -e \".[loaders]\" Includes: Package Purpose beautifulsoup4 HTML parsing python-docx Word documents pymupdf PDF documents python-magic File type detection playwright Web scraping odfpy ODT files openpyxl Excel files docling Advanced document parsing docling-core Core document processing docling-ibm-models IBM document models docling-parse Document parsing engine","title":"Document Loaders"},{"location":"installation/#development-tools","text":"pip install -e \".[dev]\" Includes: Package Purpose pytest Testing framework pytest-asyncio Async test support black Code formatting ruff Linting pre-commit Git hooks","title":"Development Tools"},{"location":"installation/#combining-modules","text":"Install multiple modules in a single command: # Agents + Vector Store pip install -e \".[ai-agents,ai-vectorstore]\" # Everything except dev tools pip install -e \".[all]\" # Everything including dev tools pip install -e \".[complete]\"","title":"Combining Modules"},{"location":"installation/#installation-recipes","text":"","title":"Installation Recipes"},{"location":"installation/#recipe-1-rag-application","text":"Build a Retrieval-Augmented Generation system: pip install -e \".[ai-vectorstore,ai-agents]\" This gives you: - Document loading and chunking - Vector storage (FAISS or PostgreSQL) - Embedding models - AI agents for query processing","title":"Recipe 1: RAG Application"},{"location":"installation/#recipe-2-simple-chatbot","text":"Build a conversational AI without vector storage: pip install -e \".[ai-agents]\"","title":"Recipe 2: Simple Chatbot"},{"location":"installation/#recipe-3-document-processing-pipeline","text":"Process and index documents without AI agents: pip install -e \".[ai-vectorstore]\"","title":"Recipe 3: Document Processing Pipeline"},{"location":"installation/#recipe-4-llm-abstraction-layer","text":"Use multiple LLM providers with a unified interface: pip install -e \".[llm-gateway]\"","title":"Recipe 4: LLM Abstraction Layer"},{"location":"installation/#recipe-5-full-development-environment","text":"Complete setup for contributing: pip install -e \".[complete]\" pre-commit install","title":"Recipe 5: Full Development Environment"},{"location":"installation/#environment-setup","text":"","title":"Environment Setup"},{"location":"installation/#api-keys","text":"Create a .env file in your project root: # OpenAI (for GPT models and embeddings) OPENAI_API_KEY=sk-your-openai-key # Mistral AI (for Mistral models) MISTRAL_API_KEY=your-mistral-key # Cohere (for Cohere embeddings) COHERE_API_KEY=your-cohere-key Load in your code: from dotenv import load_dotenv load_dotenv()","title":"API Keys"},{"location":"installation/#postgresql-with-pgvector","text":"","title":"PostgreSQL with pgvector"},{"location":"installation/#option-1-docker-recommended","text":"docker run -d \\ --name postgres-vectorstore \\ -e POSTGRES_PASSWORD=postgres \\ -e POSTGRES_DB=vectorstore_db \\ -p 5432:5432 \\ pgvector/pgvector:pg16","title":"Option 1: Docker (Recommended)"},{"location":"installation/#option-2-docker-compose","text":"Create docker-compose.yml : version: '3.8' services: postgres: image: pgvector/pgvector:pg16 environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: postgres POSTGRES_DB: vectorstore_db ports: - \"5432:5432\" volumes: - postgres_data:/var/lib/postgresql/data volumes: postgres_data: Run: docker compose up -d","title":"Option 2: Docker Compose"},{"location":"installation/#environment-variables-for-postgresql","text":"export POSTGRES_HOST=localhost export POSTGRES_PORT=5432 export POSTGRES_DB=vectorstore_db export POSTGRES_USER=postgres export POSTGRES_PASSWORD=postgres","title":"Environment Variables for PostgreSQL"},{"location":"installation/#django-configuration","text":"For PostgreSQL-backed vector stores, configure Django: import os import django from django.conf import settings if not settings.configured: settings.configure( INSTALLED_APPS=[ 'django.contrib.contenttypes', 'rakam_systems.ai_vectorstore.components.vectorstore', ], DATABASES={ 'default': { 'ENGINE': 'django.db.backends.postgresql', 'NAME': os.getenv('POSTGRES_DB', 'vectorstore_db'), 'USER': os.getenv('POSTGRES_USER', 'postgres'), 'PASSWORD': os.getenv('POSTGRES_PASSWORD', 'postgres'), 'HOST': os.getenv('POSTGRES_HOST', 'localhost'), 'PORT': os.getenv('POSTGRES_PORT', '5432'), } }, DEFAULT_AUTO_FIELD='django.db.models.BigAutoField', ) django.setup()","title":"Django Configuration"},{"location":"installation/#verification","text":"","title":"Verification"},{"location":"installation/#verify-core-installation","text":"# Test core imports from rakam_systems.ai_core.base import BaseComponent from rakam_systems.ai_core.interfaces import ToolComponent, VectorStore print(\"\u2705 Core installed successfully!\")","title":"Verify Core Installation"},{"location":"installation/#verify-ai-agents","text":"try: from rakam_systems.ai_agents import BaseAgent from rakam_systems.ai_core.interfaces.agent import AgentInput, AgentOutput print(\"\u2705 AI Agents installed successfully!\") except ImportError as e: print(f\"\u274c AI Agents not installed: {e}\")","title":"Verify AI Agents"},{"location":"installation/#verify-ai-vectorstore","text":"try: from rakam_systems.ai_vectorstore import ( VectorStoreConfig, Node, VSFile, ConfigurableEmbeddings ) print(\"\u2705 AI Vectorstore installed successfully!\") except ImportError as e: print(f\"\u274c AI Vectorstore not installed: {e}\")","title":"Verify AI Vectorstore"},{"location":"installation/#verify-llm-gateway","text":"try: from rakam_systems.ai_agents.components.llm_gateway import ( OpenAIGateway, MistralGateway, LLMGatewayFactory ) print(\"\u2705 LLM Gateway installed successfully!\") except ImportError as e: print(f\"\u274c LLM Gateway not installed: {e}\")","title":"Verify LLM Gateway"},{"location":"installation/#full-verification-script","text":"#!/usr/bin/env python3 \"\"\"Verify rakam_systems installation.\"\"\" def check_module(name: str, import_fn): \"\"\"Check if a module is properly installed.\"\"\" try: import_fn() print(f\"\u2705 {name}\") return True except ImportError as e: print(f\"\u274c {name}: {e}\") return False def main(): print(\"=\" * 50) print(\"Rakam Systems Installation Verification\") print(\"=\" * 50) results = [] # Core (always required) results.append(check_module( \"Core\", lambda: __import__('rakam_systems.ai_core.base', fromlist=['BaseComponent']) )) # AI Agents results.append(check_module( \"AI Agents\", lambda: __import__('rakam_systems.ai_agents', fromlist=['BaseAgent']) )) # AI Vectorstore results.append(check_module( \"AI Vectorstore\", lambda: __import__('rakam_systems.ai_vectorstore', fromlist=['VectorStoreConfig']) )) # LLM Gateway results.append(check_module( \"LLM Gateway\", lambda: __import__('rakam_systems.ai_agents.components.llm_gateway', fromlist=['OpenAIGateway']) )) print(\"=\" * 50) passed = sum(results) total = len(results) print(f\"Result: {passed}/{total} modules installed\") if passed == total: print(\"\ud83c\udf89 All modules installed successfully!\") else: print(\"\u26a0\ufe0f Some modules are missing. Install them with:\") print(' pip install -e \".[all]\"') if __name__ == \"__main__\": main()","title":"Full Verification Script"},{"location":"installation/#docker-setup","text":"","title":"Docker Setup"},{"location":"installation/#development-with-docker","text":"Create a Dockerfile : FROM python:3.11-slim WORKDIR /app # Install system dependencies RUN apt-get update && apt-get install -y \\ libmagic1 \\ libpq-dev \\ gcc \\ && rm -rf /var/lib/apt/lists/* # Copy package files COPY app/rakam_systems /app/rakam_systems # Install the package WORKDIR /app/rakam_systems RUN pip install -e \".[all]\" # Set working directory back WORKDIR /app # Default command CMD [\"python\"]","title":"Development with Docker"},{"location":"installation/#docker-compose-for-full-stack","text":"version: '3.8' services: app: build: . volumes: - .:/app environment: - OPENAI_API_KEY=${OPENAI_API_KEY} - POSTGRES_HOST=postgres - POSTGRES_PORT=5432 - POSTGRES_DB=vectorstore_db - POSTGRES_USER=postgres - POSTGRES_PASSWORD=postgres depends_on: - postgres postgres: image: pgvector/pgvector:pg16 environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: postgres POSTGRES_DB: vectorstore_db ports: - \"5432:5432\" volumes: - postgres_data:/var/lib/postgresql/data volumes: postgres_data:","title":"Docker Compose for Full Stack"},{"location":"installation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"installation/#common-issues","text":"","title":"Common Issues"},{"location":"installation/#1-modulenotfounderror","text":"ModuleNotFoundError: No module named 'rakam_systems' Solution: Install from the correct directory: cd app/rakam_systems pip install -e .","title":"1. ModuleNotFoundError"},{"location":"installation/#2-missing-optional-dependencies","text":"ImportError: cannot import name 'BaseAgent' from 'rakam_systems.ai_agents' Solution: Install the required module: pip install -e \".[ai-agents]\"","title":"2. Missing Optional Dependencies"},{"location":"installation/#3-django-not-configured","text":"django.core.exceptions.ImproperlyConfigured: Requested setting INSTALLED_APPS... Solution: Configure Django before importing Django-dependent components: import django from django.conf import settings settings.configure( INSTALLED_APPS=['rakam_systems.ai_vectorstore.components.vectorstore'], DATABASES={'default': {...}} ) django.setup() # Now import Django-dependent components from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore","title":"3. Django Not Configured"},{"location":"installation/#4-pytorch-installation-issues","text":"PyTorch is large (~2GB). For CPU-only installation: pip install torch --index-url https://download.pytorch.org/whl/cpu pip install -e \".[ai-vectorstore]\" For CUDA support: pip install torch --index-url https://download.pytorch.org/whl/cu118 pip install -e \".[ai-vectorstore]\"","title":"4. PyTorch Installation Issues"},{"location":"installation/#5-faiss-gpu-support","text":"Replace CPU version with GPU version: pip uninstall faiss-cpu pip install faiss-gpu","title":"5. FAISS GPU Support"},{"location":"installation/#6-libmagic-not-found","text":"On macOS: brew install libmagic On Ubuntu/Debian: apt-get install libmagic1 On Windows: pip install python-magic-bin","title":"6. libmagic Not Found"},{"location":"installation/#7-postgresql-connection-refused","text":"Ensure PostgreSQL is running: # Check if running docker ps | grep postgres # Start if not running docker start postgres-vectorstore # Or start with docker compose docker compose up -d postgres","title":"7. PostgreSQL Connection Refused"},{"location":"installation/#8-permission-errors","text":"Use a virtual environment: python -m venv venv source venv/bin/activate # Linux/macOS # or .\\venv\\Scripts\\activate # Windows pip install -e \".[all]\"","title":"8. Permission Errors"},{"location":"installation/#getting-help","text":"Documentation : See docs/ directory Examples : Check rakam_systems/examples/ Issues : GitHub Issues","title":"Getting Help"},{"location":"installation/#upgrading","text":"","title":"Upgrading"},{"location":"installation/#upgrade-to-latest-version","text":"cd app/rakam_systems git pull pip install -e \".[all]\" --upgrade","title":"Upgrade to Latest Version"},{"location":"installation/#regenerate-locked-dependencies","text":"pip install -e \".[complete]\" pip freeze > requirements.txt","title":"Regenerate Locked Dependencies"},{"location":"installation/#uninstallation","text":"pip uninstall rakam-systems To remove all dependencies: pip uninstall rakam-systems pip freeze | xargs pip uninstall -y # Be careful: removes ALL packages","title":"Uninstallation"},{"location":"installation/#next-steps","text":"After installation: Read the Documentation Components Guide Development Guide Explore Examples rakam_systems/examples/ai_agents_examples/ rakam_systems/examples/ai_vectorstore_examples/ Try Quick Start Code import asyncio from rakam_systems.ai_agents import BaseAgent async def main(): agent = BaseAgent( name=\"my_agent\", model=\"openai:gpt-4o\", system_prompt=\"You are a helpful assistant.\" ) result = await agent.arun(\"Hello, world!\") print(result.output_text) asyncio.run(main())","title":"Next Steps"},{"location":"installation/#summary-of-commands","text":"# Core only pip install -e . # AI Agents pip install -e \".[ai-agents]\" # AI Vectorstore pip install -e \".[ai-vectorstore]\" # LLM Gateway pip install -e \".[llm-gateway]\" # Document Loaders pip install -e \".[loaders]\" # All features pip install -e \".[all]\" # All features + dev tools pip install -e \".[complete]\" # Custom combination pip install -e \".[ai-agents,ai-vectorstore,dev]\" License: Apache 2.0 Support: GitHub Repository","title":"Summary of Commands"},{"location":"pipelines/","text":"Data Ingestion Evaluation","title":"Pipelines"},{"location":"pipelines/#_1","text":"","title":""},{"location":"pipelines/#data-ingestion","text":"","title":"Data Ingestion"},{"location":"pipelines/#evaluation","text":"","title":"Evaluation"},{"location":"quick_start/","text":"Rakam Systems Quick Start Guide Get up and running with rakam_systems in 5 minutes! \ud83d\udcd1 Table of Contents Prerequisites Installation Quick Start: AI Agent Quick Start: Agent with Tools Quick Start: Structured Output Quick Start: Configurable Agent Quick Start: Vector Store Quick Start: Configurable Vector Store Quick Start: RAG Pipeline Quick Start: LLM Gateway Quick Start: Document Loading Quick Start: Configuration from YAML Common Patterns Next Steps Prerequisites Python 3.10 or higher OpenAI API key (for most examples) pip package manager Installation # Navigate to the package cd app/rakam_systems # Install AI Agents module pip install -e \".[ai-agents]\" # Or install everything pip install -e \".[all]\" Set your API key: export OPENAI_API_KEY=\"sk-your-api-key\" Quick Start: AI Agent Create a simple AI agent in just a few lines: import asyncio from dotenv import load_dotenv load_dotenv() from rakam_systems.ai_agents import BaseAgent async def main(): # Create an agent agent = BaseAgent( name=\"my_assistant\", model=\"openai:gpt-4o\", system_prompt=\"You are a helpful assistant that provides concise answers.\" ) # Ask a question result = await agent.arun(\"What is Python?\") print(result.output_text) asyncio.run(main()) With Streaming import asyncio from rakam_systems.ai_agents import BaseAgent async def main(): agent = BaseAgent( name=\"streaming_agent\", model=\"openai:gpt-4o\", system_prompt=\"You are a helpful assistant.\" ) # Stream the response print(\"Response: \", end=\"\", flush=True) async for chunk in agent.astream(\"Tell me a short story about a robot.\"): print(chunk, end=\"\", flush=True) print() asyncio.run(main()) With Model Settings import asyncio from rakam_systems.ai_agents import BaseAgent from rakam_systems.ai_core.interfaces import ModelSettings async def main(): agent = BaseAgent( name=\"creative_agent\", model=\"openai:gpt-4o\", system_prompt=\"You are a creative writer.\" ) # Use custom temperature and max tokens result = await agent.arun( \"Write a haiku about programming.\", model_settings=ModelSettings(temperature=0.9, max_tokens=100) ) print(result.output_text) asyncio.run(main()) Quick Start: Agent with Tools Create an agent that can use tools: import asyncio from rakam_systems.ai_agents import BaseAgent from rakam_systems.ai_core.interfaces.tool import ToolComponent # Define a tool function def get_weather(city: str, units: str = \"celsius\") -> str: \"\"\"Get weather for a city (mock implementation).\"\"\" return f\"Weather in {city}: 22\u00b0{'C' if units == 'celsius' else 'F'}, Sunny\" def calculate(expression: str) -> str: \"\"\"Calculate a mathematical expression.\"\"\" try: result = eval(expression) return f\"Result: {result}\" except Exception as e: return f\"Error: {e}\" # Create tool components weather_tool = ToolComponent.from_function( function=get_weather, name=\"get_weather\", description=\"Get the current weather for a city\", json_schema={ \"type\": \"object\", \"properties\": { \"city\": {\"type\": \"string\", \"description\": \"City name\"}, \"units\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"default\": \"celsius\" } }, \"required\": [\"city\"], \"additionalProperties\": False } ) calculator_tool = ToolComponent.from_function( function=calculate, name=\"calculator\", description=\"Calculate a mathematical expression\", json_schema={ \"type\": \"object\", \"properties\": { \"expression\": {\"type\": \"string\", \"description\": \"Math expression to evaluate\"} }, \"required\": [\"expression\"], \"additionalProperties\": False } ) async def main(): # Create agent with tools agent = BaseAgent( name=\"tool_agent\", model=\"openai:gpt-4o\", system_prompt=\"You are a helpful assistant with access to tools.\", tools=[weather_tool, calculator_tool] ) # Ask questions that use tools result = await agent.arun(\"What's the weather in Paris?\") print(f\"Weather: {result.output_text}\\n\") result = await agent.arun(\"What is 25 * 4 + 100?\") print(f\"Calculation: {result.output_text}\") asyncio.run(main()) Quick Start: Structured Output Get structured, typed responses from your agent: import asyncio from pydantic import BaseModel, Field from rakam_systems.ai_agents import BaseAgent # Define output structure class MovieReview(BaseModel): title: str = Field(description=\"Movie title\") rating: float = Field(ge=0, le=10, description=\"Rating out of 10\") summary: str = Field(description=\"Brief summary\") pros: list[str] = Field(description=\"List of positive aspects\") cons: list[str] = Field(description=\"List of negative aspects\") recommended: bool = Field(description=\"Whether to recommend\") async def main(): # Create agent with structured output agent = BaseAgent( name=\"movie_critic\", model=\"openai:gpt-4o\", system_prompt=\"You are a movie critic. Analyze movies thoroughly.\", output_type=MovieReview # Enforces structured output ) result = await agent.arun(\"Review the movie 'Inception' by Christopher Nolan\") # Access typed output review: MovieReview = result.output print(f\"Title: {review.title}\") print(f\"Rating: {review.rating}/10\") print(f\"Summary: {review.summary}\") print(f\"Pros: {', '.join(review.pros)}\") print(f\"Cons: {', '.join(review.cons)}\") print(f\"Recommended: {'Yes' if review.recommended else 'No'}\") asyncio.run(main()) Quick Start: Configurable Agent Create agents from YAML configuration files for production deployments: Agent Configuration File Create config/agent_config.yaml : version: \"1.0\" # Define reusable prompts prompts: customer_support: name: \"customer_support\" description: \"Friendly customer support specialist\" system_prompt: | You are a friendly and empathetic customer support specialist. Your goal is to help customers solve their problems efficiently. Guidelines: - Listen actively to customer concerns - Provide clear, step-by-step solutions - Always maintain a professional yet warm tone skills: - \"Problem solving\" - \"Customer communication\" tags: - \"support\" code_assistant: name: \"code_assistant\" description: \"Expert programming assistant\" system_prompt: | You are an expert programming assistant with deep knowledge of: - Multiple programming languages (Python, JavaScript, TypeScript, etc.) - Software design patterns and best practices - Code optimization and debugging When generating code: 1. Follow best practices and conventions 2. Include comments and documentation 3. Consider edge cases and error handling # Define reusable tools tools: get_weather: name: \"get_weather\" type: \"direct\" module: \"rakam_systems.ai_agents.components.tools.example_tools\" function: \"get_current_weather\" description: \"Get the current weather for a location\" category: \"utility\" tags: [\"weather\", \"external\"] json_schema: type: \"object\" properties: location: type: \"string\" description: \"City name or location\" units: type: \"string\" enum: [\"celsius\", \"fahrenheit\"] default: \"celsius\" required: [\"location\"] additionalProperties: false analyze_sentiment: name: \"analyze_sentiment\" type: \"direct\" module: \"rakam_systems.ai_agents.components.tools.example_tools\" function: \"analyze_sentiment\" description: \"Analyze the sentiment of text\" category: \"nlp\" json_schema: type: \"object\" properties: text: type: \"string\" description: \"Text to analyze\" required: [\"text\"] additionalProperties: false # Define agents agents: support_agent: name: \"support_agent\" description: \"Customer support specialist\" # Model configuration llm_config: model: \"openai:gpt-4o\" temperature: 0.7 max_tokens: 2000 parallel_tool_calls: true # Reference to prompt library prompt_config: \"customer_support\" # Reference to tools library tools: - \"get_weather\" - \"analyze_sentiment\" # Enable input/output tracking enable_tracking: true tracking_output_dir: \"./agent_tracking/support\" # Additional metadata metadata: version: \"1.0\" department: \"customer_support\" # Agent with structured output defined inline sql_agent: name: \"sql_agent\" description: \"SQL query assistant with structured output\" llm_config: model: \"openai:gpt-4o\" temperature: 0.2 # Low temperature for consistent output max_tokens: 3000 prompt_config: \"code_assistant\" tools: [] # Define structured output directly in YAML (no Python class needed!) output_type: name: \"SQLAgentOutput\" description: \"Structured output for SQL queries\" fields: answer: type: str description: \"The answer to the user's question\" sql_query: type: str description: \"The generated SQL query\" default: \"\" explanation: type: str description: \"Explanation of the query\" tables_used: type: list description: \"List of tables referenced\" default_factory: list enable_tracking: true Loading Agents from Configuration import asyncio from rakam_systems.ai_core.config_loader import ConfigurationLoader async def main(): # Initialize configuration loader loader = ConfigurationLoader() # Load configuration from YAML config = loader.load_from_yaml(\"config/agent_config.yaml\") # Validate configuration (optional but recommended) is_valid, errors = loader.validate_config() if not is_valid: for error in errors: print(f\"Config error: {error}\") return # Create a single agent support_agent = loader.create_agent(\"support_agent\", config) # Use the agent result = await support_agent.arun(\"What's the weather in New York?\") print(f\"Support Agent: {result.output_text}\") # Create agent with structured output sql_agent = loader.create_agent(\"sql_agent\", config) result = await sql_agent.arun(\"Write a query to find all users who signed up last month\") # Access structured output print(f\"Answer: {result.output.answer}\") print(f\"SQL: {result.output.sql_query}\") print(f\"Tables: {result.output.tables_used}\") # Create all agents at once all_agents = loader.create_all_agents(config) print(f\"Created {len(all_agents)} agents: {list(all_agents.keys())}\") asyncio.run(main()) Using Tool Registry from rakam_systems.ai_core.config_loader import ConfigurationLoader from rakam_systems.ai_core.interfaces.tool_registry import ToolRegistry loader = ConfigurationLoader() config = loader.load_from_yaml(\"config/agent_config.yaml\") # Get tool registry with all configured tools registry = loader.get_tool_registry(config) # Query tools by category utility_tools = registry.get_tools_by_category(\"utility\") print(f\"Utility tools: {[t.name for t in utility_tools]}\") # Query tools by tag external_tools = registry.get_tools_by_tag(\"external\") print(f\"External tools: {[t.name for t in external_tools]}\") # Get specific tool weather_tool = registry.get_tool(\"get_weather\") print(f\"Weather tool: {weather_tool.description}\") Configuration Options Reference Model Configuration ( llm_config ) Option Type Default Description model string required Model identifier (e.g., openai:gpt-4o ) temperature float 0.7 Creativity (0.0-1.0) max_tokens int 2000 Maximum response tokens parallel_tool_calls bool true Execute tools in parallel extra_settings dict {} Additional provider settings Tool Configuration Option Type Required Description name string \u2713 Unique tool identifier type string \u2713 direct or mcp module string \u2713* Python module path (for direct) function string \u2713* Function name (for direct) description string \u2713 Tool description for LLM json_schema object \u2713 Parameter schema category string - Tool category for filtering tags list - Tags for filtering Inline Output Type Option Type Required Description name string \u2713 Model class name description string - Model description fields dict \u2713 Field definitions Field options: type , description , default , default_factory , required Quick Start: Vector Store Create a vector store for semantic search: Using FAISS (In-Memory) from rakam_systems.ai_vectorstore.components.vectorstore.faiss_vector_store import FaissStore from rakam_systems.ai_vectorstore.core import Node, NodeMetadata # Create sample documents documents = [ \"Python is a high-level programming language.\", \"Machine learning is a subset of artificial intelligence.\", \"Deep learning uses neural networks with multiple layers.\", \"Natural language processing helps computers understand text.\", \"Vector databases store data based on semantic similarity.\", ] # Create nodes nodes = [] for i, doc in enumerate(documents): metadata = NodeMetadata( source_file_uuid=\"docs_001\", position=i, custom={\"category\": \"tech\"} ) nodes.append(Node(content=doc, metadata=metadata)) # Initialize FAISS store store = FaissStore( name=\"my_store\", base_index_path=\"./my_indexes\", embedding_model=\"Snowflake/snowflake-arctic-embed-m\", initialising=True ) # Create collection and add nodes store.create_collection_from_nodes(\"tech_docs\", nodes) # Search query = \"What is machine learning?\" results, result_nodes = store.search( collection_name=\"tech_docs\", query=query, distance_type=\"cosine\", number=3 ) print(f\"Query: {query}\\n\") for node_id, (metadata, content, distance) in results.items(): print(f\" [{distance:.4f}] {content}\") Using PostgreSQL with pgvector import os import django from django.conf import settings # Configure Django (required for PostgreSQL backend) if not settings.configured: settings.configure( INSTALLED_APPS=[ 'django.contrib.contenttypes', 'rakam_systems.ai_vectorstore.components.vectorstore', ], DATABASES={ 'default': { 'ENGINE': 'django.db.backends.postgresql', 'NAME': os.getenv('POSTGRES_DB', 'vectorstore_db'), 'USER': os.getenv('POSTGRES_USER', 'postgres'), 'PASSWORD': os.getenv('POSTGRES_PASSWORD', 'postgres'), 'HOST': os.getenv('POSTGRES_HOST', 'localhost'), 'PORT': os.getenv('POSTGRES_PORT', '5432'), } }, DEFAULT_AUTO_FIELD='django.db.models.BigAutoField', ) django.setup() from rakam_systems.ai_vectorstore import ( ConfigurablePgVectorStore, VectorStoreConfig, Node, NodeMetadata ) # Create configuration config = VectorStoreConfig( name=\"my_pg_store\", embedding={ \"model_type\": \"sentence_transformer\", \"model_name\": \"Snowflake/snowflake-arctic-embed-m\" }, search={ \"similarity_metric\": \"cosine\", \"default_top_k\": 5 } ) # Initialize store store = ConfigurablePgVectorStore(config=config) store.setup() # Add documents nodes = [ Node( content=\"Python is great for data science.\", metadata=NodeMetadata(source_file_uuid=\"doc1\", position=0) ), Node( content=\"JavaScript runs in web browsers.\", metadata=NodeMetadata(source_file_uuid=\"doc1\", position=1) ), ] store.add_nodes(nodes) # Search results = store.search(\"What language is good for data?\", top_k=3) for r in results: print(f\"[{r['score']:.4f}] {r['content']}\") # Cleanup store.shutdown() Quick Start: Configurable Vector Store The ConfigurablePgVectorStore provides a production-ready vector store with comprehensive configuration options. VectorStoreConfig Overview from rakam_systems.ai_vectorstore.config import ( VectorStoreConfig, EmbeddingConfig, DatabaseConfig, SearchConfig, IndexConfig, load_config ) # Create configuration programmatically config = VectorStoreConfig( name=\"production_store\", # Embedding configuration embedding=EmbeddingConfig( model_type=\"sentence_transformer\", # or \"openai\", \"cohere\" model_name=\"Snowflake/snowflake-arctic-embed-m\", batch_size=128, normalize=True, # api_key loaded from env: OPENAI_API_KEY or COHERE_API_KEY ), # Database configuration (uses env vars by default) database=DatabaseConfig( host=\"localhost\", # or POSTGRES_HOST env var port=5432, # or POSTGRES_PORT env var database=\"vectorstore_db\", user=\"postgres\", password=\"postgres\", ), # Search configuration search=SearchConfig( similarity_metric=\"cosine\", # \"cosine\", \"l2\", \"dot_product\" default_top_k=5, enable_hybrid_search=True, # Combine vector + keyword search hybrid_alpha=0.7, # Vector weight (0.3 for keyword) rerank=True, ), # Indexing configuration index=IndexConfig( chunk_size=512, chunk_overlap=50, batch_insert_size=10000, ), # Additional options enable_caching=True, cache_size=1000, enable_logging=True, log_level=\"INFO\", ) # Validate configuration config.validate() Configuration from YAML Create config/vectorstore.yaml : name: production_vectorstore embedding: model_type: sentence_transformer model_name: Snowflake/snowflake-arctic-embed-m batch_size: 128 normalize: true database: host: localhost port: 5432 database: vectorstore_db user: postgres password: postgres search: similarity_metric: cosine default_top_k: 5 enable_hybrid_search: true hybrid_alpha: 0.7 rerank: true index: chunk_size: 512 chunk_overlap: 50 batch_insert_size: 10000 enable_caching: true cache_size: 1000 enable_logging: true log_level: INFO Load and use: from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig # Load from YAML config = VectorStoreConfig.from_yaml(\"config/vectorstore.yaml\") # Or load from JSON config = VectorStoreConfig.from_json(\"config/vectorstore.json\") # Or use the helper function (auto-detects format) from rakam_systems.ai_vectorstore.config import load_config config = load_config(\"config/vectorstore.yaml\") # Create store store = ConfigurablePgVectorStore(config=config) store.setup() Using Different Embedding Models from rakam_systems.ai_vectorstore.config import VectorStoreConfig, EmbeddingConfig # Local embeddings with Sentence Transformers config_local = VectorStoreConfig( embedding=EmbeddingConfig( model_type=\"sentence_transformer\", model_name=\"Snowflake/snowflake-arctic-embed-m\", # 768 dimensions # model_name=\"all-MiniLM-L6-v2\", # 384 dimensions batch_size=128, ) ) # OpenAI embeddings config_openai = VectorStoreConfig( embedding=EmbeddingConfig( model_type=\"openai\", model_name=\"text-embedding-3-small\", # api_key loaded from OPENAI_API_KEY env var ) ) # Cohere embeddings config_cohere = VectorStoreConfig( embedding=EmbeddingConfig( model_type=\"cohere\", model_name=\"embed-english-v3.0\", # api_key loaded from COHERE_API_KEY env var ) ) Multi-Model Support Each embedding model automatically gets dedicated tables, preventing mixing of incompatible vector spaces: from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig, EmbeddingConfig # Store using MiniLM model config_minilm = VectorStoreConfig( embedding=EmbeddingConfig( model_type=\"sentence_transformer\", model_name=\"all-MiniLM-L6-v2\" # 384D ) ) store_minilm = ConfigurablePgVectorStore(config=config_minilm) # Tables: application_nodeentry_all_minilm_l6_v2 # Store using Arctic model config_arctic = VectorStoreConfig( embedding=EmbeddingConfig( model_type=\"sentence_transformer\", model_name=\"Snowflake/snowflake-arctic-embed-m\" # 768D ) ) store_arctic = ConfigurablePgVectorStore(config=config_arctic) # Tables: application_nodeentry_snowflake_arctic_embed_m # Both can coexist without conflicts! Important : Even if two models have the same dimensions, their vector spaces are different! Model-specific tables prevent meaningless search results from mixed embeddings. Hybrid Search Combine vector similarity with keyword search: from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig config = VectorStoreConfig() config.search.enable_hybrid_search = True config.search.hybrid_alpha = 0.7 # 70% vector, 30% keyword store = ConfigurablePgVectorStore(config=config) store.setup() # Regular search (uses config defaults) results = store.search(\"machine learning algorithms\", top_k=10) # Hybrid search with custom alpha results = store.hybrid_search( query=\"machine learning algorithms\", top_k=10, alpha=0.5 # 50/50 split ) for r in results: print(f\"[{r['score']:.4f}] {r['content'][:100]}...\") Full Example with All Features import os import django from django.conf import settings # Configure Django if not settings.configured: settings.configure( INSTALLED_APPS=[ 'django.contrib.contenttypes', 'rakam_systems.ai_vectorstore.components.vectorstore', ], DATABASES={ 'default': { 'ENGINE': 'django.db.backends.postgresql', 'NAME': os.getenv('POSTGRES_DB', 'vectorstore_db'), 'USER': os.getenv('POSTGRES_USER', 'postgres'), 'PASSWORD': os.getenv('POSTGRES_PASSWORD', 'postgres'), 'HOST': os.getenv('POSTGRES_HOST', 'localhost'), 'PORT': os.getenv('POSTGRES_PORT', '5432'), } }, DEFAULT_AUTO_FIELD='django.db.models.BigAutoField', ) django.setup() from rakam_systems.ai_vectorstore import ( ConfigurablePgVectorStore, VectorStoreConfig, Node, NodeMetadata, VSFile ) from rakam_systems.ai_vectorstore.components.loader import AdaptiveLoader # Create configuration config = VectorStoreConfig.from_yaml(\"config/vectorstore.yaml\") # Initialize store with context manager with ConfigurablePgVectorStore(config=config) as store: # Load documents loader = AdaptiveLoader(config={ \"chunk_size\": config.index.chunk_size, \"chunk_overlap\": config.index.chunk_overlap }) # Load from file vsfile = loader.load_as_vsfile(\"documents/manual.pdf\") store.add_vsfile(vsfile) # Or add nodes directly nodes = [ Node( content=\"Python is excellent for data science and machine learning.\", metadata=NodeMetadata( source_file_uuid=\"doc_001\", position=0, custom={\"category\": \"programming\", \"topic\": \"python\"} ) ), Node( content=\"PostgreSQL with pgvector enables efficient vector similarity search.\", metadata=NodeMetadata( source_file_uuid=\"doc_001\", position=1, custom={\"category\": \"database\", \"topic\": \"vectors\"} ) ), ] store.add_nodes(nodes) # Search with filters results = store.search( query=\"vector database search\", top_k=5, # Metadata filters can be applied here ) print(\"Search Results:\") for r in results: print(f\" [{r['score']:.4f}] {r['content'][:80]}...\") print(f\" Source: {r.get('source_file_uuid', 'N/A')}\") # Hybrid search print(\"\\nHybrid Search Results:\") hybrid_results = store.hybrid_search( query=\"PostgreSQL vector\", top_k=5, alpha=0.6 ) for r in hybrid_results: print(f\" [{r['score']:.4f}] {r['content'][:80]}...\") # Get collection stats count = store.count() print(f\"\\nTotal vectors in store: {count}\") # Store automatically cleaned up Configuration Reference EmbeddingConfig Option Type Default Description model_type str sentence_transformer Backend: sentence_transformer , openai , cohere model_name str Snowflake/snowflake-arctic-embed-m Model identifier api_key str None API key (auto-loaded from env) batch_size int 128 Batch size for encoding normalize bool True Normalize embeddings dimensions int None Vector dimensions (auto-detected) DatabaseConfig Option Type Default Description host str localhost PostgreSQL host (or POSTGRES_HOST env) port int 5432 PostgreSQL port (or POSTGRES_PORT env) database str vectorstore_db Database name (or POSTGRES_DB env) user str postgres Username (or POSTGRES_USER env) password str postgres Password (or POSTGRES_PASSWORD env) pool_size int 10 Connection pool size SearchConfig Option Type Default Description similarity_metric str cosine Metric: cosine , l2 , dot_product default_top_k int 5 Default results count enable_hybrid_search bool True Enable keyword + vector search hybrid_alpha float 0.7 Vector weight (1-alpha for keyword) rerank bool True Rerank results IndexConfig Option Type Default Description chunk_size int 512 Chunk size in tokens chunk_overlap int 50 Overlap between chunks batch_insert_size int 10000 Batch size for inserts Quick Start: RAG Pipeline Build a complete Retrieval-Augmented Generation pipeline: import asyncio from rakam_systems.ai_agents import BaseAgent from rakam_systems.ai_vectorstore.components.vectorstore.faiss_vector_store import FaissStore from rakam_systems.ai_vectorstore.components.loader import AdaptiveLoader from rakam_systems.ai_core.interfaces.tool import ToolComponent # Step 1: Load and index documents loader = AdaptiveLoader(config={\"chunk_size\": 512, \"chunk_overlap\": 50}) # Load documents (supports PDF, DOCX, TXT, MD, HTML, etc.) # nodes = loader.load_as_nodes(\"path/to/document.pdf\") # For demo, use sample text from rakam_systems.ai_vectorstore.core import Node, NodeMetadata sample_docs = [ \"Our company was founded in 2020 and specializes in AI solutions.\", \"We offer three main products: AI Assistant, Data Analytics, and Automation.\", \"The AI Assistant can handle customer queries 24/7 with 95% accuracy.\", \"Our pricing starts at $99/month for small businesses.\", \"Enterprise customers get dedicated support and custom integrations.\", \"We have offices in New York, London, and Tokyo.\", ] nodes = [ Node(content=doc, metadata=NodeMetadata(source_file_uuid=\"company_info\", position=i)) for i, doc in enumerate(sample_docs) ] # Step 2: Create vector store store = FaissStore( name=\"rag_store\", base_index_path=\"./rag_indexes\", embedding_model=\"Snowflake/snowflake-arctic-embed-m\", initialising=True ) store.create_collection_from_nodes(\"knowledge_base\", nodes) # Step 3: Create search tool def search_knowledge_base(query: str, top_k: int = 3) -> str: \"\"\"Search the knowledge base for relevant information.\"\"\" results, _ = store.search( collection_name=\"knowledge_base\", query=query, distance_type=\"cosine\", number=top_k ) context = \"\\n\".join([ f\"- {content}\" for _, (_, content, _) in results.items() ]) return f\"Relevant information:\\n{context}\" search_tool = ToolComponent.from_function( function=search_knowledge_base, name=\"search_knowledge_base\", description=\"Search the company knowledge base for information\", json_schema={ \"type\": \"object\", \"properties\": { \"query\": {\"type\": \"string\", \"description\": \"Search query\"}, \"top_k\": {\"type\": \"integer\", \"default\": 3, \"description\": \"Number of results\"} }, \"required\": [\"query\"], \"additionalProperties\": False } ) # Step 4: Create RAG agent async def main(): agent = BaseAgent( name=\"rag_agent\", model=\"openai:gpt-4o\", system_prompt=\"\"\"You are a helpful customer service agent for our company. Use the search_knowledge_base tool to find relevant information before answering. Always base your answers on the retrieved information.\"\"\", tools=[search_tool] ) # Ask questions questions = [ \"When was the company founded?\", \"What products do you offer?\", \"How much does the service cost?\", ] for question in questions: print(f\"\\nQ: {question}\") result = await agent.arun(question) print(f\"A: {result.output_text}\") asyncio.run(main()) Quick Start: LLM Gateway Use the LLM Gateway for direct LLM interactions: from rakam_systems.ai_agents.components.llm_gateway import ( OpenAIGateway, MistralGateway, LLMGatewayFactory, LLMRequest ) # Create gateway using factory gateway = LLMGatewayFactory.create( provider=\"openai\", model=\"gpt-4o\" ) # Simple text generation request = LLMRequest( system_prompt=\"You are a helpful assistant.\", user_prompt=\"Explain quantum computing in simple terms.\", temperature=0.7, max_tokens=200 ) response = gateway.generate(request) print(f\"Response: {response.content}\") print(f\"Tokens used: {response.usage}\") Structured Output with Gateway from pydantic import BaseModel from rakam_systems.ai_agents.components.llm_gateway import OpenAIGateway, LLMRequest class Recipe(BaseModel): name: str ingredients: list[str] instructions: list[str] prep_time_minutes: int servings: int gateway = OpenAIGateway(model=\"gpt-4o\") request = LLMRequest( system_prompt=\"You are a chef. Create recipes based on user requests.\", user_prompt=\"Give me a simple pasta recipe.\" ) recipe = gateway.generate_structured(request, Recipe) print(f\"Recipe: {recipe.name}\") print(f\"Prep time: {recipe.prep_time_minutes} minutes\") print(f\"Ingredients: {', '.join(recipe.ingredients)}\") Streaming with Gateway from rakam_systems.ai_agents.components.llm_gateway import OpenAIGateway, LLMRequest gateway = OpenAIGateway(model=\"gpt-4o\") request = LLMRequest( user_prompt=\"Write a poem about coding.\", temperature=0.8 ) print(\"Poem:\\n\") for chunk in gateway.stream(request): print(chunk, end=\"\", flush=True) print() Quick Start: Document Loading Load and process various document types: from rakam_systems.ai_vectorstore.components.loader import AdaptiveLoader # Create loader loader = AdaptiveLoader(config={ \"chunk_size\": 512, \"chunk_overlap\": 50, \"encoding\": \"utf-8\" }) # Load as text text = loader.load_as_text(\"document.pdf\") print(f\"Loaded {len(text)} characters\") # Load as chunks chunks = loader.load_as_chunks(\"document.pdf\") print(f\"Created {len(chunks)} chunks\") # Load as nodes (with metadata) nodes = loader.load_as_nodes( \"document.pdf\", custom_metadata={\"category\": \"technical\", \"author\": \"John\"} ) print(f\"Created {len(nodes)} nodes\") # Load as VSFile (complete document representation) vsfile = loader.load_as_vsfile(\"document.pdf\") print(f\"File: {vsfile.file_name}, UUID: {vsfile.uuid}\") Supported File Types Type Extensions Text .txt , .text Markdown .md , .markdown PDF .pdf Word .docx , .doc OpenDocument .odt HTML .html , .htm , .xhtml Email .eml , .msg Data .json , .csv , .tsv , .xlsx , .xls Code .py , .js , .ts , .java , .cpp , .go , .rs , .rb Quick Start: Configuration from YAML Load agents and tools from configuration files: # config/agents.yaml version: \"1.0\" prompts: assistant: system_prompt: | You are a helpful AI assistant. Always be accurate and concise. tools: get_time: name: \"get_time\" type: \"direct\" module: \"datetime\" function: \"datetime.now\" description: \"Get current date and time\" json_schema: type: \"object\" properties: {} agents: main_agent: name: \"main_agent\" model_config: model: \"openai:gpt-4o\" temperature: 0.7 prompt_config: \"assistant\" tools: - \"get_time\" enable_tracking: true from rakam_systems.ai_core.config_loader import ConfigurationLoader loader = ConfigurationLoader() # Load configuration config = loader.load_from_yaml(\"config/agents.yaml\") # Create agent from config agent = loader.create_agent(\"main_agent\", config) # Use the agent result = await agent.arun(\"What time is it?\") Next Steps Now that you've completed the quick start: Read the Full Documentation Components Guide - Detailed component documentation Development Guide - How to extend the framework Installation Guide - Advanced installation options Explore More Examples rakam_systems/examples/ai_agents_examples/ - Agent examples rakam_systems/examples/ai_vectorstore_examples/ - Vector store examples rakam_systems/examples/configs/ - Configuration examples Build Your Application Start with a simple agent Add tools for your specific use case Integrate vector storage for RAG Scale with PostgreSQL for production Join the Community GitHub Repository Report issues and request features Contribute to the project Common Patterns Error Handling import asyncio from rakam_systems.ai_agents import BaseAgent async def main(): agent = BaseAgent( name=\"safe_agent\", model=\"openai:gpt-4o\", system_prompt=\"You are helpful.\" ) try: result = await agent.arun(\"Hello!\") print(result.output_text) except Exception as e: print(f\"Error: {e}\") asyncio.run(main()) Context Manager Pattern from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore # Automatic setup and cleanup with ConfigurablePgVectorStore(config=config) as store: store.add_nodes(nodes) results = store.search(\"query\") # shutdown() called automatically Async Batch Processing import asyncio from rakam_systems.ai_agents import BaseAgent async def process_queries(queries: list[str]): agent = BaseAgent( name=\"batch_agent\", model=\"openai:gpt-4o\", system_prompt=\"Answer concisely.\" ) # Process queries concurrently tasks = [agent.arun(q) for q in queries] results = await asyncio.gather(*tasks) return [r.output_text for r in results] queries = [\"What is Python?\", \"What is JavaScript?\", \"What is Rust?\"] answers = asyncio.run(process_queries(queries)) for q, a in zip(queries, answers): print(f\"Q: {q}\\nA: {a}\\n\") Happy Building! \ud83d\ude80","title":"Quick Start"},{"location":"quick_start/#rakam-systems-quick-start-guide","text":"Get up and running with rakam_systems in 5 minutes!","title":"Rakam Systems Quick Start Guide"},{"location":"quick_start/#table-of-contents","text":"Prerequisites Installation Quick Start: AI Agent Quick Start: Agent with Tools Quick Start: Structured Output Quick Start: Configurable Agent Quick Start: Vector Store Quick Start: Configurable Vector Store Quick Start: RAG Pipeline Quick Start: LLM Gateway Quick Start: Document Loading Quick Start: Configuration from YAML Common Patterns Next Steps","title":"\ud83d\udcd1 Table of Contents"},{"location":"quick_start/#prerequisites","text":"Python 3.10 or higher OpenAI API key (for most examples) pip package manager","title":"Prerequisites"},{"location":"quick_start/#installation","text":"# Navigate to the package cd app/rakam_systems # Install AI Agents module pip install -e \".[ai-agents]\" # Or install everything pip install -e \".[all]\" Set your API key: export OPENAI_API_KEY=\"sk-your-api-key\"","title":"Installation"},{"location":"quick_start/#quick-start-ai-agent","text":"Create a simple AI agent in just a few lines: import asyncio from dotenv import load_dotenv load_dotenv() from rakam_systems.ai_agents import BaseAgent async def main(): # Create an agent agent = BaseAgent( name=\"my_assistant\", model=\"openai:gpt-4o\", system_prompt=\"You are a helpful assistant that provides concise answers.\" ) # Ask a question result = await agent.arun(\"What is Python?\") print(result.output_text) asyncio.run(main())","title":"Quick Start: AI Agent"},{"location":"quick_start/#with-streaming","text":"import asyncio from rakam_systems.ai_agents import BaseAgent async def main(): agent = BaseAgent( name=\"streaming_agent\", model=\"openai:gpt-4o\", system_prompt=\"You are a helpful assistant.\" ) # Stream the response print(\"Response: \", end=\"\", flush=True) async for chunk in agent.astream(\"Tell me a short story about a robot.\"): print(chunk, end=\"\", flush=True) print() asyncio.run(main())","title":"With Streaming"},{"location":"quick_start/#with-model-settings","text":"import asyncio from rakam_systems.ai_agents import BaseAgent from rakam_systems.ai_core.interfaces import ModelSettings async def main(): agent = BaseAgent( name=\"creative_agent\", model=\"openai:gpt-4o\", system_prompt=\"You are a creative writer.\" ) # Use custom temperature and max tokens result = await agent.arun( \"Write a haiku about programming.\", model_settings=ModelSettings(temperature=0.9, max_tokens=100) ) print(result.output_text) asyncio.run(main())","title":"With Model Settings"},{"location":"quick_start/#quick-start-agent-with-tools","text":"Create an agent that can use tools: import asyncio from rakam_systems.ai_agents import BaseAgent from rakam_systems.ai_core.interfaces.tool import ToolComponent # Define a tool function def get_weather(city: str, units: str = \"celsius\") -> str: \"\"\"Get weather for a city (mock implementation).\"\"\" return f\"Weather in {city}: 22\u00b0{'C' if units == 'celsius' else 'F'}, Sunny\" def calculate(expression: str) -> str: \"\"\"Calculate a mathematical expression.\"\"\" try: result = eval(expression) return f\"Result: {result}\" except Exception as e: return f\"Error: {e}\" # Create tool components weather_tool = ToolComponent.from_function( function=get_weather, name=\"get_weather\", description=\"Get the current weather for a city\", json_schema={ \"type\": \"object\", \"properties\": { \"city\": {\"type\": \"string\", \"description\": \"City name\"}, \"units\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"default\": \"celsius\" } }, \"required\": [\"city\"], \"additionalProperties\": False } ) calculator_tool = ToolComponent.from_function( function=calculate, name=\"calculator\", description=\"Calculate a mathematical expression\", json_schema={ \"type\": \"object\", \"properties\": { \"expression\": {\"type\": \"string\", \"description\": \"Math expression to evaluate\"} }, \"required\": [\"expression\"], \"additionalProperties\": False } ) async def main(): # Create agent with tools agent = BaseAgent( name=\"tool_agent\", model=\"openai:gpt-4o\", system_prompt=\"You are a helpful assistant with access to tools.\", tools=[weather_tool, calculator_tool] ) # Ask questions that use tools result = await agent.arun(\"What's the weather in Paris?\") print(f\"Weather: {result.output_text}\\n\") result = await agent.arun(\"What is 25 * 4 + 100?\") print(f\"Calculation: {result.output_text}\") asyncio.run(main())","title":"Quick Start: Agent with Tools"},{"location":"quick_start/#quick-start-structured-output","text":"Get structured, typed responses from your agent: import asyncio from pydantic import BaseModel, Field from rakam_systems.ai_agents import BaseAgent # Define output structure class MovieReview(BaseModel): title: str = Field(description=\"Movie title\") rating: float = Field(ge=0, le=10, description=\"Rating out of 10\") summary: str = Field(description=\"Brief summary\") pros: list[str] = Field(description=\"List of positive aspects\") cons: list[str] = Field(description=\"List of negative aspects\") recommended: bool = Field(description=\"Whether to recommend\") async def main(): # Create agent with structured output agent = BaseAgent( name=\"movie_critic\", model=\"openai:gpt-4o\", system_prompt=\"You are a movie critic. Analyze movies thoroughly.\", output_type=MovieReview # Enforces structured output ) result = await agent.arun(\"Review the movie 'Inception' by Christopher Nolan\") # Access typed output review: MovieReview = result.output print(f\"Title: {review.title}\") print(f\"Rating: {review.rating}/10\") print(f\"Summary: {review.summary}\") print(f\"Pros: {', '.join(review.pros)}\") print(f\"Cons: {', '.join(review.cons)}\") print(f\"Recommended: {'Yes' if review.recommended else 'No'}\") asyncio.run(main())","title":"Quick Start: Structured Output"},{"location":"quick_start/#quick-start-configurable-agent","text":"Create agents from YAML configuration files for production deployments:","title":"Quick Start: Configurable Agent"},{"location":"quick_start/#agent-configuration-file","text":"Create config/agent_config.yaml : version: \"1.0\" # Define reusable prompts prompts: customer_support: name: \"customer_support\" description: \"Friendly customer support specialist\" system_prompt: | You are a friendly and empathetic customer support specialist. Your goal is to help customers solve their problems efficiently. Guidelines: - Listen actively to customer concerns - Provide clear, step-by-step solutions - Always maintain a professional yet warm tone skills: - \"Problem solving\" - \"Customer communication\" tags: - \"support\" code_assistant: name: \"code_assistant\" description: \"Expert programming assistant\" system_prompt: | You are an expert programming assistant with deep knowledge of: - Multiple programming languages (Python, JavaScript, TypeScript, etc.) - Software design patterns and best practices - Code optimization and debugging When generating code: 1. Follow best practices and conventions 2. Include comments and documentation 3. Consider edge cases and error handling # Define reusable tools tools: get_weather: name: \"get_weather\" type: \"direct\" module: \"rakam_systems.ai_agents.components.tools.example_tools\" function: \"get_current_weather\" description: \"Get the current weather for a location\" category: \"utility\" tags: [\"weather\", \"external\"] json_schema: type: \"object\" properties: location: type: \"string\" description: \"City name or location\" units: type: \"string\" enum: [\"celsius\", \"fahrenheit\"] default: \"celsius\" required: [\"location\"] additionalProperties: false analyze_sentiment: name: \"analyze_sentiment\" type: \"direct\" module: \"rakam_systems.ai_agents.components.tools.example_tools\" function: \"analyze_sentiment\" description: \"Analyze the sentiment of text\" category: \"nlp\" json_schema: type: \"object\" properties: text: type: \"string\" description: \"Text to analyze\" required: [\"text\"] additionalProperties: false # Define agents agents: support_agent: name: \"support_agent\" description: \"Customer support specialist\" # Model configuration llm_config: model: \"openai:gpt-4o\" temperature: 0.7 max_tokens: 2000 parallel_tool_calls: true # Reference to prompt library prompt_config: \"customer_support\" # Reference to tools library tools: - \"get_weather\" - \"analyze_sentiment\" # Enable input/output tracking enable_tracking: true tracking_output_dir: \"./agent_tracking/support\" # Additional metadata metadata: version: \"1.0\" department: \"customer_support\" # Agent with structured output defined inline sql_agent: name: \"sql_agent\" description: \"SQL query assistant with structured output\" llm_config: model: \"openai:gpt-4o\" temperature: 0.2 # Low temperature for consistent output max_tokens: 3000 prompt_config: \"code_assistant\" tools: [] # Define structured output directly in YAML (no Python class needed!) output_type: name: \"SQLAgentOutput\" description: \"Structured output for SQL queries\" fields: answer: type: str description: \"The answer to the user's question\" sql_query: type: str description: \"The generated SQL query\" default: \"\" explanation: type: str description: \"Explanation of the query\" tables_used: type: list description: \"List of tables referenced\" default_factory: list enable_tracking: true","title":"Agent Configuration File"},{"location":"quick_start/#loading-agents-from-configuration","text":"import asyncio from rakam_systems.ai_core.config_loader import ConfigurationLoader async def main(): # Initialize configuration loader loader = ConfigurationLoader() # Load configuration from YAML config = loader.load_from_yaml(\"config/agent_config.yaml\") # Validate configuration (optional but recommended) is_valid, errors = loader.validate_config() if not is_valid: for error in errors: print(f\"Config error: {error}\") return # Create a single agent support_agent = loader.create_agent(\"support_agent\", config) # Use the agent result = await support_agent.arun(\"What's the weather in New York?\") print(f\"Support Agent: {result.output_text}\") # Create agent with structured output sql_agent = loader.create_agent(\"sql_agent\", config) result = await sql_agent.arun(\"Write a query to find all users who signed up last month\") # Access structured output print(f\"Answer: {result.output.answer}\") print(f\"SQL: {result.output.sql_query}\") print(f\"Tables: {result.output.tables_used}\") # Create all agents at once all_agents = loader.create_all_agents(config) print(f\"Created {len(all_agents)} agents: {list(all_agents.keys())}\") asyncio.run(main())","title":"Loading Agents from Configuration"},{"location":"quick_start/#using-tool-registry","text":"from rakam_systems.ai_core.config_loader import ConfigurationLoader from rakam_systems.ai_core.interfaces.tool_registry import ToolRegistry loader = ConfigurationLoader() config = loader.load_from_yaml(\"config/agent_config.yaml\") # Get tool registry with all configured tools registry = loader.get_tool_registry(config) # Query tools by category utility_tools = registry.get_tools_by_category(\"utility\") print(f\"Utility tools: {[t.name for t in utility_tools]}\") # Query tools by tag external_tools = registry.get_tools_by_tag(\"external\") print(f\"External tools: {[t.name for t in external_tools]}\") # Get specific tool weather_tool = registry.get_tool(\"get_weather\") print(f\"Weather tool: {weather_tool.description}\")","title":"Using Tool Registry"},{"location":"quick_start/#configuration-options-reference","text":"","title":"Configuration Options Reference"},{"location":"quick_start/#model-configuration-llm_config","text":"Option Type Default Description model string required Model identifier (e.g., openai:gpt-4o ) temperature float 0.7 Creativity (0.0-1.0) max_tokens int 2000 Maximum response tokens parallel_tool_calls bool true Execute tools in parallel extra_settings dict {} Additional provider settings","title":"Model Configuration (llm_config)"},{"location":"quick_start/#tool-configuration","text":"Option Type Required Description name string \u2713 Unique tool identifier type string \u2713 direct or mcp module string \u2713* Python module path (for direct) function string \u2713* Function name (for direct) description string \u2713 Tool description for LLM json_schema object \u2713 Parameter schema category string - Tool category for filtering tags list - Tags for filtering","title":"Tool Configuration"},{"location":"quick_start/#inline-output-type","text":"Option Type Required Description name string \u2713 Model class name description string - Model description fields dict \u2713 Field definitions Field options: type , description , default , default_factory , required","title":"Inline Output Type"},{"location":"quick_start/#quick-start-vector-store","text":"Create a vector store for semantic search:","title":"Quick Start: Vector Store"},{"location":"quick_start/#using-faiss-in-memory","text":"from rakam_systems.ai_vectorstore.components.vectorstore.faiss_vector_store import FaissStore from rakam_systems.ai_vectorstore.core import Node, NodeMetadata # Create sample documents documents = [ \"Python is a high-level programming language.\", \"Machine learning is a subset of artificial intelligence.\", \"Deep learning uses neural networks with multiple layers.\", \"Natural language processing helps computers understand text.\", \"Vector databases store data based on semantic similarity.\", ] # Create nodes nodes = [] for i, doc in enumerate(documents): metadata = NodeMetadata( source_file_uuid=\"docs_001\", position=i, custom={\"category\": \"tech\"} ) nodes.append(Node(content=doc, metadata=metadata)) # Initialize FAISS store store = FaissStore( name=\"my_store\", base_index_path=\"./my_indexes\", embedding_model=\"Snowflake/snowflake-arctic-embed-m\", initialising=True ) # Create collection and add nodes store.create_collection_from_nodes(\"tech_docs\", nodes) # Search query = \"What is machine learning?\" results, result_nodes = store.search( collection_name=\"tech_docs\", query=query, distance_type=\"cosine\", number=3 ) print(f\"Query: {query}\\n\") for node_id, (metadata, content, distance) in results.items(): print(f\" [{distance:.4f}] {content}\")","title":"Using FAISS (In-Memory)"},{"location":"quick_start/#using-postgresql-with-pgvector","text":"import os import django from django.conf import settings # Configure Django (required for PostgreSQL backend) if not settings.configured: settings.configure( INSTALLED_APPS=[ 'django.contrib.contenttypes', 'rakam_systems.ai_vectorstore.components.vectorstore', ], DATABASES={ 'default': { 'ENGINE': 'django.db.backends.postgresql', 'NAME': os.getenv('POSTGRES_DB', 'vectorstore_db'), 'USER': os.getenv('POSTGRES_USER', 'postgres'), 'PASSWORD': os.getenv('POSTGRES_PASSWORD', 'postgres'), 'HOST': os.getenv('POSTGRES_HOST', 'localhost'), 'PORT': os.getenv('POSTGRES_PORT', '5432'), } }, DEFAULT_AUTO_FIELD='django.db.models.BigAutoField', ) django.setup() from rakam_systems.ai_vectorstore import ( ConfigurablePgVectorStore, VectorStoreConfig, Node, NodeMetadata ) # Create configuration config = VectorStoreConfig( name=\"my_pg_store\", embedding={ \"model_type\": \"sentence_transformer\", \"model_name\": \"Snowflake/snowflake-arctic-embed-m\" }, search={ \"similarity_metric\": \"cosine\", \"default_top_k\": 5 } ) # Initialize store store = ConfigurablePgVectorStore(config=config) store.setup() # Add documents nodes = [ Node( content=\"Python is great for data science.\", metadata=NodeMetadata(source_file_uuid=\"doc1\", position=0) ), Node( content=\"JavaScript runs in web browsers.\", metadata=NodeMetadata(source_file_uuid=\"doc1\", position=1) ), ] store.add_nodes(nodes) # Search results = store.search(\"What language is good for data?\", top_k=3) for r in results: print(f\"[{r['score']:.4f}] {r['content']}\") # Cleanup store.shutdown()","title":"Using PostgreSQL with pgvector"},{"location":"quick_start/#quick-start-configurable-vector-store","text":"The ConfigurablePgVectorStore provides a production-ready vector store with comprehensive configuration options.","title":"Quick Start: Configurable Vector Store"},{"location":"quick_start/#vectorstoreconfig-overview","text":"from rakam_systems.ai_vectorstore.config import ( VectorStoreConfig, EmbeddingConfig, DatabaseConfig, SearchConfig, IndexConfig, load_config ) # Create configuration programmatically config = VectorStoreConfig( name=\"production_store\", # Embedding configuration embedding=EmbeddingConfig( model_type=\"sentence_transformer\", # or \"openai\", \"cohere\" model_name=\"Snowflake/snowflake-arctic-embed-m\", batch_size=128, normalize=True, # api_key loaded from env: OPENAI_API_KEY or COHERE_API_KEY ), # Database configuration (uses env vars by default) database=DatabaseConfig( host=\"localhost\", # or POSTGRES_HOST env var port=5432, # or POSTGRES_PORT env var database=\"vectorstore_db\", user=\"postgres\", password=\"postgres\", ), # Search configuration search=SearchConfig( similarity_metric=\"cosine\", # \"cosine\", \"l2\", \"dot_product\" default_top_k=5, enable_hybrid_search=True, # Combine vector + keyword search hybrid_alpha=0.7, # Vector weight (0.3 for keyword) rerank=True, ), # Indexing configuration index=IndexConfig( chunk_size=512, chunk_overlap=50, batch_insert_size=10000, ), # Additional options enable_caching=True, cache_size=1000, enable_logging=True, log_level=\"INFO\", ) # Validate configuration config.validate()","title":"VectorStoreConfig Overview"},{"location":"quick_start/#configuration-from-yaml","text":"Create config/vectorstore.yaml : name: production_vectorstore embedding: model_type: sentence_transformer model_name: Snowflake/snowflake-arctic-embed-m batch_size: 128 normalize: true database: host: localhost port: 5432 database: vectorstore_db user: postgres password: postgres search: similarity_metric: cosine default_top_k: 5 enable_hybrid_search: true hybrid_alpha: 0.7 rerank: true index: chunk_size: 512 chunk_overlap: 50 batch_insert_size: 10000 enable_caching: true cache_size: 1000 enable_logging: true log_level: INFO Load and use: from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig # Load from YAML config = VectorStoreConfig.from_yaml(\"config/vectorstore.yaml\") # Or load from JSON config = VectorStoreConfig.from_json(\"config/vectorstore.json\") # Or use the helper function (auto-detects format) from rakam_systems.ai_vectorstore.config import load_config config = load_config(\"config/vectorstore.yaml\") # Create store store = ConfigurablePgVectorStore(config=config) store.setup()","title":"Configuration from YAML"},{"location":"quick_start/#using-different-embedding-models","text":"from rakam_systems.ai_vectorstore.config import VectorStoreConfig, EmbeddingConfig # Local embeddings with Sentence Transformers config_local = VectorStoreConfig( embedding=EmbeddingConfig( model_type=\"sentence_transformer\", model_name=\"Snowflake/snowflake-arctic-embed-m\", # 768 dimensions # model_name=\"all-MiniLM-L6-v2\", # 384 dimensions batch_size=128, ) ) # OpenAI embeddings config_openai = VectorStoreConfig( embedding=EmbeddingConfig( model_type=\"openai\", model_name=\"text-embedding-3-small\", # api_key loaded from OPENAI_API_KEY env var ) ) # Cohere embeddings config_cohere = VectorStoreConfig( embedding=EmbeddingConfig( model_type=\"cohere\", model_name=\"embed-english-v3.0\", # api_key loaded from COHERE_API_KEY env var ) )","title":"Using Different Embedding Models"},{"location":"quick_start/#multi-model-support","text":"Each embedding model automatically gets dedicated tables, preventing mixing of incompatible vector spaces: from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig, EmbeddingConfig # Store using MiniLM model config_minilm = VectorStoreConfig( embedding=EmbeddingConfig( model_type=\"sentence_transformer\", model_name=\"all-MiniLM-L6-v2\" # 384D ) ) store_minilm = ConfigurablePgVectorStore(config=config_minilm) # Tables: application_nodeentry_all_minilm_l6_v2 # Store using Arctic model config_arctic = VectorStoreConfig( embedding=EmbeddingConfig( model_type=\"sentence_transformer\", model_name=\"Snowflake/snowflake-arctic-embed-m\" # 768D ) ) store_arctic = ConfigurablePgVectorStore(config=config_arctic) # Tables: application_nodeentry_snowflake_arctic_embed_m # Both can coexist without conflicts! Important : Even if two models have the same dimensions, their vector spaces are different! Model-specific tables prevent meaningless search results from mixed embeddings.","title":"Multi-Model Support"},{"location":"quick_start/#hybrid-search","text":"Combine vector similarity with keyword search: from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore, VectorStoreConfig config = VectorStoreConfig() config.search.enable_hybrid_search = True config.search.hybrid_alpha = 0.7 # 70% vector, 30% keyword store = ConfigurablePgVectorStore(config=config) store.setup() # Regular search (uses config defaults) results = store.search(\"machine learning algorithms\", top_k=10) # Hybrid search with custom alpha results = store.hybrid_search( query=\"machine learning algorithms\", top_k=10, alpha=0.5 # 50/50 split ) for r in results: print(f\"[{r['score']:.4f}] {r['content'][:100]}...\")","title":"Hybrid Search"},{"location":"quick_start/#full-example-with-all-features","text":"import os import django from django.conf import settings # Configure Django if not settings.configured: settings.configure( INSTALLED_APPS=[ 'django.contrib.contenttypes', 'rakam_systems.ai_vectorstore.components.vectorstore', ], DATABASES={ 'default': { 'ENGINE': 'django.db.backends.postgresql', 'NAME': os.getenv('POSTGRES_DB', 'vectorstore_db'), 'USER': os.getenv('POSTGRES_USER', 'postgres'), 'PASSWORD': os.getenv('POSTGRES_PASSWORD', 'postgres'), 'HOST': os.getenv('POSTGRES_HOST', 'localhost'), 'PORT': os.getenv('POSTGRES_PORT', '5432'), } }, DEFAULT_AUTO_FIELD='django.db.models.BigAutoField', ) django.setup() from rakam_systems.ai_vectorstore import ( ConfigurablePgVectorStore, VectorStoreConfig, Node, NodeMetadata, VSFile ) from rakam_systems.ai_vectorstore.components.loader import AdaptiveLoader # Create configuration config = VectorStoreConfig.from_yaml(\"config/vectorstore.yaml\") # Initialize store with context manager with ConfigurablePgVectorStore(config=config) as store: # Load documents loader = AdaptiveLoader(config={ \"chunk_size\": config.index.chunk_size, \"chunk_overlap\": config.index.chunk_overlap }) # Load from file vsfile = loader.load_as_vsfile(\"documents/manual.pdf\") store.add_vsfile(vsfile) # Or add nodes directly nodes = [ Node( content=\"Python is excellent for data science and machine learning.\", metadata=NodeMetadata( source_file_uuid=\"doc_001\", position=0, custom={\"category\": \"programming\", \"topic\": \"python\"} ) ), Node( content=\"PostgreSQL with pgvector enables efficient vector similarity search.\", metadata=NodeMetadata( source_file_uuid=\"doc_001\", position=1, custom={\"category\": \"database\", \"topic\": \"vectors\"} ) ), ] store.add_nodes(nodes) # Search with filters results = store.search( query=\"vector database search\", top_k=5, # Metadata filters can be applied here ) print(\"Search Results:\") for r in results: print(f\" [{r['score']:.4f}] {r['content'][:80]}...\") print(f\" Source: {r.get('source_file_uuid', 'N/A')}\") # Hybrid search print(\"\\nHybrid Search Results:\") hybrid_results = store.hybrid_search( query=\"PostgreSQL vector\", top_k=5, alpha=0.6 ) for r in hybrid_results: print(f\" [{r['score']:.4f}] {r['content'][:80]}...\") # Get collection stats count = store.count() print(f\"\\nTotal vectors in store: {count}\") # Store automatically cleaned up","title":"Full Example with All Features"},{"location":"quick_start/#configuration-reference","text":"","title":"Configuration Reference"},{"location":"quick_start/#embeddingconfig","text":"Option Type Default Description model_type str sentence_transformer Backend: sentence_transformer , openai , cohere model_name str Snowflake/snowflake-arctic-embed-m Model identifier api_key str None API key (auto-loaded from env) batch_size int 128 Batch size for encoding normalize bool True Normalize embeddings dimensions int None Vector dimensions (auto-detected)","title":"EmbeddingConfig"},{"location":"quick_start/#databaseconfig","text":"Option Type Default Description host str localhost PostgreSQL host (or POSTGRES_HOST env) port int 5432 PostgreSQL port (or POSTGRES_PORT env) database str vectorstore_db Database name (or POSTGRES_DB env) user str postgres Username (or POSTGRES_USER env) password str postgres Password (or POSTGRES_PASSWORD env) pool_size int 10 Connection pool size","title":"DatabaseConfig"},{"location":"quick_start/#searchconfig","text":"Option Type Default Description similarity_metric str cosine Metric: cosine , l2 , dot_product default_top_k int 5 Default results count enable_hybrid_search bool True Enable keyword + vector search hybrid_alpha float 0.7 Vector weight (1-alpha for keyword) rerank bool True Rerank results","title":"SearchConfig"},{"location":"quick_start/#indexconfig","text":"Option Type Default Description chunk_size int 512 Chunk size in tokens chunk_overlap int 50 Overlap between chunks batch_insert_size int 10000 Batch size for inserts","title":"IndexConfig"},{"location":"quick_start/#quick-start-rag-pipeline","text":"Build a complete Retrieval-Augmented Generation pipeline: import asyncio from rakam_systems.ai_agents import BaseAgent from rakam_systems.ai_vectorstore.components.vectorstore.faiss_vector_store import FaissStore from rakam_systems.ai_vectorstore.components.loader import AdaptiveLoader from rakam_systems.ai_core.interfaces.tool import ToolComponent # Step 1: Load and index documents loader = AdaptiveLoader(config={\"chunk_size\": 512, \"chunk_overlap\": 50}) # Load documents (supports PDF, DOCX, TXT, MD, HTML, etc.) # nodes = loader.load_as_nodes(\"path/to/document.pdf\") # For demo, use sample text from rakam_systems.ai_vectorstore.core import Node, NodeMetadata sample_docs = [ \"Our company was founded in 2020 and specializes in AI solutions.\", \"We offer three main products: AI Assistant, Data Analytics, and Automation.\", \"The AI Assistant can handle customer queries 24/7 with 95% accuracy.\", \"Our pricing starts at $99/month for small businesses.\", \"Enterprise customers get dedicated support and custom integrations.\", \"We have offices in New York, London, and Tokyo.\", ] nodes = [ Node(content=doc, metadata=NodeMetadata(source_file_uuid=\"company_info\", position=i)) for i, doc in enumerate(sample_docs) ] # Step 2: Create vector store store = FaissStore( name=\"rag_store\", base_index_path=\"./rag_indexes\", embedding_model=\"Snowflake/snowflake-arctic-embed-m\", initialising=True ) store.create_collection_from_nodes(\"knowledge_base\", nodes) # Step 3: Create search tool def search_knowledge_base(query: str, top_k: int = 3) -> str: \"\"\"Search the knowledge base for relevant information.\"\"\" results, _ = store.search( collection_name=\"knowledge_base\", query=query, distance_type=\"cosine\", number=top_k ) context = \"\\n\".join([ f\"- {content}\" for _, (_, content, _) in results.items() ]) return f\"Relevant information:\\n{context}\" search_tool = ToolComponent.from_function( function=search_knowledge_base, name=\"search_knowledge_base\", description=\"Search the company knowledge base for information\", json_schema={ \"type\": \"object\", \"properties\": { \"query\": {\"type\": \"string\", \"description\": \"Search query\"}, \"top_k\": {\"type\": \"integer\", \"default\": 3, \"description\": \"Number of results\"} }, \"required\": [\"query\"], \"additionalProperties\": False } ) # Step 4: Create RAG agent async def main(): agent = BaseAgent( name=\"rag_agent\", model=\"openai:gpt-4o\", system_prompt=\"\"\"You are a helpful customer service agent for our company. Use the search_knowledge_base tool to find relevant information before answering. Always base your answers on the retrieved information.\"\"\", tools=[search_tool] ) # Ask questions questions = [ \"When was the company founded?\", \"What products do you offer?\", \"How much does the service cost?\", ] for question in questions: print(f\"\\nQ: {question}\") result = await agent.arun(question) print(f\"A: {result.output_text}\") asyncio.run(main())","title":"Quick Start: RAG Pipeline"},{"location":"quick_start/#quick-start-llm-gateway","text":"Use the LLM Gateway for direct LLM interactions: from rakam_systems.ai_agents.components.llm_gateway import ( OpenAIGateway, MistralGateway, LLMGatewayFactory, LLMRequest ) # Create gateway using factory gateway = LLMGatewayFactory.create( provider=\"openai\", model=\"gpt-4o\" ) # Simple text generation request = LLMRequest( system_prompt=\"You are a helpful assistant.\", user_prompt=\"Explain quantum computing in simple terms.\", temperature=0.7, max_tokens=200 ) response = gateway.generate(request) print(f\"Response: {response.content}\") print(f\"Tokens used: {response.usage}\")","title":"Quick Start: LLM Gateway"},{"location":"quick_start/#structured-output-with-gateway","text":"from pydantic import BaseModel from rakam_systems.ai_agents.components.llm_gateway import OpenAIGateway, LLMRequest class Recipe(BaseModel): name: str ingredients: list[str] instructions: list[str] prep_time_minutes: int servings: int gateway = OpenAIGateway(model=\"gpt-4o\") request = LLMRequest( system_prompt=\"You are a chef. Create recipes based on user requests.\", user_prompt=\"Give me a simple pasta recipe.\" ) recipe = gateway.generate_structured(request, Recipe) print(f\"Recipe: {recipe.name}\") print(f\"Prep time: {recipe.prep_time_minutes} minutes\") print(f\"Ingredients: {', '.join(recipe.ingredients)}\")","title":"Structured Output with Gateway"},{"location":"quick_start/#streaming-with-gateway","text":"from rakam_systems.ai_agents.components.llm_gateway import OpenAIGateway, LLMRequest gateway = OpenAIGateway(model=\"gpt-4o\") request = LLMRequest( user_prompt=\"Write a poem about coding.\", temperature=0.8 ) print(\"Poem:\\n\") for chunk in gateway.stream(request): print(chunk, end=\"\", flush=True) print()","title":"Streaming with Gateway"},{"location":"quick_start/#quick-start-document-loading","text":"Load and process various document types: from rakam_systems.ai_vectorstore.components.loader import AdaptiveLoader # Create loader loader = AdaptiveLoader(config={ \"chunk_size\": 512, \"chunk_overlap\": 50, \"encoding\": \"utf-8\" }) # Load as text text = loader.load_as_text(\"document.pdf\") print(f\"Loaded {len(text)} characters\") # Load as chunks chunks = loader.load_as_chunks(\"document.pdf\") print(f\"Created {len(chunks)} chunks\") # Load as nodes (with metadata) nodes = loader.load_as_nodes( \"document.pdf\", custom_metadata={\"category\": \"technical\", \"author\": \"John\"} ) print(f\"Created {len(nodes)} nodes\") # Load as VSFile (complete document representation) vsfile = loader.load_as_vsfile(\"document.pdf\") print(f\"File: {vsfile.file_name}, UUID: {vsfile.uuid}\")","title":"Quick Start: Document Loading"},{"location":"quick_start/#supported-file-types","text":"Type Extensions Text .txt , .text Markdown .md , .markdown PDF .pdf Word .docx , .doc OpenDocument .odt HTML .html , .htm , .xhtml Email .eml , .msg Data .json , .csv , .tsv , .xlsx , .xls Code .py , .js , .ts , .java , .cpp , .go , .rs , .rb","title":"Supported File Types"},{"location":"quick_start/#quick-start-configuration-from-yaml","text":"Load agents and tools from configuration files: # config/agents.yaml version: \"1.0\" prompts: assistant: system_prompt: | You are a helpful AI assistant. Always be accurate and concise. tools: get_time: name: \"get_time\" type: \"direct\" module: \"datetime\" function: \"datetime.now\" description: \"Get current date and time\" json_schema: type: \"object\" properties: {} agents: main_agent: name: \"main_agent\" model_config: model: \"openai:gpt-4o\" temperature: 0.7 prompt_config: \"assistant\" tools: - \"get_time\" enable_tracking: true from rakam_systems.ai_core.config_loader import ConfigurationLoader loader = ConfigurationLoader() # Load configuration config = loader.load_from_yaml(\"config/agents.yaml\") # Create agent from config agent = loader.create_agent(\"main_agent\", config) # Use the agent result = await agent.arun(\"What time is it?\")","title":"Quick Start: Configuration from YAML"},{"location":"quick_start/#next-steps","text":"Now that you've completed the quick start: Read the Full Documentation Components Guide - Detailed component documentation Development Guide - How to extend the framework Installation Guide - Advanced installation options Explore More Examples rakam_systems/examples/ai_agents_examples/ - Agent examples rakam_systems/examples/ai_vectorstore_examples/ - Vector store examples rakam_systems/examples/configs/ - Configuration examples Build Your Application Start with a simple agent Add tools for your specific use case Integrate vector storage for RAG Scale with PostgreSQL for production Join the Community GitHub Repository Report issues and request features Contribute to the project","title":"Next Steps"},{"location":"quick_start/#common-patterns","text":"","title":"Common Patterns"},{"location":"quick_start/#error-handling","text":"import asyncio from rakam_systems.ai_agents import BaseAgent async def main(): agent = BaseAgent( name=\"safe_agent\", model=\"openai:gpt-4o\", system_prompt=\"You are helpful.\" ) try: result = await agent.arun(\"Hello!\") print(result.output_text) except Exception as e: print(f\"Error: {e}\") asyncio.run(main())","title":"Error Handling"},{"location":"quick_start/#context-manager-pattern","text":"from rakam_systems.ai_vectorstore import ConfigurablePgVectorStore # Automatic setup and cleanup with ConfigurablePgVectorStore(config=config) as store: store.add_nodes(nodes) results = store.search(\"query\") # shutdown() called automatically","title":"Context Manager Pattern"},{"location":"quick_start/#async-batch-processing","text":"import asyncio from rakam_systems.ai_agents import BaseAgent async def process_queries(queries: list[str]): agent = BaseAgent( name=\"batch_agent\", model=\"openai:gpt-4o\", system_prompt=\"Answer concisely.\" ) # Process queries concurrently tasks = [agent.arun(q) for q in queries] results = await asyncio.gather(*tasks) return [r.output_text for r in results] queries = [\"What is Python?\", \"What is JavaScript?\", \"What is Rust?\"] answers = asyncio.run(process_queries(queries)) for q, a in zip(queries, answers): print(f\"Q: {q}\\nA: {a}\\n\") Happy Building! \ud83d\ude80","title":"Async Batch Processing"},{"location":"start/","text":"","title":"Getting Started"},{"location":"templates/","text":"Agent Templates RAG Template","title":"Templates"},{"location":"templates/#_1","text":"","title":""},{"location":"templates/#agent-templates","text":"","title":"Agent Templates"},{"location":"templates/#rag-template","text":"","title":"RAG Template"},{"location":"tutorials/","text":"","title":"Tutorials"}]}